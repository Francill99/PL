## Standard libraries
import os
import numpy as np
import random
import math
import time
import copy
import argparse
import torch
import gc

## PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader


device = torch.device("cpu")
print("Device:", device)

"""## Functions"""

#Checkpoints (to save model parameters during training)
class SaveBestModel:
    def __init__(self, name, print=True, best_valid_loss=-float('inf')): #object initialized with best_loss = +infinite
        self.best_valid_loss = best_valid_loss
        self.print = print
        self.name = name

    def __call__(
        self, current_valid_loss,
        epoch, model, optimizer, criterion, time, asymmetry=None, overlap=None
    ):
        if current_valid_loss > self.best_valid_loss:
            self.best_valid_loss = current_valid_loss
            if self.print==True:
                print("Saving best model for epoch: %d, current val loss: %.4e, t: %.1f\n" % (epoch+1, current_valid_loss, time))
            # method to save a model (the state_dict: a python dictionary object that
            # maps each layer to its parameter tensor) and other useful parametrers
            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html
            if asymmetry is not None and overlap is not None:
                torch.save({
                    'epoch': epoch+1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': criterion,
                    "time": time,
                    "asymmetry": asymmetry,
                    "overlap": overlap,
                    }, self.name)
            else:
                torch.save({
                    'epoch': epoch+1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': criterion,
                    "time": time,
                }, self.name)

#Checkpoints (to save model parameters during training)
class Save_Model:
    def __init__(self, name, print=True): #object initialized with best_loss = +infinite
        self.print = print
        self.name = name

    def __call__(
        self, current_valid_loss,
        epoch, model, optimizer, criterion, time, asymmetry=None, overlap=None
    ):

        if asymmetry is not None and overlap is not None:
            torch.save({
                'epoch': epoch+1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': criterion,
                "time": time,
                "asymmetry": asymmetry,
                "overlap": overlap,
                }, self.name)
        else:
            torch.save({
                'epoch': epoch+1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': criterion,
                "time": time,
            }, self.name)

def loss_single_i(output):                        #out.shape = [B]
    g_i_b = output
    loss = g_i_b.mean()
    return loss

def running_mean(x, N):
    cumsum = np.cumsum(np.insert(x, 0, 0))
    return (cumsum[N:] - cumsum[:-N]) / float(N)


def evaluate_loss(model, dataset, device,l):
    train_loss = 0.0
    counter = 0
    model.eval()
    with torch.no_grad():
        for batch_element in dataloader: #takes a batch from the train dataloader
            for i in range(N):
                counter += 1
                inp_data = batch_element
                inp_data = inp_data.to(device)

                B = inp_data.shape[0]

                loss = model(inp_data, i_rand= i, lambd=l)
                train_loss += loss.item()

        train_loss = train_loss/(counter)

    return train_loss

def compute_asymmetry(J: np.ndarray) -> float:
    # Ensure that J is a NumPy array
    J = np.array(J)

    # Step 1: Compute the difference between J and its transpose
    asymmetry_matrix = J - J.T

    # Step 2: Square each element in the asymmetry matrix
    squared_diff_matrix = np.square(asymmetry_matrix)

    # Step 3: Compute the mean of the squared differences
    asymmetry = np.mean(squared_diff_matrix)

    return asymmetry

def start_overlap_binary(xi: torch.Tensor, init_overlap: float) -> torch.Tensor:
    # Copy xi to avoid modifying the input directly
    init_vectors = xi.clone()

    # Get the dimensions of the input tensor
    X, N, d = init_vectors.shape

    # For each position of X
    for x in range(X):
        # Calculate the number of rows to flip (multiply by -1)
        num_rows_to_flip = int(N * ((1-init_overlap) / 2.))

        # Randomly choose 'num_rows_to_flip' unique indices from the N rows
        indices_to_flip = torch.randperm(N)[:num_rows_to_flip]

        # Multiply the selected rows by -1
        init_vectors[x, indices_to_flip, :] *= -1

    return init_vectors

def basins_of_attraction_xi(init_overlaps_array, model, dataset, num_of_run, n, device):
    max_overlap_xi_list = []

    # Loop over each init_overlap value
    for init_overlap in init_overlaps_array:
        # Generate input_vectors based on the init_overlap
        input_vectors = start_overlap_binary(dataset.xi, init_overlap)

        # Normalize input_vectors using the model's normalization method
        input_vectors = model.normalize_x(input_vectors)

        # Send the input_vectors to the appropriate device (e.g., GPU/CPU)
        input_vectors = input_vectors.to(device)

        # Run the converge function to get the necessary outputs
        overlap_max_n_xi, overlap_max_n_f, max_overlap_xi, max_overlap_f, final_overlap, _, overlap_argmax = converge(input_vectors[:num_of_run], model, dataset, n)

        # Append the computed max_overlap_xi to the list
        max_overlap_xi_list.append(overlap_max_n_xi[:num_of_run,-1])

    # Convert the list to a NumPy array
    max_overlap_xi_array = np.array(max_overlap_xi_list)

    return max_overlap_xi_array

"""### Dynamics analisys functions"""

def max_overlap(x_new, vectors):
    """
    Computes the maximum overlap and corresponding indices between x_new and vectors.

    Args:
    - x_new: Tensor of shape [batch_size, n, N, d].
    - vectors: Tensor of shape [X, N, d].

    Returns:
    - overlap_max: Tensor of maximum overlap values of shape [batch_size].
    - max_values: Tensor of corresponding max values of shape [batch_size].
    """
    batch_size, n, N, d = x_new.shape
    X = vectors.shape[0]

    # Step 1: Compute dot products for each [N, d] pair in x_new and vectors.
    # Resulting shape: [batch_size, n, X, N]
    dot_products = torch.einsum('bnid,xid->bnxi', x_new, vectors)

    # Step 2: Compute the mean over the N dimension.
    # Resulting shape: [batch_size, n, X]
    dot_products_mean = dot_products.mean(dim=3)    #[b,n,x]

    # Step 3: Compute max and argmax over the n dimension.
    # Resulting shapes: [batch_size, n], [batch_size, n]
    overlap_max_n, overlap_argmax_n = torch.max(dot_products_mean, dim=-1)

    # Step 4: Compute max and argmax over the X dimension.
    # Resulting shapes: [batch_size], [batch_size]
    overlap_max, overlap_argmax = torch.max(overlap_max_n, dim=1)
    max_values = overlap_max_n.gather(1, overlap_argmax.unsqueeze(-1)).squeeze()

    return overlap_max_n.cpu().numpy(), overlap_max.cpu().numpy(), max_values.cpu().numpy(), overlap_argmax.cpu().numpy()

def converge(input_vectors, model, dataset, n, features=False):
    """
    Function to perform dynamics and compute overlaps.

    Args:
    - input_vectors: Tensor of shape [batch_size, N, d].
    - model: The model instance with a dyn_n_step method.
    - dataset: The dataset instance containing xi and f.
    - n: The number of steps to simulate.

    Returns:
    - max_overlap_xi: Tensor of max overlap values with xi.
    - max_overlap_f: Tensor of max overlap values with f.
    - final_overlap: Tensor of final overlap values with input_vectors.
    """
    # Step 1: Compute x_new using the model's dyn_n_step method
    x_new = model.dyn_n_step(input_vectors, n)  #[B,n,N,d]

    # Step 2: Calculate max overlaps using dataset.xi and dataset.f
    overlap_max_n_xi, max_overlap_xi, _,overlap_argmax = max_overlap(x_new, dataset.xi.to(device))
    if features == True:
        overlap_max_n_f, max_overlap_f, _, _ = max_overlap(x_new, dataset.f.to(device))
    else:
        overlap_max_n_f = np.zeros_like(overlap_max_n_xi)
        max_overlap_f = np.zeros_like(max_overlap_xi)

    input_overlap_n = torch.einsum('bnid,bid->bni', x_new, input_vectors).mean(dim=-1)  #[b,n]
    max_input_overlap = torch.max(input_overlap_n, dim=1)[0].cpu().numpy()

    return overlap_max_n_xi, overlap_max_n_f, max_overlap_xi, max_overlap_f, input_overlap_n.cpu().numpy(), max_input_overlap, overlap_argmax

def converge_input_vector_compute_overlap(input_data, model, init_overlap, n):
    input_vectors = start_overlap_binary(input_data, init_overlap)

    input_vectors = model.normalize_x(input_vectors)
    input_vectors = input_vectors.to(device)
    x_new = model.dyn_n_step(input_vectors, n)  #[B, n, N,i]
    overlap = torch.einsum('bnid,bid->bni', x_new, input_data).mean(dim=-1)  #[b,n]
    return overlap

def basins_of_attraction_inp_vectors(input_data, init_overlaps_array, model, n):
    max_overlap_inp_vectors_list = []

    # Loop over each init_overlap value
    for init_overlap in init_overlaps_array:
        # Generate input_vectors based on the init_overlap
        overlaps = converge_input_vector_compute_overlap(input_data, model, init_overlap, n)[:,-1]
        max_overlap_inp_vectors_list.append(overlaps.detach().cpu())

    # Convert the list to a NumPy array
    max_overlap_inp_vectors_array = np.array(max_overlap_inp_vectors_list)
    return max_overlap_inp_vectors_array

"""## High-level functions"""

def initialize(N=1000, P=400, D=0, d=1, on_sphere=True, l=1, device='cuda'):
    # Initialize the dataset
    dataset = CustomDataset(P, N, D, d, seed=444, sigma=0.5, on_sphere=on_sphere, coefficients="binary")
    if D>0:
        dataset.RF(seed=444)

    # Initialize the model
    model = TwoBodiesModel(N, d, on_sphere)
    model.to(device)  # Move the model to the specified device

    # Apply the Hebb rule
    model.Hebb(dataset.xi, 'Tensorial')

    # Return the dataset and model
    return dataset, model

def train_model(model, dataloader, dataloader_f, dataloader_gen, epochs, learning_rate, max_norm, device, data_PATH, model_name, init_overlap, n, l, fake_opt, J2, norm_J2, valid_every, ALPHA, epochs_to_save, model_name_base, save):
    # Initial setup
    norm_0 = torch.tensor(1)
    norm = torch.tensor(1)
    save_model_epoch = np.empty(len(epochs_to_save), dtype=object)

    # Initialize SaveModel class
    save_model = Save_Model(data_PATH + model_name, print=False)
    for i_e, e in enumerate(epochs_to_save):
        save_model_epoch[i_e] = Save_Model(data_PATH+model_name_base+"ep{}.pth".format(e), print=False)
    aa = 0
    # Initialize histories
    hist_loss = []
    hist_vloss = []
    hist_asymm = []
    hist_diff = []
    hist_J_norm = []

    t_in = time.time()

    # Training loop
    for epoch in range(epochs):
        t0 = time.time()
        model.train()
        train_loss = 0.0
        counter = 0

        # Training batch-wise
        for batch_element in dataloader:
            counter += 1
            inp_data = batch_element.to(device)

            # Compute loss
            loss = model(inp_data, lambd=l)

            # Check for valid loss values (no NaN or Inf)
            if (torch.isnan(loss).any() == False) and (torch.isinf(loss).any() == False):
                model.zero_grad()
                with torch.no_grad():
                    # Backward and gradient descent
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
                    for param in model.parameters():
                        param.data -= learning_rate * param.grad
                    train_loss += loss.item()
            else:
                print("Detected nan "+ model_name_base+" epoch{} lr{}".format(epoch, learning_rate))
                model.J.data *= 0.1
                learning_rate *= 0.1

        # Average training loss
        train_loss = train_loss / counter
        hist_loss.append(train_loss)
        model.eval()

        # Validation and model saving
        if epoch % valid_every == 0 and epoch > 0:
            vali_loss = 0.0
            counter = 0

            with torch.no_grad():
                for batch_element in dataloader:
                    a=1
                    counter += 1
                    inp_data = batch_element.to(device)
                    # Overlap and model dynamics computations
                    input_vectors = start_overlap_binary(inp_data, init_overlap)
                    input_vectors = model.normalize_x(input_vectors)
                    if (epoch == epochs-1):
                        n = 100             
                    #x_new = model.dyn_n_step(input_vectors, n)
                    '''
                    # Overlap calculations
                    overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)  # [b,n]
                    final_overlaps = overlaps[:, -1]
                    max_input_overlap, _ = torch.max(overlaps, dim=-1)

                    vloss = final_overlaps.mean().cpu().numpy()
                    vali_loss += vloss
                    '''

            if counter != 0:
                vali_loss = vali_loss / counter
            hist_vloss.append(vali_loss)

            counter_f = 0
            vali_loss_f = 0

            with torch.no_grad():
                for batch_element in dataloader_f:
                    a=1
                    
                    counter_f +=1
                    inp_data = batch_element
                    inp_data = inp_data.to(device)
                    input_vectors = start_overlap_binary(inp_data, init_overlap)
                    input_vectors = model.normalize_x(input_vectors)
                    if (epoch == epochs-1):
                        n = 100
                    #x_new = model.dyn_n_step(input_vectors, n)
                    '''
                    overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)         #[b,n]

                    final_overlaps = overlaps[:,-1]

                    vloss_f = final_overlaps.mean().cpu().numpy()
                    ###################################
                    vali_loss_f += vloss_f
                    #data = model.parallel_step(inp_data, g=g, iterations)
                    '''
            if counter_f !=0:
                vali_loss_f = vali_loss_f/(counter_f)

            counter_gen = 0
            vali_loss_gen = 0

            with torch.no_grad():  # Disable gradient computation for evaluation
                for batch_element in dataloader_gen:
                    a=1
                    counter_gen += 1
                    inp_data = batch_element.to(device)
        
                    # Overlap and model dynamics computations
                    input_vectors = start_overlap_binary(inp_data, init_overlap)
                    input_vectors = model.normalize_x(input_vectors)
                    if (epoch == epochs-1):
                        n = 100
                    #x_new = model.dyn_n_step(input_vectors, n)
                    '''
                    # Overlap calculations
                    overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)
                    final_overlaps = overlaps[:, -1]
                    max_input_overlap, _ = torch.max(overlaps, dim=-1)
        
                    # Compute validation loss
                    vloss_gen = final_overlaps.mean().cpu().numpy()
                    vali_loss_gen += vloss_gen
                    '''

            if counter_gen != 0:
                vali_loss_gen = vali_loss_gen / counter_gen

            elapsed_time = time.time() - t0
            time_from_in = time.time() - t_in

            #Save checkpoints
            if (epoch in epochs_to_save) and save==True:
                save_model_epoch[aa](vali_loss, epoch, model, fake_opt, hist_vloss, time_from_in)
                aa +=1

            # Save last model
            if (epoch == epochs-1):
                if save==True:
                    save_model(vali_loss, epoch, model, fake_opt, hist_vloss, time_from_in)
                else:
                    to_save = np.array([vali_loss,vali_loss_f,vali_loss_gen])
                    #np.save(data_PATH + model_name_base+"overlaps",to_save)
                    #print(to_save)
            # Compute model parameters for logging
            J = model.J.squeeze().cpu().detach().numpy()
            norm_J = np.linalg.norm(J)
            asymmetry = compute_asymmetry(J)
            diff_Hebb = np.linalg.norm(J2 * norm_J / norm_J2 - J) / norm_J

            # Append to history
            hist_asymm.append(asymmetry)
            hist_diff.append(diff_Hebb)
            hist_J_norm.append(norm_J)
    #############################################
            
    model.eval()
    vali_loss = 0.0
    counter = 0
    with torch.no_grad():
        for batch_element in dataloader:
            counter += 1
            inp_data = batch_element.to(device)
            # Overlap and model dynamics computations
            input_vectors = start_overlap_binary(inp_data, init_overlap)
            input_vectors = model.normalize_x(input_vectors)
            if (epoch == epochs-1):
                n = 100
            x_new = model.dyn_n_step(input_vectors, n)
            # Overlap calculations
            overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)  # [b,n]
            final_overlaps = overlaps[:, -1]
            max_input_overlap, _ = torch.max(overlaps, dim=-1)
            vloss = final_overlaps.mean().cpu().numpy()
            vali_loss += vloss
    if counter != 0:
        vali_loss = vali_loss / counter
    hist_vloss.append(vali_loss)
    counter_f = 0
    vali_loss_f = 0
    with torch.no_grad():
        for batch_element in dataloader_f:
            counter_f +=1
            inp_data = batch_element
            inp_data = inp_data.to(device)
            input_vectors = start_overlap_binary(inp_data, init_overlap)
            input_vectors = model.normalize_x(input_vectors)
            if (epoch == epochs-1):
                n = 100
            x_new = model.dyn_n_step(input_vectors, n)
            overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)         #[b,n]
            final_overlaps = overlaps[:,-1]
            vloss_f = final_overlaps.mean().cpu().numpy()
            ###################################
            vali_loss_f += vloss_f
            #data = model.parallel_step(inp_data, g=g, iterations)
    if counter_f !=0:
        vali_loss_f = vali_loss_f/(counter_f)
    counter_gen = 0
    vali_loss_gen = 0
    with torch.no_grad():  # Disable gradient computation for evaluation
        for batch_element in dataloader_gen:
            counter_gen += 1
            inp_data = batch_element.to(device)

            # Overlap and model dynamics computations
            input_vectors = start_overlap_binary(inp_data, init_overlap)
            input_vectors = model.normalize_x(input_vectors)
            if (epoch == epochs-1):
                n = 100
            x_new = model.dyn_n_step(input_vectors, n)

            # Overlap calculations
            overlaps = torch.einsum('bnid,bid->bni', x_new, inp_data).mean(dim=-1)
            final_overlaps = overlaps[:, -1]
            max_input_overlap, _ = torch.max(overlaps, dim=-1)

            # Compute validation loss
            vloss_gen = final_overlaps.mean().cpu().numpy()
            vali_loss_gen += vloss_gen
    if counter_gen != 0:
        vali_loss_gen = vali_loss_gen / counter_gen
    elapsed_time = time.time() - t0
    time_from_in = time.time() - t_in
    #Save checkpoints
    if (epoch in epochs_to_save) and save==True:
        save_model_epoch[aa](vali_loss, epoch, model, fake_opt, hist_vloss, time_from_in)
        aa +=1
    # Save last model
    if (epoch == epochs-1):
        if save==True:
            save_model(vali_loss, epoch, model, fake_opt, hist_vloss, time_from_in)
        else:
            to_save = np.array([vali_loss,vali_loss_f,vali_loss_gen])
            #np.save(data_PATH + model_name_base+"overlaps",to_save)
            print(to_save)
            J = model.J.squeeze().cpu().detach().numpy()
            asymmetry = compute_asymmetry(J)
            print(asymmetry)
    # Compute model parameters for logging
    J = model.J.squeeze().cpu().detach().numpy()
    norm_J = np.linalg.norm(J)
    asymmetry = compute_asymmetry(J)
    diff_Hebb = np.linalg.norm(J2 * norm_J / norm_J2 - J) / norm_J
    # Append to history
    hist_asymm.append(asymmetry)
    hist_diff.append(diff_Hebb)
    hist_J_norm.append(norm_J)

    # Return training history for further analysis
    return hist_loss, hist_vloss, hist_asymm, hist_diff, hist_J_norm

"""# Models"""

class TwoBodiesModel(nn.Module):
    def __init__(self, N, d, on_sphere, r=1):
        super(TwoBodiesModel, self).__init__()
        self.N = N
        self.d = d
        self.on_sphere = on_sphere
        self.r = r

        self.J = nn.Parameter(torch.randn(N, N, d, d))
        diagonal = self.J.data.diagonal(dim1=0, dim2=1)  # Get diagonal elements
        diagonal.fill_(0)
        self.normalize_J()

        self.mask = torch.ones(N, N, device=self.J.device)  # Shape [N, N]
        self.mask.fill_diagonal_(0)  # Set diagonal to 0

        # Expand the mask to match the shape of J (to broadcast correctly)
        self.mask = self.mask.unsqueeze(-1).unsqueeze(-1)  # Shape [N, N, 1, 1]

    def normalize_J(self):
        with torch.no_grad():
            self.J.data *= torch.sqrt(torch.tensor(1/(self.N*self.d)))

    def symmetrize_J(self):
        with torch.no_grad():
            self.J.data = (self.J.data + self.J.data.transpose(0,1))/2

    def normalize_x(self, x):
        """
        Normalize each d-dimensional vector in x on the sphere if self.on_sphere is True.
        """
        if self.on_sphere:
            with torch.no_grad():
                norms = x.norm(dim=-1, keepdim=True)+1e-9
                x = x / norms
        return x

    def Hebb(self, xi, form):
        """
        Calculates J based on the Hebb rule.
        xi -- input tensor of shape [P, N, d]
        form -- a string, either "Isotropic" or "Tensorial"
        """
        P = xi.shape[0]  # Number of patterns
        N = self.N
        d = self.d

        if form not in ["Isotropic", "Tensorial"]:
            raise ValueError("Form must be either 'Isotropic' or 'Tensorial'")

        # Zero out self.J to calculate the new values based on xi
        with torch.no_grad():
            self.J.zero_()

            # Hebbian rule for Isotropic or Tensorial form
            if form == "Isotropic":
                for mu in range(P):
                    # For Isotropic form: J_ij = (1/N) * sum(xi_i^mu * xi_j^mu)
                    for i in range(N):
                        for j in range(N):
                            if i != j:
                                self.J[i, j, :, :] += torch.sum(xi[mu, i, :] * xi[mu, j, :]) / N
            elif form == "Tensorial":
                for mu in range(P):
                    xi_mu = xi[mu].to(device)  # Shape: (N, D)
                    outer_products = torch.einsum('ia,jb->ijab', xi_mu, xi_mu) / N  # Shape: (N, N, D)

                    # Zero out diagonal elements
                    indices = torch.arange(N)
                    outer_products[indices, indices] = 0

                    # Update self.J
                    self.J += outer_products

                diagonal = self.J.data.diagonal(dim1=0, dim2=1)  # Get diagonal elements
                diagonal.fill_(0)

    def dyn_step(self, x, a=None):
        """
        Computes x_{t+1} = sum_j J_ij * x_j and then normalizes it.
        x -- input tensor of shape [N, d]
        """
        B, N, d = x.shape
        diagonal = self.J.data.diagonal(dim1=0, dim2=1)  # Get diagonal elements
        diagonal.fill_(0)
        with torch.no_grad():
            if a is None:
                x_new = torch.einsum('ijab,Bjb->Bia', self.J, x)
            else:
                x_new = x + a*torch.einsum('ijab,Bjb->Bia', self.J, x)
            x_new = self.normalize_x(x_new)
        return x_new

    def dyn_n_step(self, x, n, a=None, bar=False):
        """
        Computes the dynamics for n steps.
        Args:
        x -- input tensor of shape [B, N, d]
        n -- number of steps
        Returns:
        Tensor of shape [B, n, N, d] containing the dynamics over n steps
        """
        B, N, d = x.shape
        # Initialize the tensor to store the states over n steps
        x_new = torch.zeros(n, B, N, d, device=x.device)
        x_new_temp = x.clone()

        # Iterate and evolve the system for n steps
        if bar==True:
            for t in range(n):
                x_new_temp = self.dyn_step(x_new_temp, a)
                x_new[t] = x_new_temp
        else:
            for t in range(n):
                x_new_temp = self.dyn_step(x_new_temp, a)
                x_new[t] = x_new_temp

        return x_new.permute(1, 0, 2, 3)

    def Z_i_mu_func(self, y_i_mu, lambd, r=1):
        if self.d == 1:
            Z_i_mu = 2*torch.cosh(lambd*r*y_i_mu)  # [M, N]

        else:
            # Compute Z_i^mu for spherical spins
            nu = (self.d - 2) / 2  # Order of the Bessel function
            Z_i_mu = (2 * torch.pi ** (self.d / 2) * self.r ** (self.d - 1) /
                sp.special.gamma((self.d - 1) / 2)) * \
                (2 / (lambd * y_i_mu)) ** nu * \
                BesselIvFunction.apply(torch.tensor(nu), self.r * lambd * y_i_mu)  # [M, N]
        return Z_i_mu


    def forward(self, xi_batch, lambd, alpha=None, i_rand=None, r=1, l2=False):
         #* self.mask.to(device)
        diagonal = self.J.data.diagonal(dim1=0, dim2=1)  # Get diagonal elements
        diagonal.fill_(0)
        J_masked = self.J
        if i_rand is not None:
            J_x = torch.einsum('jab,mjb->ma', J_masked[i_rand], xi_batch)  # [M, d]
            y_i_mu = J_x.norm(dim=-1)  # Taking the norm over the last dimension -> [M]
            x_J_x = torch.einsum('ma,ma->m', xi_batch[:,i_rand], J_x)  # [M, N]

            # Compute the energy term for each mu: - dot_product + lam^-1 * log(Z_i_mu)
            if alpha==None:
                energy_i_mu = -x_J_x + (1 / lambd) * torch.log(self.Z_i_mu_func(y_i_mu,lambd,r))  # [M]
            else:
                energy_i_mu = -x_J_x + (1 / lambd) * alpha * y_i_mu**2

        else:
            J_x = torch.einsum('ijab,mjb->mia', J_masked, xi_batch)   # [M, d]
            y_i_mu = J_x.norm(dim=-1)  # Taking the norm over the last dimension -> [M,N]
            x_J_x = torch.einsum('mia,mia->mi', xi_batch, J_x)  # [M, N]
            # Compute the energy term for each mu: - dot_product + lam^-1 * log(Z_i_mu)
                        # Compute the energy term for each mu: - dot_product + lam^-1 * log(Z_i_mu)
            if alpha==None:
                energy_i_mu = -x_J_x + (1 / lambd) * torch.log(self.Z_i_mu_func(y_i_mu,lambd,r)+1e-9)  # [M,N]
            else:
                energy_i_mu = -x_J_x + (1 / lambd) * alpha * y_i_mu**2
            energy_i_mu = energy_i_mu.mean(dim=1)

        if l2==False:
            return energy_i_mu.mean(dim=0)
        else:
            return -x_J_x.mean() + alpha*(self.J.data**2).mean()


class CustomDataset(Dataset):
    def __init__(self, P, N, D, d, sigma, seed=None, on_sphere=True, coefficients="gaussian"):
        """
        P: Number of patterns
        N: Number of sites
        D: Number of features in the random field
        d: Dimensionality of each site
        sigma: Standard deviation of the Gaussian noise
        on_sphere: If True, normalize data to lie on a sphere
        coefficients: Type of coefficients ('binary' or 'gaussian')
        """
        self.P = P
        self.N = N
        self.D = D
        self.d = d
        self.sigma = sigma
        self.on_sphere = on_sphere
        self.coefficients = coefficients

        if seed is not None:
            torch.manual_seed(seed)

        # xi of shape [P, N, d]
        self.xi = torch.randn(P, N, d) * sigma

        # If D > 0, modify xi using the RF method
        if self.D > 0:
            self.RF()

        # Normalize xi along last dimension if on_sphere is True
        if self.on_sphere:
            self.xi = self.normalize(self.xi)

    def RF(self, seed=None):
        if seed is not None:
            torch.manual_seed(seed)
        # Create tensor f of shape [D, N, d] with random Gaussian numbers (std = sigma)
        self.f = torch.randint(0, 2, (self.D, self.N, self.d)).float() * 2 - 1

        # Normalize f along the last dimension if on_sphere is True
        if self.on_sphere:
            self.f = self.normalize(self.f)

        # Create tensor c of shape [P, D] based on coefficient type
        if self.coefficients == "binary":
            self.c = torch.randint(0, 2, (self.P, self.D)).float() * 2 - 1  # Random +1 or -1
        elif self.coefficients == "gaussian":
            self.c = torch.randn(self.P, self.D)  # Gaussian random numbers
        else:
            raise ValueError("coefficients must be 'binary' or 'gaussian'")

        # Divide c by sqrt(D)
        self.c = self.c / math.sqrt(self.D)

        # Update xi: xi[p] = sum_{k=0}^D c[p,k] * f[k]
        self.xi = torch.einsum('pk,kia->pia', self.c, self.f)
        # Create a mask where self.xi == 0
        mask = self.xi == 0

        # Generate random binary values (-1 or +1) for the positions where mask is True
        random_binary_values = torch.randint(0, 2, self.xi.shape, device=self.xi.device) * 2 - 1
        
        # Assign the random binary values to positions where self.xi == 0
        self.xi = torch.where(mask, random_binary_values, self.xi)
        if self.on_sphere:
            self.xi = self.normalize(self.xi)

    def get_generalization(self, P_hat, L=None):  #P_hat number of generalization vectors to get, L number of features to use
        if L==None:
            L = self.D
        if L>self.D:
            raise ValueError("L must be less than or equal to D")

        # Create tensor c of shape [P, N] based on coefficient type
        if self.coefficients == "binary":
            self.c = torch.randint(0, 2, (P_hat, self.D)).float() * 2 - 1  # Random +1 or -1
        elif self.coefficients == "gaussian":
            self.c = torch.randn(P_hat, self.D)  # Gaussian random numbers
        else:
            raise ValueError("coefficients must be 'binary' or 'gaussian'")

        # Divide c by sqrt(D)
        self.c = self.c / math.sqrt(self.D)

        #For each row of self.c, set to zero self.D-L random entries among the self.D. For each row, different random entries to set to zero
        indices_to_zero = torch.rand(self.P, self.D)
        indices_to_zero = indices_to_zero.argsort(dim=1)[:,:self.D-L]
        self.c = self.c.scatter(1, indices_to_zero, 0)

        # xi_new: xi[p] = sum_{k=0}^D c[p,k] * f[k]
        self.xi_new = torch.einsum('pk,kia->pia', self.c, self.f)
        self.xi_new = self.normalize(self.xi_new)
        return self.xi_new


    def normalize(self, x):
        # Normalize each d-dimensional vector in x along the last dimension
        norms = x.norm(dim=-1, keepdim=True)+1e-9
        return x / norms

    def __len__(self):
        # Return the number of patterns P
        return self.P

    def __getitem__(self, index):
        # Return the pattern xi at the given index
        return self.xi[index]

class DatasetF(Dataset):
    def __init__(self, D, f):
        self.D = D
        self.f = f

    def __len__(self):
        return self.D

    def __getitem__(self, index):
        return self.f[index]

"""# Lambda analysis


"""### Training GD"""
def main(N, alpha_P, alpha_D, l, D, d, on_sphere, init_overlap, n, device, data_PATH, ALPHA):
    P = int(alpha_P * N)
    D = int(alpha_D * N)
    P_generalization=1000
    print("P={}, D={}, lambda={}".format(P, D, l))
    model_name_base = "GD_capacity_N_{}_P_{}_D{}_l_{}_".format(N, P, D, l)
    
    if ALPHA is None:
        model_name = "GD_capacity_N_{}_P_{}_D{}_l_{}_10kepochs_lr100.pth".format(N, P, D, l, d)
    else:
        model_name = "GD_capacity_N_{}_P_{}_D{}_l_{}_A_{}_10kepochs_lr100.pth".format(N, P, D, l, d, ALPHA)

    torch.cuda.empty_cache()
    gc.collect()
    on_sphere=True

    dataset, model = initialize(N, P, D, d, on_sphere, l, device)
    dataset_f = DatasetF(D, dataset.f)
    xi_generalization = dataset.get_generalization(P_generalization)
    dataset_generalization = DatasetF(P_generalization, xi_generalization)

    model2 = TwoBodiesModel(N, d, on_sphere)
    model2.to(device)
    model2.Hebb(dataset.xi, 'Tensorial')  # Applying the Hebb rule
    J2 = model2.J.squeeze().cpu().detach().numpy()
    norm_J2 = np.linalg.norm(J2)

    batch_size = P
    batch_size_f = D

    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)
    dataloader_f = torch.utils.data.DataLoader(dataset_f, batch_size=batch_size_f, shuffle=False, drop_last=False, num_workers=2)
    dataloader_generalization = torch.utils.data.DataLoader(dataset_generalization, batch_size=P_generalization, shuffle=False, drop_last=False, num_workers=2)
    

    epochs = 401
    learning_rate = 1000
    valid_every = 10
    max_norm = 20
    epochs_to_save = [1000]
    save = False

    fake_opt = torch.optim.SGD(model.parameters(), lr=learning_rate)

    print("epochs{} lr{} max_norm{} init_overlap{} n{} l{}".format(epochs, learning_rate, max_norm, init_overlap, n, l))
    

    # Train the model
    hist_loss, hist_vloss, hist_asymm, hist_diff, hist_J_norm = train_model(
        model, dataloader, dataloader_f,dataloader_generalization, epochs, learning_rate, max_norm, device, data_PATH, model_name, init_overlap, n, l, fake_opt, J2, norm_J2, valid_every, ALPHA, epochs_to_save, model_name_base, save,
    )

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Training GD")

    # Define all the parameters
    parser.add_argument("--N", type=int, required=True)
    parser.add_argument("--alpha_P", type=float, required=True)
    parser.add_argument("--alpha_D", type=float, required=True)
    parser.add_argument("--l", type=float, required=True)
    parser.add_argument("--D", type=int, default=0)
    parser.add_argument("--d", type=int, default=1)
    parser.add_argument("--on_sphere", type=bool, default=True)
    parser.add_argument("--init_overlap", type=float, default=1.0)
    parser.add_argument("--n", type=int, default=10)
    parser.add_argument("--device", type=str, default="cpu")
    parser.add_argument("--data_PATH", type=str, required=True)
    parser.add_argument("--ALPHA", type=float, default=None)

    args = parser.parse_args()

    # Run the main function with the parsed arguments
    main(args.N, args.alpha_P, args.alpha_D, args.l, args.D, args.d, args.on_sphere, args.init_overlap, args.n, args.device, args.data_PATH, args.ALPHA)