{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# path to the directory that CONTAINS the package folder\n",
    "project_root = Path(\"/home/benedetti/PL\")   # adjust to your real path\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PL.utils.saving import load_training\n",
    "\n",
    "from PL.training.training_classifier import train_model, initialize\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "data_PATH = \"/home/benedetti/VecPerc/data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=250, P=250\n"
     ]
    }
   ],
   "source": [
    "# Nature of variables and loss\n",
    "d=4\n",
    "spin_type = \"vector\"\n",
    "loss_type = \"CE\"\n",
    "init_Hebb = True\n",
    "fixed_norm = False\n",
    "l2 = None\n",
    "\n",
    "# Dataset\n",
    "alpha_P = 1.\n",
    "N=250\n",
    "sigma = 1.\n",
    "P = int(alpha_P * N)\n",
    "print(\"N={}, P={}\".format(N,P))\n",
    "\n",
    "# Model and training\n",
    "l=1.    \n",
    "downf=0.01\n",
    "gamma=0.0\n",
    "lr = 0.001\n",
    "\n",
    "epochs=100\n",
    "valid_every = 10\n",
    "max_grad = 20.\n",
    "batch_size = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, model, optimizer = initialize(N, P, d, lr, spin_type,device, gamma, init_Hebb=init_Hebb, downf=downf)\n",
    "J2 = model.J.data.clone().cpu()\n",
    "norm_J2 = torch.norm(J2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:1000  lr:0.01  max_norm:20.0  l:1.0\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,          # important for fast H2D\n",
    "    persistent_workers=True,  # avoids worker respawn overhead\n",
    "    prefetch_factor=2,        # default is 2 when num_workers>0\n",
    ")\n",
    "\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "epochs_to_save = [10,20,40,200]\n",
    "save = True\n",
    "print(\"epochs:{}  lr:{}  max_norm:{}  l:{}\".format(epochs, lr, max_grad, l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_CE_GD_TEACHER_fixedFalse_N_250_P_250_l_1.0_epochs1000_lr0.01_l2None.pth\n"
     ]
    }
   ],
   "source": [
    "model_name_base = \"{}_{}_GD_TEACHER_fixed{}_N_{}_P_{}_l_{}_epochs{}_lr{}_l2{}\".format(spin_type, loss_type, fixed_norm, N, P, l, epochs, lr, l2)\n",
    "model_name = model_name_base + \".pth\"\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch norm train_loss learning_rate train_metric R\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.3718712627887726 2.3172829151153564 0.01 0.9585466980934143 0.4322293996810913\n",
      "40 0.6058017015457153 1.9084330797195435 0.01 0.9692817330360413 0.43833285570144653\n",
      "60 0.7946677803993225 1.6331883668899536 0.01 0.9757778644561768 0.4421040117740631\n",
      "80 0.9570391178131104 1.427817940711975 0.01 0.980095386505127 0.4446665048599243\n",
      "100 1.101583480834961 1.2646045684814453 0.01 0.9831610918045044 0.4465242922306061\n",
      "120 1.233109951019287 1.1294084787368774 0.01 0.9854447841644287 0.4479347765445709\n",
      "140 1.354600429534912 1.0141229629516602 0.01 0.9872085452079773 0.4490426480770111\n",
      "160 1.468050479888916 0.9136911630630493 0.01 0.9886098504066467 0.44993630051612854\n",
      "180 1.5748720169067383 0.8247545957565308 0.01 0.9897487163543701 0.4506722688674927\n",
      "200 1.6761069297790527 0.7449724674224854 0.01 0.9906914830207825 0.45128902792930603\n",
      "220 1.7725497484207153 0.6726498007774353 0.01 0.9914840459823608 0.45181310176849365\n",
      "240 1.8648229837417603 0.6065205335617065 0.01 0.9921592473983765 0.4522639214992523\n",
      "260 1.9534249305725098 0.5456146597862244 0.01 0.99274080991745 0.4526556730270386\n",
      "280 2.038761854171753 0.48917242884635925 0.01 0.9932467937469482 0.45299920439720154\n",
      "300 2.1211700439453125 0.4365882873535156 0.01 0.993690550327301 0.4533027708530426\n",
      "320 2.200932025909424 0.38737165927886963 0.01 0.9940828084945679 0.4535728991031647\n",
      "340 2.278287172317505 0.3411196768283844 0.01 0.9944319725036621 0.45381465554237366\n",
      "360 2.353440999984741 0.29749739170074463 0.01 0.9947444200515747 0.45403239130973816\n",
      "380 2.4265713691711426 0.25622352957725525 0.01 0.9950256943702698 0.4542293846607208\n",
      "400 2.497833490371704 0.21705955266952515 0.01 0.9952800869941711 0.45440834760665894\n",
      "420 2.5673630237579346 0.17980121076107025 0.01 0.9955112338066101 0.4545716941356659\n",
      "440 2.6352810859680176 0.14427274465560913 0.01 0.9957220554351807 0.45472124218940735\n",
      "460 2.7016942501068115 0.11032134294509888 0.01 0.9959151148796082 0.45485877990722656\n",
      "480 2.7666983604431152 0.07781372219324112 0.01 0.9960924386978149 0.4549855589866638\n",
      "500 2.8303794860839844 0.04663270711898804 0.01 0.9962559342384338 0.4551027715206146\n",
      "520 2.892814874649048 0.016674667596817017 0.01 0.9964070320129395 0.45521149039268494\n",
      "540 2.9540748596191406 -0.012152141891419888 0.01 0.9965471029281616 0.4553125500679016\n",
      "560 3.01422381401062 -0.03992972895503044 0.01 0.9966772198677063 0.45540672540664673\n",
      "580 3.07331919670105 -0.06673146039247513 0.01 0.9967985153198242 0.4554947316646576\n",
      "600 3.131415367126465 -0.09262321889400482 0.01 0.9969118237495422 0.45557695627212524\n",
      "620 3.1885604858398438 -0.11766446381807327 0.01 0.9970176219940186 0.455654114484787\n",
      "640 3.244800329208374 -0.14190919697284698 0.01 0.9971169829368591 0.45572659373283386\n",
      "660 3.300175666809082 -0.16540606319904327 0.01 0.9972102642059326 0.4557948410511017\n",
      "680 3.3547263145446777 -0.18819980323314667 0.01 0.997298002243042 0.45585912466049194\n",
      "700 3.40848708152771 -0.21033115684986115 0.01 0.9973806738853455 0.4559198021888733\n",
      "720 3.461491346359253 -0.2318372279405594 0.01 0.9974586963653564 0.45597726106643677\n",
      "740 3.513770580291748 -0.25275230407714844 0.01 0.9975324869155884 0.45603156089782715\n",
      "760 3.5653536319732666 -0.2731078267097473 0.01 0.9976022839546204 0.4560830295085907\n",
      "780 3.616266965866089 -0.2929328978061676 0.01 0.9976685047149658 0.4561319649219513\n",
      "800 3.666536569595337 -0.312254399061203 0.01 0.9977313280105591 0.4561784267425537\n",
      "820 3.7161858081817627 -0.3310970067977905 0.01 0.9977909326553345 0.45622265338897705\n",
      "840 3.765237331390381 -0.3494839668273926 0.01 0.997847855091095 0.45626476407051086\n",
      "860 3.8137123584747314 -0.36743658781051636 0.01 0.9979019165039062 0.4563049077987671\n",
      "880 3.8616302013397217 -0.3849748373031616 0.01 0.9979535341262817 0.4563432037830353\n",
      "900 3.9090099334716797 -0.402117520570755 0.01 0.9980027675628662 0.45637980103492737\n",
      "920 3.95586895942688 -0.4188818633556366 0.01 0.9980497360229492 0.4564148783683777\n",
      "940 4.002224922180176 -0.4352841079235077 0.01 0.9980947971343994 0.4564483165740967\n",
      "960 4.048092365264893 -0.45133957266807556 0.01 0.9981377720832825 0.4564804136753082\n",
      "980 4.093486785888672 -0.467062771320343 0.01 0.998179018497467 0.4565112292766571\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model, fixed_norm, dataset, dataloader, epochs, \n",
    "    lr, max_grad, device, data_PATH, l, optimizer, J2, \n",
    "    norm_J2, valid_every, epochs_to_save, model_name_base, save, l2, loss_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=250, P=250\n"
     ]
    }
   ],
   "source": [
    "# Nature of variables and loss\n",
    "spin_type = \"vector\"\n",
    "loss_type = \"CE\"\n",
    "init_Hebb = True\n",
    "fixed_norm = True\n",
    "l2 = None\n",
    "\n",
    "# Dataset\n",
    "alpha_P = 1.\n",
    "N=250\n",
    "sigma = 1.\n",
    "P = int(alpha_P * N)\n",
    "print(\"N={}, P={}\".format(N,P))\n",
    "\n",
    "# Model and training   \n",
    "downf=1.\n",
    "gamma=0.0\n",
    "lr = 1.\n",
    "epochs=100\n",
    "valid_every = 50\n",
    "max_grad = 20.\n",
    "batch_size = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_arr = np.array([0.1,0.2,0.4,0.8,1.6,3.2,6.4])\n",
    "#l_arr = np.array([1.])\n",
    "d_arr = np.array([4,8,16,32])\n",
    "alpha_P_arr = np.array([0.5, 1., 2., 4., 8., 16.])\n",
    "seeds_arr = np.arange(100,105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 24.84552764892578 1.0 0.9795817732810974 0.32115504145622253\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 9.91299057006836 1.0 0.9881325960159302 0.32459861040115356\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 3.1222832202911377 1.0 0.994861900806427 0.32731521129608154\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.44964656233787537 1.0 0.9982433915138245 0.32869723439216614\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.38010019063949585 1.0 0.9994816780090332 0.3292126953601837\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 -0.3659989833831787 1.0 0.9755575060844421 0.3229447603225708\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.034025758504867554 1.0 0.9351913928985596 0.3135838508605957\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 26.28374671936035 1.0 0.9598166942596436 0.4358828067779541\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 11.154642105102539 1.0 0.9726051688194275 0.44425925612449646\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.004931926727295 1.0 0.9855921864509583 0.45296820998191833\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.979523777961731 1.0 0.9941436648368835 0.45891207456588745\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.09013736993074417 1.0 0.9980413317680359 0.4617396593093872\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.352411687374115 1.0 0.9994232654571533 0.4627889394760132\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.3342428207397461 1.0 0.9998427033424377 0.4631255567073822\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 27.251466751098633 1.0 0.9342548251152039 0.5660180449485779\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.036623001098633 1.0 0.9497774243354797 0.5813277363777161\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.690676212310791 1.0 0.9684659242630005 0.6013361811637878\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 1.426232099533081 1.0 0.9840918183326721 0.6202230453491211\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.16961854696273804 1.0 0.9933752417564392 0.6332802772521973\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.21036005020141602 1.0 0.9976425766944885 0.6402312517166138\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.2593458294868469 1.0 0.9992603659629822 0.6431826949119568\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.000000238418579 27.878982543945312 1.0 0.9154564142227173 0.7060474753379822\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.620574951171875 1.0 0.9309375286102295 0.7257947325706482\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.164468765258789 1.0 0.951321005821228 0.7554793953895569\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.7492856979370117 1.0 0.9709948301315308 0.7903162240982056\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.36536359786987305 1.0 0.9851037263870239 0.8227090239524841\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.09825189411640167 1.0 0.9931636452674866 0.8482456207275391\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.19704851508140564 1.0 0.9971200823783875 0.8660871982574463\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.283851623535156 1.0 0.9147781729698181 0.8229541778564453\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 13.002357482910156 1.0 0.9282786846160889 0.8408972024917603\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.480474948883057 1.0 0.9474316239356995 0.868915319442749\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.964063286781311 1.0 0.967469334602356 0.9033904075622559\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.4895983636379242 1.0 0.9827871322631836 0.9364611506462097\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.03216323256492615 1.0 0.9919566512107849 0.9625106453895569\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.16311602294445038 1.0 0.9965970516204834 0.9801353216171265\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 28.508485794067383 1.0 0.9270480275154114 0.8970308899879456\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 13.21558666229248 1.0 0.9381585717201233 0.9108186960220337\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.659123420715332 1.0 0.9546228051185608 0.931962788105011\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 2.0848090648651123 1.0 0.9726917743682861 0.9565140604972839\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.5552575588226318 1.0 0.9866383075714111 0.9771058559417725\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.000000238418579 -0.0017560155829414725 1.0 0.994530439376831 0.9899739027023315\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.1505625694990158 1.0 0.9980655908584595 0.9962710738182068\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 24.244138717651367 1.0 0.9830047488212585 0.23446285724639893\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 7.498340129852295 1.0 0.9885250926017761 0.23591811954975128\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 0.15315966308116913 1.0 0.9942321181297302 0.23740914463996887\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -2.274935483932495 1.0 0.9978330135345459 0.23834434151649475\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -2.481470823287964 1.0 0.9993296265602112 0.23873016238212585\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.9561256170272827 1.0 0.999813437461853 0.2388528436422348\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.3465107679367065 1.0 0.9999508261680603 0.23888647556304932\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 27.0634708404541 1.0 0.9680330753326416 0.33143535256385803\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 9.958223342895508 1.0 0.9756377339363098 0.33461856842041016\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 2.0127756595611572 1.0 0.9854040145874023 0.338699609041214\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.0912178754806519 1.0 0.993502140045166 0.3420940041542053\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.8131517171859741 1.0 0.9977367520332336 0.34388214349746704\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.6015403270721436 1.0 0.9993244409561157 0.3445568084716797\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.1639759540557861 1.0 0.9998149871826172 0.3447655737400055\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 29.08226776123047 1.0 0.9437610507011414 0.44977620244026184\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 11.793158531188965 1.0 0.9529983997344971 0.4559279978275299\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 3.510282039642334 1.0 0.9669491052627563 0.4654066264629364\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -0.05147375911474228 1.0 0.981935977935791 0.4759354889392853\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.1870959997177124 1.0 0.9923769235610962 0.4836089015007019\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.2558091878890991 1.0 0.9973729252815247 0.4874400496482849\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9818206429481506 1.0 0.9992106556892395 0.48888933658599854\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 30.39668083190918 1.0 0.919119656085968 0.5808674097061157\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.01595401763916 1.0 0.9285345673561096 0.5901005268096924\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 4.558460235595703 1.0 0.944014310836792 0.6060891151428223\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 0.729875922203064 1.0 0.963644802570343 0.6283087730407715\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6833755970001221 1.0 0.9809268116950989 0.6507415771484375\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -0.9612300395965576 1.0 0.9917387366294861 0.6672146916389465\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.8194295763969421 1.0 0.9969581961631775 0.6764354705810547\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 31.229923248291016 1.0 0.9043324589729309 0.7104926705360413\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.800182342529297 1.0 0.9128553867340088 0.7215286493301392\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 5.247941017150879 1.0 0.927564799785614 0.7417352199554443\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 1.2644370794296265 1.0 0.9480841159820557 0.7732948064804077\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.3259275257587433 1.0 0.9687215685844421 0.8117843270301819\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.7452273964881897 1.0 0.9837229251861572 0.8487805128097534\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6953779458999634 1.0 0.9923698902130127 0.8788868188858032\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 31.74787139892578 1.0 0.9062448740005493 0.8174178600311279\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 14.289591789245605 1.0 0.9135348200798035 0.8276330828666687\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 5.681799411773682 1.0 0.926572322845459 0.8465995192527771\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 1.603492021560669 1.0 0.9460088610649109 0.8769721984863281\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.10326414555311203 1.0 0.9671698212623596 0.9143863916397095\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6191596388816833 1.0 0.983361005783081 0.948689877986908\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6303223967552185 1.0 0.9927276968955994 0.9733507633209229\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -7.89609432220459 1.0 0.9905114769935608 0.1731090545654297\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -13.0294771194458 1.0 0.9932572245597839 0.17363369464874268\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -13.685361862182617 1.0 0.9963924288749695 0.17422817647457123\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -11.656396865844727 1.0 0.998582661151886 0.17464078962802887\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -8.638943672180176 1.0 0.9995517730712891 0.17482255399227142\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -5.833827495574951 1.0 0.9998740553855896 0.17488275468349457\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.701678514480591 1.0 0.9999666810035706 0.17489996552467346\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.1982779502868652 1.0 0.9799244999885559 0.24433687329292297\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -8.048272132873535 1.0 0.9841635227203369 0.24555793404579163\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -9.859804153442383 1.0 0.9900335073471069 0.2472408562898636\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -9.16714859008789 1.0 0.995385468006134 0.24876883625984192\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.213302135467529 1.0 0.998367965221405 0.2496189922094345\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.072192668914795 1.0 0.9995119571685791 0.24994513392448425\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.308342695236206 1.0 0.9998665452003479 0.25004640221595764\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.9114848375320435 1.0 0.9617420434951782 0.33672553300857544\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.311474800109863 1.0 0.9673999547958374 0.33917170763015747\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.779294490814209 1.0 0.9764015674591064 0.34307703375816345\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.987036228179932 1.0 0.9868599772453308 0.3476453125476837\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.884557247161865 1.0 0.9945266246795654 0.35103389620780945\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.336421966552734 1.0 0.9981727004051208 0.35266730189323425\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.921071767807007 1.0 0.9994670152664185 0.35325437784194946\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 4.736882209777832 1.0 0.9377425312995911 0.44850167632102966\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.6763200759887695 1.0 0.943946361541748 0.45266401767730713\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.493858814239502 1.0 0.9547759890556335 0.46006327867507935\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.241127967834473 1.0 0.9699954390525818 0.4707893431186676\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.738275527954102 1.0 0.984649121761322 0.48159533739089966\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.6666109561920166 1.0 0.9938574433326721 0.48873794078826904\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.5563008785247803 1.0 0.9979584217071533 0.492055743932724\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 6.665920257568359 1.0 0.9118458032608032 0.5787515640258789\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 0.1518862098455429 1.0 0.9178654551506042 0.5848749876022339\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.8552134037017822 1.0 0.9289133548736572 0.5964816808700562\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.916275978088379 1.0 0.9463524222373962 0.6159538626670837\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -3.8068385124206543 1.0 0.9668231010437012 0.6412960290908813\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.0864205360412598 1.0 0.9833563566207886 0.6649435758590698\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.2225112915039062 1.0 0.9930365085601807 0.6811846494674683\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 7.880233287811279 1.0 0.9003477096557617 0.7087393999099731\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.311264991760254 1.0 0.9055800437927246 0.7157031893730164\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.800178050994873 1.0 0.9154069423675537 0.7292554378509521\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.040600299835205 1.0 0.9318482875823975 0.7535949349403381\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.173166275024414 1.0 0.9533021450042725 0.7898193597793579\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.682966947555542 1.0 0.9730800986289978 0.8311788439750671\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.9844563007354736 1.0 0.9864224791526794 0.868696928024292\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -131.1717071533203 1.0 0.9945612549781799 0.12437552958726883\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -83.67924499511719 1.0 0.9960881471633911 0.1245705783367157\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.20173645019531 1.0 0.9978759288787842 0.1247984990477562\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -37.839378356933594 1.0 0.9991587996482849 0.12496189773082733\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.65685510635376 -24.659778594970703 1.0 0.9997338056564331 0.12503503262996674\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -15.440499305725098 1.0 0.9999253153800964 0.1250593066215515\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.337141036987305 1.0 0.9999802112579346 0.12506619095802307\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -119.683349609375 1.0 0.988659143447876 0.1749756783246994\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -73.63270568847656 1.0 0.9909677505493164 0.17541204392910004\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -48.45619201660156 1.0 0.9942358732223511 0.1760285496711731\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -32.76502990722656 1.0 0.9973068237304688 0.17660699784755707\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -21.739177703857422 1.0 0.9990482926368713 0.17693476378917694\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -13.876440048217773 1.0 0.9997166395187378 0.17706049978733063\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -8.528547286987305 1.0 0.9999228119850159 0.17709921300411224\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -111.29376983642578 1.0 0.9778655767440796 0.24268850684165955\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -66.00042724609375 1.0 0.9810296893119812 0.24357379972934723\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -42.148258209228516 1.0 0.9861487746238708 0.24500688910484314\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -28.278759002685547 1.0 0.9922651648521423 0.24672140181064606\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -18.997140884399414 1.0 0.9968188405036926 0.24800163507461548\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.358469009399414 1.0 0.9989591240882874 0.248605415225029\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -7.730485916137695 1.0 0.9997014403343201 0.24881525337696075\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -105.35962677001953 1.0 0.9597153663635254 0.3330061435699463\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -60.45248031616211 1.0 0.9636486172676086 0.3346816599369049\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -37.3126220703125 1.0 0.9705929756164551 0.33765754103660583\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.55698013305664 1.0 0.9805976152420044 0.34198838472366333\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -16.545927047729492 1.0 0.9903846979141235 0.34628692269325256\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -10.933023452758789 1.0 0.9963611960411072 0.3489527702331543\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.96006441116333 1.0 0.9988560676574707 0.3500789403915405\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -101.21730041503906 1.0 0.933806300163269 0.4476606845855713\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.50887680053711 1.0 0.9381269812583923 0.45055729150772095\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -33.74575424194336 1.0 0.9461734890937805 0.4560231566429138\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -21.628801345825195 1.0 0.9593459367752075 0.4651900827884674\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -14.45988655090332 1.0 0.9755561351776123 0.4769235849380493\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.637121200561523 1.0 0.9886968731880188 0.4869295358657837\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.227177143096924 1.0 0.9958200454711914 0.4926247298717499\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -98.454833984375 1.0 0.9091777801513672 0.5774928331375122\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -53.850589752197266 1.0 0.9132450222969055 0.5816205143928528\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -31.287290573120117 1.0 0.9210295677185059 0.5896924138069153\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -19.52362632751465 1.0 0.934715986251831 0.6045002341270447\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.865741729736328 1.0 0.9542564153671265 0.6273686289787292\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -8.580914497375488 1.0 0.9740029573440552 0.653542160987854\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 seed:100 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -5.593498229980469 1.0 0.9878582954406738 0.6750699281692505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [15:16, 916.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 24.873151779174805 1.0 0.9782216548919678 0.3321034610271454\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 9.938603401184082 1.0 0.9873107671737671 0.3358974754810333\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 3.141707420349121 1.0 0.9945112466812134 0.3389332592487335\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.46176233887672424 1.0 0.9981290102005005 0.340496689081192\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.37254148721694946 1.0 0.9992867708206177 0.3410579562187195\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.28144511580467224 1.0 0.9619495868682861 0.33260631561279297\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.04409150779247284 1.0 0.9224909543991089 0.3236119747161865\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 26.28083038330078 1.0 0.96090167760849 0.44449934363365173\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 11.150712966918945 1.0 0.9733048677444458 0.45272719860076904\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.00063943862915 1.0 0.9858946204185486 0.4612632393836975\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.9764495491981506 1.0 0.994220495223999 0.4671146869659424\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.09183242172002792 1.0 0.9980526566505432 0.4698856770992279\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.353262335062027 1.0 0.9994240403175354 0.4708797037601471\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.3346618115901947 1.0 0.9998424649238586 0.47117340564727783\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 27.24517822265625 1.0 0.9339531064033508 0.5787388682365417\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.027535438537598 1.0 0.949108898639679 0.5940484404563904\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 4.680181980133057 1.0 0.9678217768669128 0.6143301725387573\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.4185807704925537 1.0 0.9837640523910522 0.6334951519966125\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.1652989238500595 1.0 0.9933105111122131 0.6464723348617554\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.21272538602352142 1.0 0.997664213180542 0.6531105041503906\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 -0.2606600522994995 1.0 0.9992811679840088 0.655815601348877\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 27.88750457763672 1.0 0.9153226613998413 0.7141402959823608\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.627897262573242 1.0 0.9304780960083008 0.7339920401573181\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.169516563415527 1.0 0.9511056542396545 0.7639601826667786\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.7520440816879272 1.0 0.9712356925010681 0.7990089654922485\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.3663592040538788 1.0 0.9854000210762024 0.8315288424491882\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 -0.09811749309301376 1.0 0.9933517575263977 0.8573103547096252\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.19717435538768768 1.0 0.9972285628318787 0.875273585319519\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.29062843322754 1.0 0.9107867479324341 0.8201243281364441\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 13.009196281433105 1.0 0.9248896241188049 0.8393839001655579\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.487391471862793 1.0 0.9450795650482178 0.8692735433578491\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.970559000968933 1.0 0.9663750529289246 0.9053485989570618\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.49436110258102417 1.0 0.9825363159179688 0.9389441013336182\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.029458273202180862 1.0 0.9920049905776978 0.9646681547164917\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.16183283925056458 1.0 0.9966852068901062 0.9816932082176208\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.50670051574707 1.0 0.9247820377349854 0.8948769569396973\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 13.214287757873535 1.0 0.9366439580917358 0.9092798233032227\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.658653736114502 1.0 0.9539709091186523 0.9311420917510986\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 2.0848209857940674 1.0 0.9726273417472839 0.9561968445777893\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 0.5552250742912292 1.0 0.9867554903030396 0.9769831299781799\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.0018819746328517795 1.0 0.994615375995636 0.9898908734321594\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.1506807655096054 1.0 0.9980981349945068 0.9962137937545776\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 24.1572322845459 1.0 0.9842768311500549 0.24397173523902893\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 7.423940658569336 1.0 0.9893514513969421 0.24542924761772156\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 0.09861690551042557 1.0 0.9946220517158508 0.24692796170711517\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -2.308727741241455 1.0 0.9979676008224487 0.2478777915239334\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -2.500227928161621 1.0 0.9993680715560913 0.24827681481838226\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.9659795761108398 1.0 0.9998235106468201 0.24840715527534485\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.351563572883606 1.0 0.9999535083770752 0.24844451248645782\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 27.003982543945312 1.0 0.9685022830963135 0.33504167199134827\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 9.906264305114746 1.0 0.975890040397644 0.3383360207080841\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 1.9735372066497803 1.0 0.985450029373169 0.3425975441932678\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -1.1157797574996948 1.0 0.9934625029563904 0.3462001085281372\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.8265763521194458 1.0 0.9977036714553833 0.34814584255218506\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.608444333076477 1.0 0.9993104934692383 0.348906010389328\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.1674540042877197 1.0 0.9998104572296143 0.3491540253162384\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 29.03271484375 1.0 0.9443365335464478 0.4484901428222656\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 11.747865676879883 1.0 0.9534914493560791 0.4546402394771576\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 3.4729132652282715 1.0 0.967301607131958 0.4641256034374237\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.07758975774049759 1.0 0.9821204543113708 0.47469064593315125\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.2026498317718506 1.0 0.9924484491348267 0.48242369294166565\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.2642346620559692 1.0 0.9973965883255005 0.4862987995147705\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9861904978752136 1.0 0.9992178678512573 0.48777076601982117\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 30.400815963745117 1.0 0.9184321761131287 0.58464115858078\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 13.020322799682617 1.0 0.9277327060699463 0.5940748453140259\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 4.56331729888916 1.0 0.9431308507919312 0.6104859113693237\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 0.7352948188781738 1.0 0.9628636240959167 0.6334205865859985\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.6782582402229309 1.0 0.9804299473762512 0.6566584706306458\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9573988318443298 1.0 0.9915051460266113 0.6737060546875\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.8170178532600403 1.0 0.9968714714050293 0.6831942796707153\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 31.259296417236328 1.0 0.9029201865196228 0.7142165899276733\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.828266143798828 1.0 0.9114992022514343 0.7254793643951416\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 5.273444652557373 1.0 0.9263632893562317 0.7461143136024475\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 1.2852694988250732 1.0 0.9472346901893616 0.7782954573631287\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.31147292256355286 1.0 0.9683310389518738 0.8172152638435364\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.7367180585861206 1.0 0.9836365580558777 0.8540514707565308\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.6909237504005432 1.0 0.9923874735832214 0.8835992813110352\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 31.77094268798828 1.0 0.910727858543396 0.8239518404006958\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 14.310811042785645 1.0 0.9177068471908569 0.8337555527687073\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 5.69943380355835 1.0 0.9301350712776184 0.8519340753555298\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 1.615188717842102 1.0 0.9485960006713867 0.8810399174690247\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.097990021109581 1.0 0.9686800837516785 0.9169529676437378\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6177831292152405 1.0 0.984073281288147 0.9499793648719788\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6303324103355408 1.0 0.9930038452148438 0.973858654499054\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.916467189788818 1.0 0.9901639223098755 0.1725415587425232\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -13.045373916625977 1.0 0.9930330514907837 0.17309632897377014\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -13.695635795593262 1.0 0.9962907433509827 0.17372149229049683\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -11.662057876586914 1.0 0.9985500574111938 0.17415256798267365\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -8.64185619354248 1.0 0.9995431900024414 0.17434218525886536\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -5.835299015045166 1.0 0.9998719692230225 0.1744055449962616\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.7024176120758057 1.0 0.9999661445617676 0.17442412674427032\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.2629497051239014 1.0 0.9797413945198059 0.23976542055606842\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -8.105560302734375 1.0 0.9840288758277893 0.24098831415176392\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -9.904715538024902 1.0 0.9899585843086243 0.2426748126745224\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -9.196810722351074 1.0 0.995353639125824 0.24420586228370667\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -7.230312347412109 1.0 0.998356282711029 0.24505949020385742\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -5.081241607666016 1.0 0.9995080828666687 0.24538929760456085\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -3.3129959106445312 1.0 0.9998653531074524 0.2454930990934372\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.9017821550369263 1.0 0.9620882272720337 0.33390048146247864\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.320979595184326 1.0 0.9676413536071777 0.3363048732280731\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.78804349899292 1.0 0.9765071272850037 0.34016239643096924\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -6.993862152099609 1.0 0.9868652820587158 0.3447040617465973\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -5.8888258934021 1.0 0.9945082068443298 0.34808897972106934\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -4.338731288909912 1.0 0.9981622099876404 0.349721223115921\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.9222517013549805 1.0 0.9994633793830872 0.3503049314022064\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 4.7641730308532715 1.0 0.9370593428611755 0.448567271232605\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.650264024734497 1.0 0.9433759450912476 0.4527475833892822\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.470327854156494 1.0 0.9543783068656921 0.4601651132106781\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.222235679626465 1.0 0.9697921872138977 0.47089046239852905\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.725536346435547 1.0 0.984582245349884 0.4816676080226898\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.65922474861145 1.0 0.9938439726829529 0.48877638578414917\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.5523617267608643 1.0 0.9979566335678101 0.4920772612094879\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 6.705171585083008 1.0 0.9114947319030762 0.5811991691589355\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 0.18928943574428558 1.0 0.9176405668258667 0.5873138308525085\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.821472644805908 1.0 0.9288793802261353 0.5988558530807495\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.8891818523406982 1.0 0.9465221762657166 0.6180995106697083\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.7889175415039062 1.0 0.9670912027359009 0.6429656147956848\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.0767252445220947 1.0 0.9835736751556396 0.6660259366035461\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.217867612838745 1.0 0.9931467175483704 0.6818325519561768\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 7.900821208953857 1.0 0.8988469839096069 0.709216833114624\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 1.331363558769226 1.0 0.9042127728462219 0.7162514328956604\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.7811248302459717 1.0 0.9142710566520691 0.7299115061759949\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.0237338542938232 1.0 0.9310492277145386 0.7543566226959229\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.160127639770508 1.0 0.9528589844703674 0.7905765175819397\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.6745002269744873 1.0 0.9728808403015137 0.8317870497703552\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -1.9796451330184937 1.0 0.9863343834877014 0.8691534399986267\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -131.2262420654297 1.0 0.9946490526199341 0.12411265820264816\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -83.72615814208984 1.0 0.9961446523666382 0.12430831044912338\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.65685510635376 -56.236846923828125 1.0 0.9979015588760376 0.12453744560480118\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -37.86164093017578 1.0 0.9991668462753296 0.12470215559005737\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.672317504882812 1.0 0.9997358322143555 0.124776192009449\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -15.447137832641602 1.0 0.9999257922172546 0.12480095028877258\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.340551376342773 1.0 0.9999803900718689 0.12480809539556503\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -119.70513916015625 1.0 0.9886959791183472 0.17413367331027985\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -73.65209197998047 1.0 0.9909945726394653 0.17457327246665955\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -48.471580505371094 1.0 0.9942499399185181 0.17519395053386688\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -32.77541732788086 1.0 0.9973113536834717 0.17577575147151947\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -21.745256423950195 1.0 0.9990491271018982 0.1761055737733841\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -13.879716873168945 1.0 0.9997167587280273 0.1762324720621109\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -8.530242919921875 1.0 0.9999228119850159 0.17627178132534027\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -111.2541275024414 1.0 0.9772752523422241 0.24135428667068481\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -65.96279907226562 1.0 0.980530321598053 0.24225832521915436\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -42.114768981933594 1.0 0.9857918620109558 0.2437194585800171\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -28.252473831176758 1.0 0.9920717477798462 0.245464026927948\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -18.979639053344727 1.0 0.996742844581604 0.24676352739334106\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -12.348258018493652 1.0 0.9989355206489563 0.24737457931041718\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -7.724969863891602 1.0 0.9996949434280396 0.24758632481098175\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -105.31075286865234 1.0 0.9589551687240601 0.33351975679397583\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -60.40559768676758 1.0 0.9629477858543396 0.3352336883544922\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -37.26971435546875 1.0 0.9700035452842712 0.3382774591445923\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -24.52129364013672 1.0 0.9801873564720154 0.342706561088562\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -16.520347595214844 1.0 0.9901743531227112 0.34710055589675903\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -10.917215347290039 1.0 0.9962827563285828 0.349820613861084\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.951231002807617 1.0 0.9988324642181396 0.35096582770347595\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -101.19074249267578 1.0 0.9340000152587891 0.44789421558380127\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -56.4836311340332 1.0 0.9383226037025452 0.4507797956466675\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -33.723026275634766 1.0 0.946366548538208 0.4562206566333771\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -21.61043930053711 1.0 0.9595200419425964 0.4653354585170746\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -14.44742202758789 1.0 0.9756847023963928 0.4769858717918396\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.630038261413574 1.0 0.9887688159942627 0.48690733313560486\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.223552703857422 1.0 0.9958507418632507 0.4925469756126404\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -98.42686462402344 1.0 0.9094097018241882 0.5793659687042236\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -53.82377243041992 1.0 0.9134865999221802 0.5834861397743225\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -31.262727737426758 1.0 0.9212833642959595 0.5915366411209106\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -19.50311851501465 1.0 0.9349744319915771 0.6062862873077393\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -12.851223945617676 1.0 0.954490065574646 0.6290283203125\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -8.572555541992188 1.0 0.9741703271865845 0.655021607875824\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 seed:101 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -5.589377403259277 1.0 0.9879465699195862 0.676394522190094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [30:28, 913.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 24.808406829833984 1.0 0.9789853692054749 0.32112276554107666\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 9.883188247680664 1.0 0.9878792762756348 0.32479023933410645\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.000000238418579 3.10323429107666 1.0 0.9948078393936157 0.32766464352607727\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.43907979130744934 1.0 0.9982420206069946 0.3291260600090027\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.382924348115921 1.0 0.9989174604415894 0.3295632004737854\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.1807931363582611 1.0 0.9448307156562805 0.3174987733364105\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.14164085686206818 1.0 0.908120334148407 0.30936482548713684\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 26.317235946655273 1.0 0.9534958004951477 0.44274917244911194\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 11.189566612243652 1.0 0.9683077931404114 0.45177343487739563\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.037099838256836 1.0 0.9834755063056946 0.46124836802482605\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.0027170181274414 1.0 0.9933822154998779 0.4676852226257324\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.07628648728132248 1.0 0.9978188872337341 0.4706796705722809\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.344902902841568 1.0 0.9993647336959839 0.4717487692832947\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.33032041788101196 1.0 0.9998218417167664 0.47206971049308777\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 27.284170150756836 1.0 0.9321842193603516 0.5742798447608948\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.065813064575195 1.0 0.9479585886001587 0.5893543362617493\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.71256685256958 1.0 0.967372715473175 0.6092544794082642\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.4398481845855713 1.0 0.9836902022361755 0.628096878528595\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.17694726586341858 1.0 0.9932942986488342 0.6410940885543823\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.20674043893814087 1.0 0.997645378112793 0.648017942905426\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.2576174736022949 1.0 0.9992694854736328 0.6510010957717896\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 27.924556732177734 1.0 0.9124007225036621 0.7110827565193176\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.663250923156738 1.0 0.9280957579612732 0.730656623840332\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.199332237243652 1.0 0.9493042230606079 0.7603207230567932\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.7729750871658325 1.0 0.9699840545654297 0.7954519391059875\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.37925809621810913 1.0 0.9846477508544922 0.828531801700592\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.09068632870912552 1.0 0.9929627180099487 0.8549149036407471\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.000000238418579 -0.19306950271129608 1.0 0.9970508813858032 0.8732234239578247\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 28.298316955566406 1.0 0.9114958643913269 0.8213034868240356\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 13.016112327575684 1.0 0.9252156019210815 0.8399372696876526\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.492627143859863 1.0 0.9451128244400024 0.8690934181213379\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.9737550020217896 1.0 0.9662386178970337 0.9046807885169983\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.4960791766643524 1.0 0.9823827743530273 0.9381226897239685\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.028549756854772568 1.0 0.9919037818908691 0.9638578295707703\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.1613265722990036 1.0 0.9966254830360413 0.9809823632240295\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 28.51780128479004 1.0 0.9254189729690552 0.8993154764175415\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 13.224438667297363 1.0 0.9371320009231567 0.9130640625953674\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.6666460037231445 1.0 0.9543361067771912 0.9340546131134033\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 2.0900719165802 1.0 0.9728366136550903 0.9581451416015625\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.5580777525901794 1.0 0.9868340492248535 0.9780484437942505\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.000516655680257827 1.0 0.9946364760398865 0.9903712272644043\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.15004749596118927 1.0 0.998102605342865 0.9963968396186829\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 24.216032028198242 1.0 0.9828353524208069 0.23431193828582764\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 7.475866794586182 1.0 0.9883894324302673 0.23590417206287384\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 0.13834267854690552 1.0 0.9941451549530029 0.23754167556762695\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -2.283170700073242 1.0 0.997790515422821 0.2385786920785904\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -2.4856812953948975 1.0 0.9993137121200562 0.23901668190956116\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.9582239389419556 1.0 0.9998084306716919 0.2391623556613922\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.3475592136383057 1.0 0.9999496340751648 0.23920592665672302\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 27.043415069580078 1.0 0.9686124324798584 0.3293900191783905\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 9.94112777709961 1.0 0.9760334491729736 0.33250290155410767\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 2.0002355575561523 1.0 0.985579788684845 0.33650854229927063\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.0989563465118408 1.0 0.993538498878479 0.33986955881118774\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.8173425197601318 1.0 0.9977349638938904 0.34166020154953003\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.6036717891693115 1.0 0.9993208646774292 0.3423424959182739\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.1650395393371582 1.0 0.9998134970664978 0.34255534410476685\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 29.07600975036621 1.0 0.9421012997627258 0.44422000646591187\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 11.788912773132324 1.0 0.9517045021057129 0.45046761631965637\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 3.509068012237549 1.0 0.9661353230476379 0.4600571095943451\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.82842755317688 -0.050187624990940094 1.0 0.9815616607666016 0.4706586003303528\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.1852830648422241 1.0 0.9922593832015991 0.4783484637737274\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.2545146942138672 1.0 0.9973472952842712 0.48217591643333435\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9810816645622253 1.0 0.9992064237594604 0.4836297929286957\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 30.404376983642578 1.0 0.9152776598930359 0.5762948393821716\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.024609565734863 1.0 0.9251372218132019 0.5859206914901733\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 4.56855583190918 1.0 0.9414069056510925 0.6025629639625549\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 0.7407423257827759 1.0 0.9621034860610962 0.6255519986152649\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6742120385169983 1.0 0.9802897572517395 0.6484591364860535\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.9552217721939087 1.0 0.9915543794631958 0.6649868488311768\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -0.8160586953163147 1.0 0.996919572353363 0.6740754246711731\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 31.261735916137695 1.0 0.901382565498352 0.7065885066986084\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.831046104431152 1.0 0.9100456833839417 0.71784907579422\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 5.276814937591553 1.0 0.9250670671463013 0.7385172247886658\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 1.2892796993255615 1.0 0.9461806416511536 0.770892322063446\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.3073749542236328 1.0 0.9675782322883606 0.8104062676429749\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.7332636713981628 1.0 0.9831756353378296 0.848249614238739\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.688435435295105 1.0 0.992148756980896 0.8788332343101501\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 31.77234649658203 1.0 0.9065820574760437 0.8187835216522217\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 14.312946319580078 1.0 0.9139794707298279 0.8289048671722412\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 5.702869892120361 1.0 0.9271224737167358 0.8476245999336243\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 1.6202270984649658 1.0 0.9465198516845703 0.8774869441986084\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.09239841252565384 1.0 0.9674373865127563 0.9143167734146118\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.6131249070167542 1.0 0.9834060668945312 0.9483733773231506\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.6271386742591858 1.0 0.9927008748054504 0.9730955958366394\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -7.91182279586792 1.0 0.9908915758132935 0.16796498000621796\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -13.043478965759277 1.0 0.993560254573822 0.16845062375068665\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -13.69672679901123 1.0 0.9965788722038269 0.16899776458740234\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -11.664461135864258 1.0 0.998664915561676 0.16937559843063354\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -8.643918991088867 1.0 0.9995798468589783 0.16954167187213898\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.836615562438965 1.0 0.9998822212219238 0.1695968210697174\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.7031564712524414 1.0 0.9999688863754272 0.16961275041103363\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -2.1944773197174072 1.0 0.9794925451278687 0.24141281843185425\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -8.044111251831055 1.0 0.9838184118270874 0.24261002242565155\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -9.855412483215332 1.0 0.9898106455802917 0.24426685273647308\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -9.163241386413574 1.0 0.9952783584594727 0.24578019976615906\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.210530757904053 1.0 0.9983289241790771 0.24662664532661438\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -5.0705246925354 1.0 0.9994999766349792 0.24695231020450592\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.3074262142181396 1.0 0.9998632073402405 0.24705351889133453\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 1.891742467880249 1.0 0.9627195000648499 0.33449921011924744\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -4.330714225769043 1.0 0.9682204723358154 0.3368583917617798\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.797110557556152 1.0 0.9769737720489502 0.340639591217041\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.001594066619873 1.0 0.9871563911437988 0.34508782625198364\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -5.894506931304932 1.0 0.9946401715278625 0.34840700030326843\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.342309474945068 1.0 0.9982086420059204 0.3500155806541443\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.924278497695923 1.0 0.9994772672653198 0.350597620010376\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 4.747515678405762 1.0 0.9377850890159607 0.4475518763065338\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.6664390563964844 1.0 0.9440425634384155 0.45168760418891907\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.485442638397217 1.0 0.9549363851547241 0.4590247571468353\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.235146522521973 1.0 0.9701825380325317 0.4696301221847534\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.735014915466309 1.0 0.9847875237464905 0.4802839756011963\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.665175676345825 1.0 0.9939223527908325 0.487319678068161\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.555711030960083 1.0 0.9979802370071411 0.49059581756591797\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 6.660118579864502 1.0 0.9117177724838257 0.5769617557525635\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 0.14661738276481628 1.0 0.9177854657173157 0.583103597164154\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.8595612049102783 1.0 0.9289102554321289 0.5947264432907104\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.9193668365478516 1.0 0.9464346170425415 0.614163875579834\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.808823347091675 1.0 0.9669436812400818 0.6393516659736633\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.087724208831787 1.0 0.9834553599357605 0.6627547144889832\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.2233707904815674 1.0 0.9930920004844666 0.6787816286087036\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 7.890878200531006 1.0 0.8991605639457703 0.7075279951095581\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 1.3218364715576172 1.0 0.9044557809829712 0.7145612835884094\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.7898054122924805 1.0 0.9143984913825989 0.7282420992851257\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.0307981967926025 1.0 0.9310269355773926 0.7527904510498047\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.1647417545318604 1.0 0.952716588973999 0.7892901301383972\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.676697015762329 1.0 0.9727208614349365 0.8309479355812073\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.9803723096847534 1.0 0.986240804195404 0.8687071800231934\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -131.0635223388672 1.0 0.9945603013038635 0.12463860958814621\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -83.58687591552734 1.0 0.9960802793502808 0.12483518570661545\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.13322067260742 1.0 0.9978658556938171 0.12506496906280518\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -37.79620361328125 1.0 0.9991526007652283 0.12522979080677032\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.63552474975586 1.0 0.9997313022613525 0.12530352175235748\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -15.427671432495117 1.0 0.9999245405197144 0.1253279149532318\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.330547332763672 1.0 0.9999799728393555 0.12533478438854218\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -119.60536193847656 1.0 0.9887297749519348 0.1739301085472107\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -73.56328582763672 1.0 0.991020143032074 0.1743648499250412\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -48.40108871459961 1.0 0.9942647814750671 0.1749788224697113\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -32.72789764404297 1.0 0.9973178505897522 0.17555464804172516\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -21.717485427856445 1.0 0.999051570892334 0.17588098347187042\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -13.864760398864746 1.0 0.9997175931930542 0.17600630223751068\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -8.522496223449707 1.0 0.999923050403595 0.17604495584964752\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -111.30366516113281 1.0 0.9776778817176819 0.24146851897239685\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -66.00926208496094 1.0 0.9808508157730103 0.24236765503883362\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -42.155216217041016 1.0 0.9859937429428101 0.2438250035047531\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -28.283111572265625 1.0 0.9921571612358093 0.24557140469551086\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -18.99919319152832 1.0 0.9967647790908813 0.24687866866588593\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.359270095825195 1.0 0.9989390969276428 0.24749723076820374\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -7.730782985687256 1.0 0.9996951818466187 0.2477131485939026\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -105.31848907470703 1.0 0.9589822292327881 0.3334788978099823\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -60.41294860839844 1.0 0.9629719853401184 0.33519890904426575\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -37.27631378173828 1.0 0.9700227379798889 0.33825328946113586\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -24.526575088500977 1.0 0.9801988005638123 0.3426959812641144\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -16.5239315032959 1.0 0.9901774525642395 0.3471013605594635\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -10.91930866241455 1.0 0.9962822198867798 0.3498287796974182\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -6.952352523803711 1.0 0.9988318085670471 0.35097774863243103\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -101.16744232177734 1.0 0.9337203502655029 0.4476415514945984\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.46126937866211 1.0 0.9380496740341187 0.45053303241729736\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -33.70245361328125 1.0 0.9461076855659485 0.4559856653213501\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -21.59300994873047 1.0 0.9592925310134888 0.4651245176792145\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -14.434433937072754 1.0 0.9755197167396545 0.47681841254234314\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -9.621609687805176 1.0 0.9886808395385742 0.48678818345069885\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.2186408042907715 1.0 0.9958160519599915 0.4924566149711609\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -98.41326141357422 1.0 0.9089763164520264 0.5777589678764343\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -53.81050109863281 1.0 0.9130741953849792 0.581882655620575\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -31.250112533569336 1.0 0.9209043383598328 0.5899391770362854\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -19.491769790649414 1.0 0.934636652469635 0.6047027707099915\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -12.84195327758789 1.0 0.9541934132575989 0.6275001764297485\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -8.565777778625488 1.0 0.9739387631416321 0.6536439657211304\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 seed:102 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -5.584880352020264 1.0 0.9878109693527222 0.6752075552940369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [45:41, 913.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 24.827884674072266 1.0 0.9819044470787048 0.2971366345882416\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 9.892302513122559 1.0 0.9893748760223389 0.3000600337982178\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 3.104351282119751 1.0 0.9953632354736328 0.3024190366268158\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.4378679394721985 1.0 0.9984078407287598 0.3036530315876007\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 -0.3867804706096649 1.0 0.9995269775390625 0.3041338622570038\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.30468782782554626 1.0 0.9645360708236694 0.2978222668170929\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.011517391540110111 1.0 0.9275791645050049 0.2897058129310608\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 26.305927276611328 1.0 0.9601041078567505 0.42291420698165894\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 11.171781539916992 1.0 0.9721325039863586 0.43088066577911377\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 4.0163469314575195 1.0 0.9848977327346802 0.4395241141319275\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.9872314929962158 1.0 0.9936985373497009 0.4457113742828369\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.08517462015151978 1.0 0.9978560209274292 0.44876378774642944\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.34950801730155945 1.0 0.9993628263473511 0.4499233365058899\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.33266088366508484 1.0 0.9998253583908081 0.4503009617328644\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 27.28689193725586 1.0 0.932522714138031 0.5666171312332153\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 12.066838264465332 1.0 0.9479730129241943 0.5817697644233704\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 4.712024688720703 1.0 0.9668257236480713 0.6019067168235779\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.4400252103805542 1.0 0.9829593896865845 0.6212769150733948\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.1782759130001068 1.0 0.9928243160247803 0.634760856628418\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.20528267323970795 1.0 0.9974451065063477 0.6418161392211914\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.25657328963279724 1.0 0.9992027878761292 0.644702136516571\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 27.947675704956055 1.0 0.9121001362800598 0.6977853775024414\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 12.685050964355469 1.0 0.9280315041542053 0.7177613377571106\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.217205047607422 1.0 0.9493412375450134 0.7479732632637024\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.7848423719406128 1.0 0.9699907898902893 0.7835816144943237\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.38591012358665466 1.0 0.9846420884132385 0.8169140815734863\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.08723998069763184 1.0 0.9929537773132324 0.8434732556343079\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.19131959974765778 1.0 0.9970411062240601 0.8620203137397766\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.31829071044922 1.0 0.9073610901832581 0.8120378851890564\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 13.036185264587402 1.0 0.9219486713409424 0.8316469788551331\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.51188325881958 1.0 0.9424664378166199 0.862458348274231\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 1.990046739578247 1.0 0.9640650749206543 0.9003932476043701\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.5081467032432556 1.0 0.9808398485183716 0.9364457130432129\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.020499592646956444 1.0 0.9910527467727661 0.9641903042793274\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.1564503312110901 1.0 0.9962723255157471 0.9821103811264038\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 28.52079200744629 1.0 0.9209713935852051 0.8921769857406616\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 13.228198051452637 1.0 0.933234453201294 0.90690678358078\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.671962261199951 1.0 0.951291561126709 0.9293788075447083\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 2.096461057662964 1.0 0.9708884954452515 0.9552448987960815\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.5638838410377502 1.0 0.985812783241272 0.9766613245010376\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.00362351443618536 1.0 0.9942061305046082 0.9898351430892944\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.14753031730651855 1.0 0.9979473948478699 0.9962109923362732\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 24.237403869628906 1.0 0.9839627742767334 0.24041038751602173\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 7.491780757904053 1.0 0.9891422390937805 0.24186544120311737\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 0.14734025299549103 1.0 0.9945230484008789 0.243363618850708\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -2.2792105674743652 1.0 0.9979351758956909 0.24431000649929047\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -2.484099864959717 1.0 0.9993594884872437 0.24470485746860504\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.9575823545455933 1.0 0.9998214244842529 0.24483278393745422\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.3472843170166016 1.0 0.9999531507492065 0.24486908316612244\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 27.059743881225586 1.0 0.9672983884811401 0.3304999768733978\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 9.9564208984375 1.0 0.9750504493713379 0.33373576402664185\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 2.013347625732422 1.0 0.9850338101387024 0.3378998935222626\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.0894192457199097 1.0 0.993332028388977 0.34138017892837524\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.8115066289901733 1.0 0.9976761937141418 0.34322425723075867\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.600461483001709 1.0 0.9993059635162354 0.3439258337020874\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.1633614301681519 1.0 0.999809741973877 0.34414586424827576\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 29.054805755615234 1.0 0.9434258341789246 0.4481699466705322\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 11.768630981445312 1.0 0.9527519345283508 0.4542711675167084\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 3.4908668994903564 1.0 0.9667918682098389 0.4636739194393158\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.06432666629552841 1.0 0.9818503856658936 0.47414642572402954\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -1.194385290145874 1.0 0.9923436045646667 0.48180684447288513\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.2596330642700195 1.0 0.9973633885383606 0.4856480062007904\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9837688207626343 1.0 0.9992082715034485 0.4871143102645874\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 30.35464859008789 1.0 0.9194783568382263 0.5779794454574585\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 12.977063179016113 1.0 0.9287497997283936 0.587160587310791\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 4.525589466094971 1.0 0.9440854787826538 0.6030915379524231\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.82842755317688 0.706214964389801 1.0 0.9636712670326233 0.6252611875534058\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6977266669273376 1.0 0.980984628200531 0.6475706100463867\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9691115617752075 1.0 0.9917990565299988 0.6637905836105347\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.8235905170440674 1.0 0.9969939589500427 0.6727353930473328\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 31.22736167907715 1.0 0.9038519263267517 0.7106084227561951\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 13.798033714294434 1.0 0.9123671054840088 0.7216078639030457\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 5.24654483795166 1.0 0.9271078705787659 0.7417699098587036\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 1.2639820575714111 1.0 0.9477441310882568 0.7733408212661743\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.3257902264595032 1.0 0.9685375690460205 0.8119748830795288\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.7449582815170288 1.0 0.9836491346359253 0.8491728901863098\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6951730251312256 1.0 0.9923518300056458 0.8794000744819641\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 31.763002395629883 1.0 0.9090278148651123 0.8224207162857056\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 14.30370807647705 1.0 0.916226863861084 0.8323688507080078\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 5.693878173828125 1.0 0.9290123581886292 0.8507539629936218\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 1.6118683815002441 1.0 0.9479034543037415 0.880059540271759\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.0993533805012703 1.0 0.9683094024658203 0.9161953330039978\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6180920600891113 1.0 0.9838703870773315 0.9496158957481384\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6302661299705505 1.0 0.9929145574569702 0.9738723039627075\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.924940586090088 1.0 0.9902525544166565 0.17091338336467743\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -13.05300521850586 1.0 0.9931222796440125 0.17144376039505005\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -13.70183277130127 1.0 0.9963574409484863 0.1720355749130249\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -11.666379928588867 1.0 0.9985830187797546 0.1724390685558319\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -8.644479751586914 1.0 0.9995551109313965 0.1726139932870865\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.836754322052002 1.0 0.9998754858970642 0.17267104983329773\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.7031848430633545 1.0 0.999967098236084 0.17268703877925873\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -2.2670738697052 1.0 0.9801599979400635 0.23882029950618744\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -8.109848976135254 1.0 0.9843482971191406 0.2400004118680954\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -9.908933639526367 1.0 0.9901483058929443 0.2416260987520218\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -9.20032024383545 1.0 0.9954351186752319 0.24310052394866943\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -7.232677936553955 1.0 0.9983829855918884 0.2439209669828415\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.082620143890381 1.0 0.9995156526565552 0.2442360371351242\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.3137400150299072 1.0 0.9998673796653748 0.2443338930606842\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.8911869525909424 1.0 0.96378493309021 0.3307003080844879\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.332530975341797 1.0 0.9691371917724609 0.33294206857681274\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.8006792068481445 1.0 0.9776510000228882 0.33652791380882263\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -7.006387710571289 1.0 0.9875451922416687 0.34073591232299805\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.8988752365112305 1.0 0.994804322719574 0.34386491775512695\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.34530782699585 1.0 0.9982624053955078 0.34537240862846375\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.926032304763794 1.0 0.9994924664497375 0.34591057896614075\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 4.744576930999756 1.0 0.9380906224250793 0.4448598027229309\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.669428825378418 1.0 0.9442403316497803 0.4489496052265167\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.4883623123168945 1.0 0.9549767374992371 0.4562252163887024\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.237504005432129 1.0 0.9700766205787659 0.466795951128006\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.7362141609191895 1.0 0.9846475124359131 0.47748813033103943\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.665438652038574 1.0 0.9938378930091858 0.4845866560935974\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.5556259155273438 1.0 0.9979470372200012 0.4878949224948883\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 6.662845134735107 1.0 0.9118426442146301 0.5744799971580505\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 0.14907169342041016 1.0 0.9179040789604187 0.5805602073669434\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.857592821121216 1.0 0.9290155172348022 0.5920692086219788\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.918102264404297 1.0 0.9465190172195435 0.611328125\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.808232545852661 1.0 0.9670058488845825 0.6362955570220947\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.0875449180603027 1.0 0.9834932684898376 0.6594935059547424\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.223357677459717 1.0 0.9931100010871887 0.6753755211830139\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 7.887546062469482 1.0 0.8991093039512634 0.7080365419387817\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.3186887502670288 1.0 0.9044355750083923 0.7150943279266357\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.7926243543624878 1.0 0.9144352078437805 0.7288108468055725\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.0331263542175293 1.0 0.9311490058898926 0.75337153673172\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -3.166590929031372 1.0 0.9529091119766235 0.7897472381591797\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.6782267093658447 1.0 0.9728990793228149 0.8310918807983398\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.9815605878829956 1.0 0.9863387942314148 0.8685388565063477\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -131.2316436767578 1.0 0.9947370886802673 0.12484891712665558\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -83.73136901855469 1.0 0.9962055683135986 0.12504258751869202\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.24132537841797 1.0 0.9979322552680969 0.1252691000699997\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -37.86482620239258 1.0 0.999177873134613 0.1254318803548813\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.67423439025879 1.0 0.9997389912605286 0.12550511956214905\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -15.448185920715332 1.0 0.9999265670776367 0.12552966177463531\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -9.34109878540039 1.0 0.9999805688858032 0.12553676962852478\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -119.6929702758789 1.0 0.9885973334312439 0.17500253021717072\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -73.64103698730469 1.0 0.990902841091156 0.17544767260551453\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -48.46239471435547 1.0 0.9941778182983398 0.17607803642749786\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -32.768707275390625 1.0 0.9972701072692871 0.17667123675346375\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -21.740983963012695 1.0 0.9990322589874268 0.17700907588005066\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -13.877269744873047 1.0 0.9997112154960632 0.17713969945907593\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -8.528928756713867 1.0 0.9999212026596069 0.1771804839372635\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -111.3216323852539 1.0 0.9777299165725708 0.24100205302238464\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -66.02581787109375 1.0 0.9809078574180603 0.24189135432243347\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -42.16929626464844 1.0 0.986053466796875 0.24333098530769348\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -28.29364585876465 1.0 0.9922078251838684 0.24505271017551422\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -19.00604820251465 1.0 0.9967941641807556 0.246337890625\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.363273620605469 1.0 0.9989510178565979 0.24694427847862244\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -7.732964992523193 1.0 0.9996991157531738 0.24715556204319\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -105.3196792602539 1.0 0.9593668580055237 0.33259811997413635\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -60.41440200805664 1.0 0.9633318185806274 0.3342965841293335\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -37.27820587158203 1.0 0.9703312516212463 0.3373112678527832\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.528982162475586 1.0 0.9804179072380066 0.34169435501098633\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -16.526426315307617 1.0 0.9902927875518799 0.3460412621498108\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -10.921268463134766 1.0 0.9963266849517822 0.34873655438423157\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.953596115112305 1.0 0.9988455772399902 0.3498759865760803\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -101.22752380371094 1.0 0.9342982769012451 0.44582971930503845\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -56.5189323425293 1.0 0.9385885000228882 0.4486962556838989\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -33.75542449951172 1.0 0.9465764164924622 0.4541059732437134\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -21.637590408325195 1.0 0.9596485495567322 0.4631809592247009\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -14.46696949005127 1.0 0.9757299423217773 0.47480309009552\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -9.641946792602539 1.0 0.9887675046920776 0.48472681641578674\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -6.2300214767456055 1.0 0.9958412051200867 0.4903867244720459\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -98.44070434570312 1.0 0.90980064868927 0.5777203440666199\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -53.83721160888672 1.0 0.913844645023346 0.5818111896514893\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -31.275367736816406 1.0 0.9215784668922424 0.5898090600967407\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -19.514192581176758 1.0 0.9351617097854614 0.6044813990592957\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -12.859665870666504 1.0 0.9545415639877319 0.627161979675293\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -8.577800750732422 1.0 0.9741324186325073 0.6531825661659241\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 seed:103 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -5.59207010269165 1.0 0.9878977537155151 0.6746525168418884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:00:54, 913.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 24.86055564880371 1.0 0.9783510565757751 0.3186090588569641\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 9.926170349121094 1.0 0.987235426902771 0.3224584758281708\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 3.1329352855682373 1.0 0.9943897724151611 0.3255586326122284\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.45704731345176697 1.0 0.9980596899986267 0.32717669010162354\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.3756498694419861 1.0 0.9994247555732727 0.3278021514415741\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.3017672598361969 1.0 0.964756429195404 0.3203909397125244\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.03384426608681679 1.0 0.923494279384613 0.3114802837371826\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 26.289684295654297 1.0 0.9589463472366333 0.4273768663406372\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 11.161259651184082 1.0 0.971808135509491 0.4356185495853424\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 4.011875629425049 1.0 0.985053300857544 0.4442538022994995\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.9850529432296753 1.0 0.9938938617706299 0.45017045736312866\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 -0.0866251289844513 1.0 0.997954249382019 0.45296159386634827\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.3504369258880615 1.0 0.9993975758552551 0.45396852493286133\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.3331974744796753 1.0 0.9998358488082886 0.4542732238769531\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 27.320371627807617 1.0 0.9309057593345642 0.5583600997924805\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 12.099963188171387 1.0 0.9466583132743835 0.5729424953460693\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 4.741105079650879 1.0 0.9661130309104919 0.5923459529876709\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.46011483669281 1.0 0.9827736616134644 0.6109716296195984\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.999999761581421 0.18961204588413239 1.0 0.9928228259086609 0.6238815784454346\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.19947178661823273 1.0 0.997459352016449 0.6306564211845398\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.2536757290363312 1.0 0.9992086291313171 0.6334617733955383\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 27.941055297851562 1.0 0.9065966010093689 0.6875812411308289\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 12.680253028869629 1.0 0.9224646091461182 0.7076298594474792\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.2169084548950195 1.0 0.944720983505249 0.738450288772583\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 1.7893568277359009 1.0 0.9670475125312805 0.7754729390144348\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 0.3918440043926239 1.0 0.9830717444419861 0.8105613589286804\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.08246856927871704 1.0 0.9922175407409668 0.8387575745582581\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.1881958395242691 1.0 0.9967139363288879 0.858799397945404\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.32899284362793 1.0 0.9002481698989868 0.8149006366729736\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 13.047731399536133 1.0 0.9158176779747009 0.8351570963859558\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 5.524737358093262 1.0 0.938477635383606 0.8665098547935486\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 2.0020339488983154 1.0 0.9625746011734009 0.9041236042976379\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.516182005405426 1.0 0.9807805418968201 0.9387919902801514\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.016459187492728233 1.0 0.9913240671157837 0.9649315476417542\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 -0.15472233295440674 1.0 0.9964447021484375 0.9819127917289734\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 28.531394958496094 1.0 0.9205725193023682 0.8962850570678711\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 13.238390922546387 1.0 0.9329971671104431 0.9106584787368774\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 1.9999998807907104 5.6808671951293945 1.0 0.9513619542121887 0.9324852824211121\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 2.10286808013916 1.0 0.9712432622909546 0.9574241638183594\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.5674516558647156 1.0 0.9861817955970764 0.9779050946235657\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 0.005236297845840454 1.0 0.9944160580635071 0.9904276132583618\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d4_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.0 -0.1468808948993683 1.0 0.998039960861206 0.9964531660079956\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 24.1982364654541 1.0 0.9835872054100037 0.2368316948413849\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 7.459191799163818 1.0 0.9889618158340454 0.23839981853961945\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 0.12441235035657883 1.0 0.9944778084754944 0.23998583853244781\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -2.292933464050293 1.0 0.9979321360588074 0.24097275733947754\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -2.4915788173675537 1.0 0.9993613362312317 0.24138493835926056\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.9614797830581665 1.0 0.9998223185539246 0.24152198433876038\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.349268913269043 1.0 0.9999536275863647 0.24156343936920166\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.82842755317688 27.004322052001953 1.0 0.9686850905418396 0.3266702890396118\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 9.905755043029785 1.0 0.9760343432426453 0.3299047648906708\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 1.9721876382827759 1.0 0.9855306148529053 0.33410272002220154\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.117197036743164 1.0 0.9934882521629333 0.33765947818756104\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.8275338411331177 1.0 0.9977078437805176 0.3395839035511017\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.6089781522750854 1.0 0.9993106126785278 0.34033745527267456\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.167731523513794 1.0 0.9998102188110352 0.3405841290950775\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.82842755317688 29.02061653137207 1.0 0.944021463394165 0.4408870041370392\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 11.736839294433594 1.0 0.9529612064361572 0.4471580684185028\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 3.4643068313598633 1.0 0.9665895700454712 0.45695960521698\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.08217965066432953 1.0 0.9814775586128235 0.4680511951446533\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -1.2038547992706299 1.0 0.9920754432678223 0.4762834906578064\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -1.264040470123291 1.0 0.9972432851791382 0.4804559350013733\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9857672452926636 1.0 0.9991673827171326 0.48205915093421936\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 30.404346466064453 1.0 0.9166903495788574 0.571971595287323\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 13.024288177490234 1.0 0.9262269735336304 0.5816528797149658\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 4.567754745483398 1.0 0.9420082569122314 0.5984085202217102\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 0.7396632432937622 1.0 0.962216854095459 0.6216252446174622\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6748237609863281 1.0 0.9801630973815918 0.6449057459831238\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.9552043080329895 1.0 0.9914155006408691 0.6618573069572449\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.815771758556366 1.0 0.9968392252922058 0.6712889671325684\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 31.25651741027832 1.0 0.9027236104011536 0.706209123134613\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 13.825557708740234 1.0 0.9112556576728821 0.7173511385917664\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 5.2709503173828125 1.0 0.9260615110397339 0.7378396987915039\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 1.2834326028823853 1.0 0.9468852877616882 0.7699953317642212\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.31217536330223083 1.0 0.9679588675498962 0.8092893362045288\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.7364389896392822 1.0 0.9832978844642639 0.8470779657363892\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 -0.6901939511299133 1.0 0.9921457767486572 0.8778986930847168\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 31.771480560302734 1.0 0.9075368046760559 0.8170033097267151\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 14.311842918395996 1.0 0.9147443771362305 0.8270295262336731\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 5.701385974884033 1.0 0.9275606274604797 0.8456568717956543\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284270763397217 1.618497371673584 1.0 0.9465660452842712 0.8756106495857239\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.09367247670888901 1.0 0.967270016670227 0.912862241268158\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.8284268379211426 -0.6136536598205566 1.0 0.9832527041435242 0.9474205374717712\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d8_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 2.828427314758301 -0.6272382140159607 1.0 0.992612361907959 0.9725090265274048\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.000000476837158 -7.964737892150879 1.0 0.989666223526001 0.16866742074489594\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -13.084484100341797 1.0 0.9926561117172241 0.16923362016677856\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -13.722086906433105 1.0 0.9960691928863525 0.16987338662147522\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -11.67691421508789 1.0 0.9984540343284607 0.17031659185886383\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -8.649415969848633 1.0 0.999510645866394 0.17051176726818085\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -5.839041709899902 1.0 0.9998623728752136 0.17057634890079498\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.704267740249634 1.0 0.9999635815620422 0.17059475183486938\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.2219693660736084 1.0 0.9795166850090027 0.23823294043540955\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -8.068463325500488 1.0 0.9838749170303345 0.23942457139492035\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -9.874695777893066 1.0 0.9898854494094849 0.2410629689693451\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -9.17638111114502 1.0 0.9953349232673645 0.24254557490348816\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -7.218391418457031 1.0 0.9983550310134888 0.2433663010597229\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.074853897094727 1.0 0.9995089769363403 0.24367859959602356\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.309701681137085 1.0 0.9998657703399658 0.2437741458415985\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.8910388946533203 1.0 0.9616661667823792 0.3297768235206604\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.330408573150635 1.0 0.9673085808753967 0.33216139674186707\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -6.795312404632568 1.0 0.976303219795227 0.33599022030830383\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -6.9985175132751465 1.0 0.9867821335792542 0.34050238132476807\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -5.89141321182251 1.0 0.9944862723350525 0.34386956691741943\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.34009313583374 1.0 0.9981581568717957 0.34549835324287415\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999523162842 -2.9229536056518555 1.0 0.9994627833366394 0.3460855484008789\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 4.749838352203369 1.0 0.9382479190826416 0.4448896646499634\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.6645711660385132 1.0 0.9444050192832947 0.4489612579345703\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -4.484275817871094 1.0 0.9551381468772888 0.4561990797519684\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -5.234646797180176 1.0 0.9702059626579285 0.4667074680328369\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -4.734665393829346 1.0 0.9847240447998047 0.4773330092430115\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.6647439002990723 1.0 0.9938719272613525 0.4843844771385193\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.5553367137908936 1.0 0.9979593753814697 0.48766762018203735\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 6.683816432952881 1.0 0.9114362597465515 0.5777221918106079\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 0.16916346549987793 1.0 0.917571485042572 0.5838664174079895\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.839259386062622 1.0 0.9288007616996765 0.5954795479774475\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.9030308723449707 1.0 0.9464370012283325 0.6148706078529358\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.797816514968872 1.0 0.9669925570487976 0.6399751305580139\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.081441640853882 1.0 0.9834826588630676 0.6633248925209045\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -2.2200772762298584 1.0 0.9930976629257202 0.6793420910835266\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 7.881363868713379 1.0 0.8995288610458374 0.710812509059906\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 1.3126306533813477 1.0 0.9048420190811157 0.7178381681442261\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.7984236478805542 1.0 0.9148110747337341 0.7314963340759277\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 3.999999761581421 -3.0383942127227783 1.0 0.9314535856246948 0.7559748291969299\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -3.1708743572235107 1.0 0.9530889391899109 0.792314887046814\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -2.6811416149139404 1.0 0.9729628562927246 0.8337686657905579\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d16_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 4.0 -1.983234167098999 1.0 0.9863613843917847 0.8713722825050354\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -131.16114807128906 1.0 0.9945404529571533 0.12125653028488159\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -83.66992950439453 1.0 0.9960707426071167 0.12144627422094345\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.19451141357422 1.0 0.9978644251823425 0.12166797369718552\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -37.834651947021484 1.0 0.9991534352302551 0.1218268945813179\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -24.65705108642578 1.0 0.9997319579124451 0.12189789116382599\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -15.439034461975098 1.0 0.9999247789382935 0.12192130088806152\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_125_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -9.336381912231445 1.0 0.9999800324440002 0.12192786484956741\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.65685510635376 -119.66621398925781 1.0 0.9887250065803528 0.17193230986595154\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -73.61749267578125 1.0 0.9910205602645874 0.1723596602678299\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -48.44422912597656 1.0 0.994269847869873 0.17296265065670013\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -32.75710678100586 1.0 0.9973228573799133 0.17352762818336487\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.65685510635376 -21.734642028808594 1.0 0.9990540146827698 0.17384757101535797\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -13.874034881591797 1.0 0.9997183680534363 0.17397037148475647\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_250_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -8.527312278747559 1.0 0.9999232888221741 0.1740082949399948\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -111.27784729003906 1.0 0.977742612361908 0.24086157977581024\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -65.98542785644531 1.0 0.9809110164642334 0.24175147712230682\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -42.135013580322266 1.0 0.9860442876815796 0.24319322407245636\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -28.268388748168945 1.0 0.9921921491622925 0.24491983652114868\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -18.99017333984375 1.0 0.9967829585075378 0.2462107539176941\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.354342460632324 1.0 0.9989461302757263 0.2468203604221344\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_500_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -7.728229999542236 1.0 0.9996975064277649 0.2470325380563736\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -105.31478881835938 1.0 0.9595725536346436 0.3335515856742859\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -60.41011047363281 1.0 0.9634937047958374 0.33523881435394287\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854629516602 -37.274898529052734 1.0 0.9704257845878601 0.33823853731155396\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -24.526824951171875 1.0 0.9804410934448242 0.3426125645637512\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -16.52513885498047 1.0 0.9902814030647278 0.3469657897949219\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -10.920478820800781 1.0 0.9963157176971436 0.34967121481895447\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_1000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -6.953129768371582 1.0 0.9988409280776978 0.3508145809173584\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -101.19995880126953 1.0 0.9338698983192444 0.4479914903640747\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -56.492401123046875 1.0 0.9381880760192871 0.45088955760002136\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -33.73091125488281 1.0 0.9462266564369202 0.45635688304901123\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -21.61673927307129 1.0 0.9593802690505981 0.4655241370201111\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -14.451482772827148 1.0 0.9755634665489197 0.4772603213787079\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -9.63204574584961 1.0 0.9886881709098816 0.48728176951408386\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_2000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -6.224352836608887 1.0 0.995811402797699 0.4929998219013214\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.1_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -98.44303894042969 1.0 0.9090793132781982 0.5787826776504517\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -53.83918762207031 1.0 0.9131813645362854 0.582922101020813\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -31.276689529418945 1.0 0.9210246801376343 0.5910083055496216\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_0.8_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -19.51458740234375 1.0 0.9347898364067078 0.6058147549629211\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_1.6_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -12.859277725219727 1.0 0.9543896913528442 0.628619909286499\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_3.2_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656853675842285 -8.57734203338623 1.0 0.9741312861442566 0.6546517014503479\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 seed:104 epochs:100  lr:1.0  max_norm:20.0  \n",
      "vector_CE_GD_TEACHER_fixedTrue_N_250_P_4000_d32_l_6.4_epochs100_lr1.0_l2None.pth\n",
      "# epoch norm train_loss learning_rate train_metric R\n",
      "50 5.656854152679443 -5.591960430145264 1.0 0.9879396557807922 0.6760208010673523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:16:02, 912.42s/it]\n"
     ]
    }
   ],
   "source": [
    "curve = np.zeros((len(seeds_arr), len(d_arr), len(alpha_P_arr), len(l_arr)))\n",
    "\n",
    "for i_s, seed in tqdm(enumerate(seeds_arr)):\n",
    "    for i_d, d in enumerate(d_arr):\n",
    "        for i_a, alpha_P in enumerate(alpha_P_arr):\n",
    "            P = int(alpha_P*N)\n",
    "            dataset, model, optimizer = initialize(N, P, d, lr, spin_type,device, gamma, init_Hebb=init_Hebb, downf=downf, seed=seed)\n",
    "            J2 = model.J.clone().squeeze().cpu().detach().numpy()\n",
    "            norm_J2 = np.linalg.norm(J2)\n",
    "\n",
    "            batch_size = P\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=2,\n",
    "                pin_memory=True,          # important for fast H2D\n",
    "                persistent_workers=True,  # avoids worker respawn overhead\n",
    "                prefetch_factor=2,        # default is 2 when num_workers>0\n",
    "            )\n",
    "\n",
    "            #epochs_to_save = [10,20,50,100,200,400,800,1600]\n",
    "            #save = True\n",
    "            epochs_to_save = [1000]\n",
    "            save = False\n",
    "\n",
    "            for i_l, l in enumerate(l_arr):\n",
    "                print(\"########\")\n",
    "                print(\"d:{} alpha_P:{} lambda:{} seed:{} epochs:{}  lr:{}  max_norm:{}  \".format(d, alpha_P, l, seed, epochs, lr, max_grad))\n",
    "                model_name_base = \"{}_{}_GD_TEACHER_fixed{}_N_{}_P_{}_d{}_l_{}_epochs{}_lr{}_l2{}\".format(spin_type, loss_type, fixed_norm, N, P, d, l, epochs, lr, l2)\n",
    "                model_name = model_name_base + \".pth\"\n",
    "                print(model_name)\n",
    "                train_model(\n",
    "                    model, fixed_norm, dataset, dataloader, epochs, \n",
    "                    lr, max_grad, device, data_PATH, l, optimizer, J2, \n",
    "                    norm_J2, valid_every, epochs_to_save, model_name_base, save, l2, loss_type\n",
    "                )\n",
    "\n",
    "                R = (torch.einsum(\"iab,iab->\", dataset.T.to(device), model.J) / (dataset.T.to(device).norm() * model.J.norm())).cpu().item()\n",
    "\n",
    "                curve[i_s, i_d, i_a, i_l] = R  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"numerical_result_PL_2\"\n",
    "\n",
    "np.save(data_PATH+name, curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nature of variables and loss\n",
    "spin_type = \"vector\"\n",
    "loss_type = \"CE\"\n",
    "init_Hebb = True\n",
    "fixed_norm = True\n",
    "l2 = None\n",
    "\n",
    "# Dataset\n",
    "alpha_P = 1.\n",
    "N=250\n",
    "sigma = 1.\n",
    "\n",
    "# Model and training   \n",
    "downf=1.\n",
    "gamma=0.0\n",
    "lr = 1.\n",
    "epochs=60\n",
    "valid_every = 30\n",
    "max_grad = 20.\n",
    "\n",
    "METRIC_NAMES = [\n",
    "    \"epoch\",\n",
    "    \"norm_J\",\n",
    "    \"train_loss\",\n",
    "    \"train_accuracy\",\n",
    "    \"R\",\n",
    "    \"learning_rate\",\n",
    "    \"diff_hebb\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_loss(d: int, R: float, n_samples: int = 200_000, seed: int | None = 0) -> float:\n",
    "    if d < 1:\n",
    "        raise ValueError(\"d must be >= 1\")\n",
    "    if not (-1.0 <= R <= 1.0):\n",
    "        raise ValueError(\"R must be in [-1, 1]\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    u = rng.standard_normal((n_samples, d))\n",
    "    w = rng.standard_normal((n_samples, d))\n",
    "    v = R * u + np.sqrt(max(0.0, 1.0 - R**2)) * w\n",
    "\n",
    "    dot = np.sum(u * v, axis=1)\n",
    "    nu = np.linalg.norm(u, axis=1)\n",
    "    nv = np.linalg.norm(v, axis=1)\n",
    "\n",
    "    cosang = dot / (nu * nv)\n",
    "    cosang = np.clip(cosang, -1.0, 1.0)\n",
    "\n",
    "    return float(np.mean(np.arccos(cosang)))*2/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367453636568502\n"
     ]
    }
   ],
   "source": [
    "print(angular_loss(d=16, R=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:0.1 \n",
      "[0.33171055 0.33171052] 0.7836881080483818\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:0.2 \n",
      "[0.33539799 0.33539799] 0.7811948141125121\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:0.4 \n",
      "[0.33834127 0.33834124] 0.7792024224488748\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:0.8 \n",
      "[0.33986554 0.33986554] 0.778169752940551\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:1.6 \n",
      "[0.34044579 0.34044528] 0.7777768451214486\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:3.2 \n",
      "[0.33734196 0.33210805] 0.7834194679327856\n",
      "########\n",
      "d:4 alpha_P:0.5 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:0.5 l:6.4 \n",
      "[0.32360509 0.32335857] 0.7893236888953217\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:0.1 \n",
      "[0.43272755 0.43272755] 0.7140104109473147\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:0.2 \n",
      "[0.44129509 0.44129509] 0.7079473184187085\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:0.4 \n",
      "[0.45037755 0.45037755] 0.7014890761038033\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:0.8 \n",
      "[0.45673895 0.45673895] 0.6969461603404586\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:1.6 \n",
      "[0.45980799 0.45980808] 0.6947484459093038\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:3.2 \n",
      "[0.46093801 0.46093851] 0.6939379876042031\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:1.0 l:6.4 \n",
      "[0.46128827 0.46128851] 0.6936869488508223\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:0.1 \n",
      "[0.57766014 0.57766014] 0.6068348037080877\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:0.2 \n",
      "[0.59270638 0.59270626] 0.5950229136458152\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:0.4 \n",
      "[0.61284894 0.612849  ] 0.5789573086165861\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:0.8 \n",
      "[0.63237977 0.63238055] 0.5630777917865785\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:1.6 \n",
      "[0.64606458 0.64607364] 0.551753329173208\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:3.2 \n",
      "[0.65331608 0.65335447] 0.5456626017944771\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:2.0 l:6.4 \n",
      "[0.65637487 0.65643644] 0.5430692289559238\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:0.1 \n",
      "[0.71215963 0.71215963] 0.4944298981678131\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:0.2 \n",
      "[0.73201215 0.73201215] 0.4761779541191863\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:0.4 \n",
      "[0.76190579 0.76190603] 0.4475716331225685\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:0.8 \n",
      "[0.79699039 0.79699719] 0.4119368638932086\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:1.6 \n",
      "[0.82934195 0.82943243] 0.3764745597862754\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:3.2 \n",
      "[0.8542788  0.85475087] 0.34659912265609083\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:4.0 l:6.4 \n",
      "[0.87124336 0.87230164] 0.3244541201204246\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:0.1 \n",
      "[0.82144016 0.82144016] 0.38547629917578885\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:0.2 \n",
      "[0.84054333 0.84054333] 0.36363414063725485\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:0.4 \n",
      "[0.87019867 0.87019902] 0.32717814410106155\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:0.8 \n",
      "[0.90602309 0.90602869] 0.2774697391258507\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:1.6 \n",
      "[0.93934447 0.93940264] 0.22216045045047214\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:3.2 \n",
      "[0.96467489 0.96495217] 0.16861351019476606\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:8.0 l:6.4 \n",
      "[0.98124903 0.98186237] 0.12113460806077997\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:0.1 \n",
      "[0.89797688 0.89797688] 0.28932511729973753\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:0.2 \n",
      "[0.91185838 0.9118585 ] 0.268582515025839\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:0.4 \n",
      "[0.93303287 0.93303299] 0.2336704197093156\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:0.8 \n",
      "[0.95741934 0.95742053] 0.1859574305608631\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:1.6 \n",
      "[0.97767198 0.97767746] 0.13442855682862004\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:3.2 \n",
      "[0.99022651 0.99024194] 0.08879820523353549\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:4 alpha:16.0 l:6.4 \n",
      "[0.99634624 0.99636674] 0.05415485788582144\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:0.1 \n",
      "[0.24334304 0.24334304] 0.8430594183634765\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:0.2 \n",
      "[0.24480975 0.24480975] 0.8420969331055059\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:0.4 \n",
      "[0.2463254 0.2463254] 0.8411019426666275\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:0.8 \n",
      "[0.24728757 0.24728754] 0.8404701199569732\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:1.6 \n",
      "[0.24769373 0.24769369] 0.8402033619167713\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:3.2 \n",
      "[0.24782908 0.24782908] 0.8401144297499774\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:0.5 l:6.4 \n",
      "[0.24786954 0.24786763] 0.8400891080736143\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:0.1 \n",
      "[0.33316046 0.33316043] 0.7833464182700514\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:0.2 \n",
      "[0.33635768 0.33635768] 0.781187610758462\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:0.4 \n",
      "[0.34050962 0.34050962] 0.7783802942218243\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:0.8 \n",
      "[0.34402245 0.34402245] 0.7760016185509538\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:1.6 \n",
      "[0.34590441 0.34590444] 0.774725916969831\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:3.2 \n",
      "[0.3466253  0.34662536] 0.7742369951891888\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:1.0 l:6.4 \n",
      "[0.3468523 0.3468526] 0.774082852078974\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:0.1 \n",
      "[0.44679379 0.44679385] 0.7047593077840084\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:0.2 \n",
      "[0.45285979 0.45285979] 0.7004386722026589\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:0.4 \n",
      "[0.46221641 0.46221641] 0.6937448159549955\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:0.8 \n",
      "[0.47263435 0.47263438] 0.6862483150633254\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:1.6 \n",
      "[0.48023582 0.48023584] 0.68074861293226\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:3.2 \n",
      "[0.48402128 0.48402134] 0.6780001116459424\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:2.0 l:6.4 \n",
      "[0.48544514 0.48544425] 0.6769652966244433\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:0.1 \n",
      "[0.57673323 0.57673323] 0.6083639879515294\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:0.2 \n",
      "[0.58606941 0.58606941] 0.6010653495469384\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:0.4 \n",
      "[0.60225487 0.60225493] 0.5882658956403662\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:0.8 \n",
      "[0.62474686 0.6247474 ] 0.5701478249760479\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:1.6 \n",
      "[0.64731914 0.64732552] 0.5515351044428078\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:3.2 \n",
      "[0.66365987 0.66369522] 0.5377464103103193\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:4.0 l:6.4 \n",
      "[0.67264664 0.67271882] 0.5300312518798669\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:0.1 \n",
      "[0.70805496 0.7080549 ] 0.4989499311640151\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:0.2 \n",
      "[0.7191084  0.71910846] 0.4889130805765463\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:0.4 \n",
      "[0.73942196 0.73942244] 0.4700246738567226\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:0.8 \n",
      "[0.77128941 0.77129334] 0.439072096887616\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:1.6 \n",
      "[0.81021249 0.8102631 ] 0.39853109327440517\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:3.2 \n",
      "[0.84734809 0.84773809] 0.3558395837178226\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:8.0 l:6.4 \n",
      "[0.87699586 0.87837511] 0.3171885378964473\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:0.1 \n",
      "[0.81787014 0.81787026] 0.3901980432915998\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:0.2 \n",
      "[0.82804072 0.8280409 ] 0.37880771378160943\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:0.4 \n",
      "[0.84687787 0.84687877] 0.35686899784017434\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:0.8 \n",
      "[0.87698853 0.87699348] 0.3190230514085865\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:1.6 \n",
      "[0.91413319 0.91417181] 0.26563602150172033\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:3.2 \n",
      "[0.94827533 0.94848526] 0.2051955969957117\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:8 alpha:16.0 l:6.4 \n",
      "[0.97277033 0.97333723] 0.14731254099581825\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:0.1 \n",
      "[0.1702711  0.17027108] 0.8915588963087377\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:0.2 \n",
      "[0.17080292 0.17080292] 0.8912150571625789\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:0.4 \n",
      "[0.17140168 0.17140168] 0.8908279137637792\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:0.8 \n",
      "[0.17181456 0.17181456] 0.8905609300819431\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:1.6 \n",
      "[0.17199565 0.17199565] 0.8904438222000913\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:3.2 \n",
      "[0.1720555 0.1720555] 0.8904051226683412\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:0.5 l:6.4 \n",
      "[0.17207262 0.1720726 ] 0.8903940600764572\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:0.1 \n",
      "[0.23953056 0.23953056] 0.8464677372912522\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:0.2 \n",
      "[0.240716   0.24071601] 0.8456897003954406\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:0.4 \n",
      "[0.24234042 0.24234042] 0.8446231793691711\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:0.8 \n",
      "[0.24380396 0.24380396] 0.8436618872427754\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:1.6 \n",
      "[0.2446122 0.2446122] 0.8431308599021662\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:3.2 \n",
      "[0.24492052 0.24492052] 0.8429282586725869\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:1.0 l:6.4 \n",
      "[0.24501579 0.24501579] 0.842865656665544\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:0.1 \n",
      "[0.32970157 0.32970157] 0.7865079233583121\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:0.2 \n",
      "[0.33204713 0.33204713] 0.7849241526732884\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:0.4 \n",
      "[0.33580258 0.33580261] 0.7823854771580141\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:0.8 \n",
      "[0.34020835 0.34020838] 0.7794025965191954\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:1.6 \n",
      "[0.3434813  0.34348127] 0.777183441048046\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:3.2 \n",
      "[0.34505826 0.34505826] 0.7761131668138987\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:2.0 l:6.4 \n",
      "[0.34562308 0.34562311] 0.7757296569891238\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:0.1 \n",
      "[0.44439673 0.4443967 ] 0.7071673181401547\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:0.2 \n",
      "[0.44854355 0.44854355] 0.7042142165615042\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:0.4 \n",
      "[0.4559173 0.4559173] 0.698946057693047\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:0.8 \n",
      "[0.46661526 0.46661541] 0.691262818427848\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:1.6 \n",
      "[0.47740588 0.47740632] 0.6834631096667149\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:3.2 \n",
      "[0.48454037 0.48454165] 0.6782771399601554\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:4.0 l:6.4 \n",
      "[0.48785007 0.48785233] 0.6758630015282328\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:0.1 \n",
      "[0.57534933 0.57534939] 0.6099755581837394\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:0.2 \n",
      "[0.58139586 0.58139598] 0.6052528414868515\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:0.4 \n",
      "[0.59284735 0.5928477 ] 0.5962390630174311\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:0.8 \n",
      "[0.61203229 0.6120342 ] 0.5809223468397858\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:1.6 \n",
      "[0.63695884 0.6369735 ] 0.5605756725532408\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:3.2 \n",
      "[0.66016817 0.66025096] 0.5410890259425969\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:8.0 l:6.4 \n",
      "[0.67607886 0.67629582] 0.5273478105381169\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:0.1 \n",
      "[0.70775962 0.70776027] 0.4995727976903447\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:0.2 \n",
      "[0.71476501 0.71476579] 0.4932240894710025\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:0.4 \n",
      "[0.72838199 0.7283842 ] 0.48069330963921464\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:0.8 \n",
      "[0.75279188 0.75280178] 0.4575408243787295\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:1.6 \n",
      "[0.78903329 0.78910118] 0.4212119903578586\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:3.2 \n",
      "[0.83018667 0.83062482] 0.3760648981210231\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:16 alpha:16.0 l:6.4 \n",
      "[0.8669703  0.86859584] 0.33012439906524227\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:0.1 \n",
      "[0.12324365 0.12324364] 0.9212685817467212\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:0.2 \n",
      "[0.12343553 0.12343553] 0.9211454758850695\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:0.4 \n",
      "[0.12366005 0.12366005] 0.921001424787238\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:0.8 \n",
      "[0.12382139 0.12382139] 0.9208979072131253\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:1.6 \n",
      "[0.12389407 0.12389408] 0.9208512698401836\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:3.2 \n",
      "[0.12391851 0.12391851] 0.9208355948832154\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:0.5 l:6.4 \n",
      "[0.12392563 0.12392563] 0.920831024804345\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:0.1 \n",
      "[0.17328069 0.17328067] 0.8890517675496692\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:0.2 \n",
      "[0.17370638 0.17370637] 0.8887765656252749\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:0.4 \n",
      "[0.17430747 0.17430747] 0.8883879352863208\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:0.8 \n",
      "[0.1748714 0.1748714] 0.8880232942419347\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:1.6 \n",
      "[0.17519155 0.17519155] 0.8878162672327629\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:3.2 \n",
      "[0.17531489 0.17531487] 0.8877365174465511\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:1.0 l:6.4 \n",
      "[0.17535314 0.17535314] 0.8877117711388092\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:0.1 \n",
      "[0.24166591 0.24166591] 0.8445341992704372\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:0.2 \n",
      "[0.24254435 0.24254434] 0.8439577864605285\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:0.4 \n",
      "[0.24396622 0.24396625] 0.8430244640967092\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:0.8 \n",
      "[0.24566802 0.24566799] 0.8419070136206941\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:1.6 \n",
      "[0.24693948 0.24693948] 0.8410717669232486\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:3.2 \n",
      "[0.24753928 0.24753928] 0.8406776569958364\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:2.0 l:6.4 \n",
      "[0.24774778 0.24774776] 0.8405406556271222\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:0.1 \n",
      "[0.33229256 0.33229256] 0.7842732752156438\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:0.2 \n",
      "[0.33398664 0.33398667] 0.783129357422718\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:0.4 \n",
      "[0.33699515 0.33699518] 0.7810961163515768\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:0.8 \n",
      "[0.34137371 0.34137374] 0.7781327979185779\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:1.6 \n",
      "[0.34572038 0.34572044] 0.7751860805547872\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:3.2 \n",
      "[0.34841499 0.34841502] 0.7733568504935207\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:4.0 l:6.4 \n",
      "[0.34955168 0.3495518 ] 0.7725845556030242\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:0.1 \n",
      "[0.44655111 0.4465512 ] 0.7052178961590668\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:0.2 \n",
      "[0.44941664 0.44941673] 0.7031773858831227\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:0.4 \n",
      "[0.45482364 0.45482385] 0.6993180269170932\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:0.8 \n",
      "[0.46389568 0.4638963 ] 0.692815487625004\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:1.6 \n",
      "[0.47552395 0.47552639] 0.6844285241513329\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:3.2 \n",
      "[0.48545912 0.48546648] 0.6772127552117506\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:8.0 l:6.4 \n",
      "[0.49112141 0.491133  ] 0.6730789550422253\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.1 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:0.1 \n",
      "[0.5779959  0.57799703] 0.6075832509178808\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:0.2 \n",
      "[0.58211207 0.58211327] 0.6043664036668167\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:0.4 \n",
      "[0.5901528  0.59015572] 0.5980473363440274\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:0.8 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:0.8 \n",
      "[0.60488206 0.60489088] 0.5863487517636478\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.6 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:1.6 \n",
      "[0.62757981 0.62761801] 0.5679765444690466\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:3.2 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:3.2 \n",
      "[0.65345114 0.65361941] 0.5464178618666817\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:6.4 epochs:60  lr:1.0  max_norm:20.0  \n",
      "d:32 alpha:16.0 l:6.4 \n",
      "[0.67457408 0.67502725] 0.5281839755292896\n"
     ]
    }
   ],
   "source": [
    "l_arr = np.array([0.1,0.2,0.4,0.8,1.6,3.2,6.4])\n",
    "#l_arr = np.array([1.])\n",
    "d_arr = np.array([4,8,16,32])\n",
    "alpha_P_arr = np.array([0.5, 1., 2., 4., 8., 16.])\n",
    "\n",
    "\n",
    "curve = np.zeros((len(seeds_arr), len(d_arr), len(alpha_P_arr), len(l_arr)))\n",
    "\n",
    "for i_d, d in enumerate(d_arr):\n",
    "    for i_a, alpha_P in enumerate(alpha_P_arr):\n",
    "        for i_l, l in enumerate(l_arr):\n",
    "            P = int(alpha_P*N)\n",
    "            print(\"########\")\n",
    "            print(\"d:{} alpha_P:{} lambda:{} epochs:{}  lr:{}  max_norm:{}  \".format(d, alpha_P, l, epochs, lr, max_grad))\n",
    "            model_name_base = \"{}_{}_GD_TEACHER_fixed{}_N_{}_P_{}_d{}_l_{}_epochs{}_lr{}_l2{}\".format(spin_type, loss_type, fixed_norm, N, P, d, l, epochs, lr, l2)\n",
    "            model_name = model_name_base + \".pth\"\n",
    "            save_idx = -1\n",
    "            h5_path = os.path.join(data_PATH, model_name_base + \".h5\")\n",
    "            # 4. Load weights + metrics\n",
    "            dataset, model, optimizer = initialize(N, P, d, lr, spin_type,device, gamma, init_Hebb=init_Hebb, downf=downf)\n",
    "            model, optimizer, metrics, epoch_saved = load_training(\n",
    "                h5_path=h5_path,\n",
    "                save_idx=save_idx,\n",
    "                model=model,\n",
    "                METRIC_NAMES=METRIC_NAMES,\n",
    "                device=device,\n",
    "            )\n",
    "            print(\"d:{} alpha:{} l:{} \".format(d, alpha_P, l))\n",
    "            epsilon = angular_loss(d, metrics[\"R\"][-1])\n",
    "            curve[i_d, i_a, i_l] = metrics[\"R\"][-1] \n",
    "            print(metrics[\"R\"], epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i_d, ax in enumerate(axs):\n",
    "    d = d_arr[i_d]\n",
    "    ax.set_title(f\"d={d}\")\n",
    "    for i_l, l in enumerate(l_arr):\n",
    "        ax.plot(alpha_P_arr, curve[i_d, :, i_l], \"o\", label=\"d={}, l={}\".format(d, l))\n",
    "    ax.set_ylim((0, 1.0))\n",
    "    ax.legend()\n",
    "\n",
    "# common labels\n",
    "fig.supxlabel(r\"$\\alpha$\", fontsize=16)\n",
    "fig.supylabel(r\"$R(\\alpha)$\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"numerical_result_N500\"\n",
    "\n",
    "np.save(\"/home/benedetti/PL_library_study/analysis/\"+name, curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=250, P=250\n"
     ]
    }
   ],
   "source": [
    "# Nature of variables and loss\n",
    "spin_type = \"vector\"\n",
    "loss_type = \"CE\"\n",
    "init_Hebb = True\n",
    "fixed_norm = False\n",
    "l2 = None\n",
    "\n",
    "# Dataset\n",
    "alpha_P = 1.\n",
    "N=250\n",
    "sigma = 1.\n",
    "P = int(alpha_P * N)\n",
    "print(\"N={}, P={}\".format(N,P))\n",
    "\n",
    "# Model and training   \n",
    "downf=0.01\n",
    "gamma=0.0\n",
    "lr = 0.05\n",
    "epochs=2000\n",
    "valid_every = 10\n",
    "max_grad = 20.\n",
    "batch_size = P\n",
    "\n",
    "METRIC_NAMES = [\n",
    "    \"epoch\",\n",
    "    \"norm_J\",\n",
    "    \"train_loss\",\n",
    "    \"train_accuracy\",\n",
    "    \"R\",\n",
    "    \"learning_rate\",\n",
    "    \"diff_hebb\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_arr = np.array([1.])\n",
    "d_arr = np.array([4,8,16,32])\n",
    "alpha_P_arr = np.array([0.5, 1., 2., 4., 8., 16.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "d:4 alpha_P:0.5 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:0.5\n",
      "[0.31787392 0.32022333 0.32131776 0.32194576 0.32235119 0.32263365\n",
      " 0.32284135 0.32300031 0.32312578 0.32322732 0.32331109 0.32338148\n",
      " 0.32344142 0.32349306 0.32353806 0.32357758 0.32361263 0.32364395\n",
      " 0.32367203 0.32369742 0.32372048 0.32374156 0.32376084 0.32377854\n",
      " 0.32379493 0.32381013 0.3238242  0.32383737 0.32384965 0.32386112\n",
      " 0.32387194 0.3238821  0.32389167 0.32390073 0.32390928 0.32391736\n",
      " 0.32392505 0.32393247 0.32393941 0.323946   0.32395241 0.32395843\n",
      " 0.32396424 0.32396987 0.32397515 0.32398024 0.32398516 0.32398987\n",
      " 0.32399446 0.32399881 0.32400301 0.32400706 0.32401109 0.32401481\n",
      " 0.32401851 0.32402208 0.32402548 0.32402882 0.32403204 0.3240352\n",
      " 0.32403821 0.32404116 0.32404402 0.32404685 0.3240495  0.32405216\n",
      " 0.32405472 0.32405722 0.32405961 0.32406199 0.32406428 0.32406658\n",
      " 0.32406878 0.3240709  0.32407302 0.32407501 0.32407704 0.32407898\n",
      " 0.32408088 0.32408276 0.32408455 0.32408631 0.32408813 0.3240898\n",
      " 0.32409149 0.3240931  0.32409474 0.32409629 0.32409778 0.3240993\n",
      " 0.32410079 0.32410222 0.32410365 0.32410505 0.32410645 0.32410783\n",
      " 0.32410914 0.32411042 0.3241117  0.32411292 0.32411417 0.32411537\n",
      " 0.32411656 0.32411772 0.32411885 0.32412001 0.32412112 0.32412219\n",
      " 0.32412326 0.32412437 0.32412538 0.32412645 0.32412741 0.32412839\n",
      " 0.3241294  0.32413036 0.32413134 0.32413223 0.32413319 0.32413408\n",
      " 0.32413495 0.3241359  0.32413673 0.32413757 0.32413846 0.3241393\n",
      " 0.3241401  0.32414091 0.32414174 0.32414252 0.32414326 0.32414407\n",
      " 0.32414484 0.32414559 0.32414627 0.32414708 0.32414779 0.32414848\n",
      " 0.32414919 0.32414985 0.32415059 0.32415131 0.3241519  0.32415253\n",
      " 0.32415324 0.3241539  0.32415456 0.32415518 0.32415581 0.32415643\n",
      " 0.32415706 0.32415766 0.32415825 0.32415879 0.32415941 0.32415998\n",
      " 0.32416058 0.32416108 0.32416174 0.32416224 0.32416275 0.32416332\n",
      " 0.32416388 0.32416436 0.3241649  0.32416543 0.32416597 0.32416648\n",
      " 0.32416695 0.32416746 0.324168   0.3241685  0.32416892 0.3241694\n",
      " 0.3241699  0.32417032 0.32417083 0.32417127 0.32417175 0.3241722\n",
      " 0.32417268 0.32417312 0.32417357 0.32417399 0.32417437 0.32417479\n",
      " 0.32417527 0.32417569 0.32417616 0.32417655 0.32417694 0.32417732\n",
      " 0.32417774 0.32417813 0.32417852 0.3241789  0.32417935 0.32417971\n",
      " 0.3241801  0.32418045 0.32418084 0.32418123 0.32418159 0.32418194\n",
      " 0.3241823  0.32418263 0.32418305 0.32418337 0.3241837  0.32418406\n",
      " 0.32418445 0.32418481 0.32418513 0.32418546 0.32418579 0.32418609\n",
      " 0.32418641 0.32418677 0.32418713 0.32418743 0.32418776 0.32418802\n",
      " 0.32418835 0.32418865 0.32418898 0.32418931 0.32418963 0.3241899\n",
      " 0.32419023 0.32419053 0.32419086 0.32419112 0.32419142 0.32419169\n",
      " 0.32419199 0.32419226 0.32419252 0.32419285 0.32419312 0.32419339\n",
      " 0.32419366 0.32419392 0.32419419 0.32419452 0.32419473 0.32419503\n",
      " 0.3241953  0.32419553 0.3241958  0.32419607 0.32419634 0.32419661\n",
      " 0.32419682 0.32419711 0.32419735 0.32419759 0.32419783 0.32419807\n",
      " 0.32419834 0.3241986  0.32419881 0.32419908 0.32419929 0.32419953\n",
      " 0.32419977 0.32420003 0.32420024 0.32420051 0.32420069 0.32420093\n",
      " 0.32420114 0.32420141 0.32420161 0.32420182 0.324202   0.32420227\n",
      " 0.32420248 0.32420269 0.32420292 0.32420313 0.32420337 0.32420355\n",
      " 0.32420379 0.324204   0.32420418 0.32420439 0.32420459 0.32420483\n",
      " 0.32420501 0.32420525 0.32420543 0.32420564 0.32420585 0.32420605\n",
      " 0.3242062  0.32420644 0.32420662 0.3242068  0.32420704 0.32420722\n",
      " 0.32420737 0.32420757 0.32420775 0.32420793 0.32420817 0.32420835\n",
      " 0.32420853 0.32420871 0.32420889 0.32420906 0.32420927 0.32420945\n",
      " 0.3242096  0.32420981 0.32420996 0.32421011 0.32421029 0.32421046\n",
      " 0.32421067 0.32421082 0.324211   0.32421121 0.32421133 0.32421151\n",
      " 0.32421172 0.32421187 0.32421198 0.32421216 0.32421234 0.32421255\n",
      " 0.32421264 0.32421285 0.324213   0.32421318 0.3242133  0.32421345\n",
      " 0.32421365 0.3242138  0.32421392 0.32421413 0.32421431 0.32421443\n",
      " 0.32421458 0.32421473 0.32421494 0.32421505 0.32421514 0.32421535\n",
      " 0.32421544 0.32421565 0.3242158  0.32421589 0.32421604 0.32421625\n",
      " 0.32421634 0.32421649 0.32421663 0.32421681 0.32421693 0.32421711\n",
      " 0.32421723 0.32421735 0.3242175  0.32421765 0.32421783 0.32421792\n",
      " 0.32421806 0.32421821 0.32421833 0.32421851 0.32421863 0.32421875\n",
      " 0.32421887 0.32421899 0.32421914 0.32421929 0.32421944 0.32421952\n",
      " 0.32421967 0.32421976 0.32421994 0.32422006 0.32422018 0.32422033\n",
      " 0.32422048 0.32422057 0.32422069 0.32422081 0.32422093 0.3242211\n",
      " 0.32422119 0.32422131 0.32422146 0.32422155 0.3242217  0.32422179\n",
      " 0.32422191 0.32422206 0.32422221 0.32422227]\n",
      "########\n",
      "d:4 alpha_P:1.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:1.0\n",
      "[0.4344725  0.4407149  0.44433263 0.44670296 0.4483805  0.4496319\n",
      " 0.45060182 0.45137557 0.45200723 0.45253235 0.45297569 0.45335478\n",
      " 0.45368257 0.45396864 0.45422029 0.45444345 0.45464244 0.45482114\n",
      " 0.45498216 0.45512816 0.45526099 0.45538244 0.45549375 0.45559618\n",
      " 0.45569077 0.45577818 0.45585939 0.45593494 0.45600533 0.45607117\n",
      " 0.45613283 0.45619062 0.45624495 0.45629609 0.45634431 0.45638981\n",
      " 0.45643291 0.45647368 0.45651242 0.45654923 0.45658401 0.45661727\n",
      " 0.45664895 0.45667917 0.45670795 0.45673552 0.45676184 0.45678714\n",
      " 0.45681128 0.45683438 0.45685667 0.45687801 0.45689857 0.4569183\n",
      " 0.45693734 0.45695555 0.4569732  0.45699021 0.45700669 0.45702252\n",
      " 0.45703781 0.45705256 0.45706686 0.45708069 0.45709413 0.45710704\n",
      " 0.45711961 0.4571318  0.45714355 0.45715502 0.45716614 0.45717698\n",
      " 0.45718747 0.45719764 0.45720747 0.45721713 0.45722649 0.45723554\n",
      " 0.45724452 0.45725313 0.45726159 0.45726976 0.45727772 0.45728543\n",
      " 0.45729306 0.45730048 0.45730776 0.45731482 0.45732167 0.45732841\n",
      " 0.457335   0.45734146 0.45734766 0.4573538  0.45735982 0.45736569\n",
      " 0.45737141 0.45737702 0.45738253 0.45738795 0.4573932  0.45739838\n",
      " 0.45740333 0.45740837 0.4574132  0.45741791 0.45742264 0.45742717\n",
      " 0.45743164 0.457436   0.45744032 0.45744458 0.45744875 0.45745274\n",
      " 0.45745674 0.45746067 0.45746458 0.45746833 0.45747203 0.45747569\n",
      " 0.45747927 0.45748276 0.45748624 0.45748964 0.45749301 0.45749629\n",
      " 0.45749959 0.45750269 0.45750579 0.45750892 0.45751187 0.45751491\n",
      " 0.45751783 0.45752078 0.45752355 0.45752636 0.4575291  0.45753184\n",
      " 0.45753449 0.45753714 0.45753968 0.45754224 0.45754474 0.45754725\n",
      " 0.45754966 0.45755202 0.45755446 0.45755672 0.45755908 0.45756131\n",
      " 0.45756355 0.4575657  0.45756793 0.45757005 0.45757219 0.45757422\n",
      " 0.45757636 0.45757839 0.45758033 0.45758232 0.45758432 0.4575862\n",
      " 0.45758811 0.45758998 0.45759183 0.45759365 0.45759544 0.45759726\n",
      " 0.45759898 0.45760071 0.45760244 0.45760411 0.45760578 0.45760751\n",
      " 0.45760912 0.4576107  0.45761237 0.45761392 0.45761546 0.45761698\n",
      " 0.45761847 0.45761999 0.45762148 0.45762295 0.45762444 0.45762584\n",
      " 0.4576273  0.45762867 0.45762995 0.45763138 0.45763275 0.45763412\n",
      " 0.4576354  0.45763671 0.457638   0.45763931 0.45764059 0.45764187\n",
      " 0.45764306 0.45764428 0.45764551 0.45764673 0.45764795 0.45764911\n",
      " 0.45765024 0.45765141 0.4576526  0.45765373 0.45765483 0.45765591\n",
      " 0.45765701 0.45765805 0.45765921 0.4576602  0.45766133 0.45766234\n",
      " 0.45766336 0.45766434 0.45766532 0.45766643 0.45766741 0.45766833\n",
      " 0.45766935 0.45767027 0.45767123 0.45767224 0.45767313 0.45767409\n",
      " 0.45767501 0.4576759  0.45767677 0.45767766 0.45767865 0.45767954\n",
      " 0.45768031 0.45768118 0.45768207 0.45768288 0.45768374 0.45768455\n",
      " 0.45768538 0.45768622 0.45768705 0.45768785 0.45768866 0.45768946\n",
      " 0.45769021 0.45769098 0.45769176 0.4576925  0.45769331 0.45769405\n",
      " 0.45769477 0.45769548 0.45769629 0.45769694 0.45769772 0.45769837\n",
      " 0.45769912 0.45769987 0.45770049 0.45770121 0.45770189 0.45770258\n",
      " 0.45770326 0.45770389 0.4577046  0.45770526 0.45770591 0.45770651\n",
      " 0.45770711 0.45770782 0.45770845 0.45770907 0.45770967 0.4577103\n",
      " 0.45771089 0.45771155 0.45771214 0.4577128  0.45771337 0.45771396\n",
      " 0.45771453 0.45771509 0.45771569 0.45771626 0.45771688 0.45771736\n",
      " 0.4577179  0.45771849 0.45771903 0.45771959 0.4577201  0.4577207\n",
      " 0.45772123 0.4577218  0.45772225 0.45772281 0.45772335 0.45772386\n",
      " 0.45772436 0.4577249  0.45772538 0.45772594 0.45772639 0.45772687\n",
      " 0.45772743 0.45772788 0.4577283  0.45772886 0.45772931 0.45772985\n",
      " 0.45773026 0.45773074 0.45773119 0.45773172 0.45773217 0.45773259\n",
      " 0.45773298 0.45773354 0.45773393 0.45773444 0.45773482 0.4577353\n",
      " 0.45773578 0.45773613 0.45773661 0.45773703 0.45773745 0.45773783\n",
      " 0.45773831 0.45773861 0.45773914 0.45773953 0.45773992 0.45774031\n",
      " 0.45774072 0.45774117 0.45774159 0.45774195 0.45774233 0.45774272\n",
      " 0.4577432  0.45774353 0.45774391 0.45774424 0.45774469 0.45774505\n",
      " 0.4577454  0.45774576 0.45774615 0.45774654 0.45774689 0.45774728\n",
      " 0.45774767 0.457748   0.45774835 0.45774871 0.45774904 0.45774949\n",
      " 0.45774972 0.45775008 0.45775044 0.45775077 0.45775115 0.45775145\n",
      " 0.45775178 0.45775214 0.4577525  0.45775285 0.45775312 0.45775351\n",
      " 0.45775381 0.45775411 0.45775446 0.45775479 0.45775515 0.45775539\n",
      " 0.45775571 0.45775604 0.45775628 0.45775667 0.45775694 0.45775726\n",
      " 0.45775756 0.45775789 0.45775819 0.45775843 0.45775875 0.45775905\n",
      " 0.45775941 0.45775965 0.45775992 0.45776027 0.45776054 0.45776078\n",
      " 0.45776108 0.45776141 0.45776168 0.45776191]\n",
      "########\n",
      "d:4 alpha_P:2.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:2.0\n",
      "[0.5713228  0.58295947 0.59111142 0.59718299 0.60191667 0.60573453\n",
      " 0.60889417 0.61156255 0.61385322 0.61584574 0.61759865 0.61915535\n",
      " 0.62054896 0.62180549 0.62294531 0.62398505 0.62493807 0.62581533\n",
      " 0.62662607 0.62737793 0.62807763 0.62873048 0.62934136 0.62991452\n",
      " 0.63045329 0.63096106 0.6314404  0.63189387 0.6323235  0.6327312\n",
      " 0.63311881 0.6334877  0.63383937 0.63417488 0.63449568 0.63480246\n",
      " 0.63509613 0.63537782 0.63564807 0.63590765 0.63615716 0.6363973\n",
      " 0.63662845 0.63685131 0.63706619 0.63727367 0.63747382 0.63766754\n",
      " 0.63785464 0.63803571 0.63821113 0.63838106 0.63854563 0.63870525\n",
      " 0.63886023 0.63901055 0.6391567  0.63929856 0.63943642 0.63957059\n",
      " 0.63970119 0.63982821 0.63995177 0.64007229 0.64018971 0.64030403\n",
      " 0.64041567 0.64052445 0.64063054 0.64073431 0.64083529 0.64093417\n",
      " 0.64103067 0.64112502 0.64121729 0.64130735 0.64139551 0.64148182\n",
      " 0.64156622 0.64164877 0.64172965 0.64180881 0.64188635 0.64196235\n",
      " 0.64203686 0.64210975 0.64218122 0.64225137 0.64232016 0.64238757\n",
      " 0.64245373 0.6425187  0.64258242 0.642645   0.64270645 0.64276671\n",
      " 0.6428259  0.64288414 0.6429413  0.64299738 0.6430527  0.64310688\n",
      " 0.64316022 0.64321274 0.64326423 0.64331502 0.64336479 0.64341396\n",
      " 0.6434623  0.64350969 0.64355654 0.64360255 0.64364785 0.64369255\n",
      " 0.64373648 0.6437797  0.64382231 0.64386433 0.64390564 0.64394641\n",
      " 0.64398658 0.6440261  0.64406508 0.64410347 0.64414144 0.64417875\n",
      " 0.64421564 0.644252   0.64428782 0.64432305 0.64435792 0.64439231\n",
      " 0.64442623 0.64445961 0.64449269 0.64452517 0.64455736 0.64458919\n",
      " 0.64462054 0.64465129 0.64468193 0.64471209 0.64474189 0.64477122\n",
      " 0.64480031 0.64482898 0.64485729 0.64488524 0.6449129  0.64494014\n",
      " 0.64496714 0.64499384 0.64502025 0.64504623 0.64507198 0.64509732\n",
      " 0.64512241 0.64514732 0.64517182 0.6451962  0.64522028 0.64524388\n",
      " 0.64526743 0.64529049 0.6453135  0.64533615 0.6453588  0.64538085\n",
      " 0.64540291 0.6454246  0.64544612 0.64546746 0.64548838 0.6455093\n",
      " 0.64552993 0.64555025 0.64557052 0.64559042 0.64561027 0.64562988\n",
      " 0.64564914 0.64566833 0.6456874  0.64570618 0.64572471 0.64574319\n",
      " 0.64576137 0.64577949 0.64579737 0.64581507 0.6458326  0.64585006\n",
      " 0.64586723 0.64588428 0.64590114 0.64591795 0.64593452 0.64595091\n",
      " 0.64596719 0.64598328 0.64599925 0.64601505 0.64603072 0.64604622\n",
      " 0.64606166 0.64607692 0.64609194 0.64610696 0.64612186 0.64613658\n",
      " 0.64615107 0.64616555 0.64617991 0.6461941  0.64620823 0.64622217\n",
      " 0.64623594 0.64624971 0.64626324 0.64627677 0.64629018 0.64630342\n",
      " 0.64631659 0.64632958 0.64634258 0.64635533 0.64636809 0.64638072\n",
      " 0.64639336 0.6464057  0.64641798 0.64643019 0.64644223 0.64645427\n",
      " 0.6464662  0.64647812 0.6464898  0.64650142 0.64651299 0.64652455\n",
      " 0.64653587 0.64654702 0.64655834 0.64656937 0.64658046 0.64659131\n",
      " 0.64660221 0.646613   0.64662373 0.64663428 0.64664489 0.64665526\n",
      " 0.64666563 0.646676   0.6466862  0.64669627 0.64670646 0.64671642\n",
      " 0.64672625 0.6467362  0.64674592 0.64675564 0.64676535 0.64677495\n",
      " 0.64678442 0.64679378 0.6468032  0.64681256 0.64682174 0.64683092\n",
      " 0.64684004 0.6468491  0.64685804 0.64686698 0.64687586 0.64688462\n",
      " 0.64689338 0.64690202 0.64691067 0.64691919 0.64692771 0.64693612\n",
      " 0.64694458 0.64695293 0.64696115 0.64696938 0.64697754 0.64698565\n",
      " 0.64699382 0.6470018  0.64700973 0.6470176  0.64702547 0.64703333\n",
      " 0.64704096 0.64704871 0.64705634 0.64706397 0.64707148 0.64707905\n",
      " 0.6470865  0.64709389 0.64710122 0.64710867 0.64711583 0.6471231\n",
      " 0.64713031 0.64713746 0.6471445  0.64715159 0.64715856 0.64716554\n",
      " 0.64717245 0.64717931 0.64718616 0.6471929  0.64719975 0.64720643\n",
      " 0.64721304 0.64721966 0.64722627 0.64723289 0.64723945 0.64724594\n",
      " 0.64725232 0.6472587  0.64726508 0.64727139 0.64727771 0.64728391\n",
      " 0.64729017 0.64729637 0.64730245 0.64730859 0.64731467 0.64732069\n",
      " 0.64732677 0.64733273 0.64733863 0.64734453 0.64735037 0.64735615\n",
      " 0.64736211 0.64736784 0.6473735  0.64737928 0.64738494 0.6473906\n",
      " 0.64739615 0.64740175 0.64740735 0.64741278 0.64741832 0.6474238\n",
      " 0.64742923 0.64743465 0.64743996 0.64744526 0.64745063 0.64745599\n",
      " 0.64746124 0.64746636 0.64747173 0.64747679 0.64748204 0.64748704\n",
      " 0.64749223 0.6474973  0.64750236 0.64750737 0.64751232 0.64751732\n",
      " 0.64752221 0.64752716 0.64753193 0.64753687 0.6475417  0.64754653\n",
      " 0.64755124 0.64755613 0.64756078 0.64756548 0.64757019 0.6475749\n",
      " 0.64757949 0.64758408 0.64758873 0.64759332 0.64759791 0.64760238\n",
      " 0.64760691 0.64761138 0.64761573 0.64762026 0.64762473 0.64762908\n",
      " 0.64763343 0.64763778 0.64764214 0.64764553]\n",
      "########\n",
      "d:4 alpha_P:4.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:4.0\n",
      "[0.69535249 0.70954847 0.72061461 0.72951138 0.7368685  0.7430926\n",
      " 0.74845552 0.75314546 0.7572971  0.76101005 0.76435965 0.7674039\n",
      " 0.77018851 0.77274996 0.77511811 0.77731693 0.77936673 0.78128445\n",
      " 0.78308421 0.78477836 0.78637731 0.78789014 0.78932458 0.79068762\n",
      " 0.79198515 0.79322255 0.79440492 0.79553586 0.79661959 0.79765928\n",
      " 0.79865813 0.79961872 0.80054379 0.80143547 0.80229563 0.80312651\n",
      " 0.80392969 0.80470681 0.8054592  0.80618829 0.80689532 0.8075816\n",
      " 0.80824792 0.80889535 0.80952495 0.81013745 0.81073368 0.81131434\n",
      " 0.81188023 0.81243193 0.8129701  0.81349528 0.81400812 0.81450897\n",
      " 0.81499839 0.81547683 0.81594467 0.81640244 0.81685048 0.81728894\n",
      " 0.81771845 0.8181392  0.81855148 0.81895578 0.81935209 0.81974083\n",
      " 0.82012218 0.82049656 0.82086408 0.82122487 0.82157928 0.82192737\n",
      " 0.8222695  0.82260585 0.82293636 0.82326138 0.82358098 0.82389539\n",
      " 0.82420504 0.82450938 0.82480919 0.8251043  0.82539481 0.82568097\n",
      " 0.8259629  0.82624066 0.8265143  0.8267839  0.82704979 0.82731181\n",
      " 0.8275702  0.82782495 0.82807642 0.82832432 0.82856876 0.82881021\n",
      " 0.82904828 0.82928336 0.82951552 0.82974446 0.82997072 0.830194\n",
      " 0.83041459 0.83063245 0.83084756 0.83106017 0.83127028 0.83147788\n",
      " 0.83168304 0.83188576 0.83208615 0.83228427 0.83248007 0.83267379\n",
      " 0.83286524 0.83305466 0.83324206 0.83342725 0.83361053 0.83379191\n",
      " 0.83397132 0.83414888 0.83432454 0.83449847 0.83467048 0.83484083\n",
      " 0.8350094  0.83517641 0.83534157 0.83550519 0.83566713 0.83582765\n",
      " 0.83598655 0.83614391 0.83629966 0.83645415 0.83660704 0.83675861\n",
      " 0.83690876 0.83705747 0.83720475 0.83735085 0.83749563 0.83763897\n",
      " 0.83778119 0.83792216 0.83806169 0.83820015 0.83833748 0.83847356\n",
      " 0.83860844 0.8387422  0.83887494 0.83900642 0.8391369  0.83926618\n",
      " 0.83939457 0.83952177 0.83964801 0.83977324 0.83989739 0.84002066\n",
      " 0.84014291 0.8402642  0.84038454 0.84050387 0.84062231 0.84073997\n",
      " 0.84085661 0.8409723  0.84108716 0.84120119 0.84131449 0.84142667\n",
      " 0.84153831 0.84164894 0.84175891 0.84186786 0.84197623 0.84208375\n",
      " 0.84219056 0.84229648 0.84240174 0.84250623 0.84261012 0.84271306\n",
      " 0.84281534 0.84291714 0.843018   0.84311819 0.84321785 0.84331679\n",
      " 0.84341484 0.84351254 0.84360939 0.84370571 0.84380138 0.84389639\n",
      " 0.84399074 0.84408462 0.84417778 0.84427053 0.84436238 0.84445375\n",
      " 0.84454465 0.84463495 0.8447246  0.8448137  0.84490234 0.84499031\n",
      " 0.84507787 0.84516478 0.8452512  0.84533721 0.84542251 0.84550744\n",
      " 0.84559172 0.84567571 0.84575897 0.84584188 0.84592426 0.84600621\n",
      " 0.84608763 0.84616852 0.8462491  0.84632903 0.84640855 0.8464877\n",
      " 0.84656644 0.84664458 0.84672242 0.84679967 0.84687656 0.84695321\n",
      " 0.84702903 0.84710479 0.84718001 0.84725481 0.8473292  0.84740317\n",
      " 0.84747684 0.84754997 0.84762275 0.84769535 0.84776729 0.847839\n",
      " 0.84791023 0.84798104 0.84805161 0.8481217  0.84819162 0.848261\n",
      " 0.84833008 0.8483988  0.84846717 0.84853512 0.84860289 0.84867018\n",
      " 0.84873718 0.84880382 0.84887022 0.84893614 0.84900177 0.84906721\n",
      " 0.84913224 0.84919691 0.84926128 0.84932536 0.84938914 0.84945256\n",
      " 0.84951574 0.8495785  0.84964114 0.84970343 0.8497653  0.84982693\n",
      " 0.84988838 0.84994936 0.85001016 0.85007066 0.85013098 0.85019082\n",
      " 0.85025054 0.85030991 0.85036898 0.85042793 0.8504864  0.85054469\n",
      " 0.85060287 0.85066068 0.85071814 0.85077542 0.8508324  0.85088915\n",
      " 0.85094565 0.85100204 0.85105789 0.85111368 0.85116929 0.85122454\n",
      " 0.8512795  0.85133439 0.85138881 0.85144329 0.85149729 0.85155118\n",
      " 0.85160482 0.85165834 0.85171139 0.85176432 0.85181707 0.85186964\n",
      " 0.85192186 0.85197389 0.85202581 0.85207742 0.8521288  0.85218\n",
      " 0.85223091 0.85228175 0.85233235 0.85238272 0.85243291 0.85248291\n",
      " 0.85253263 0.85258228 0.85263163 0.8526808  0.85272956 0.85277843\n",
      " 0.85282707 0.85287547 0.85292363 0.85297167 0.85301942 0.85306716\n",
      " 0.8531146  0.85316181 0.85320884 0.85325599 0.85330254 0.85334909\n",
      " 0.8533954  0.85344166 0.85348761 0.85353351 0.85357904 0.85362452\n",
      " 0.85366988 0.85371494 0.85375994 0.85380471 0.85384935 0.85389394\n",
      " 0.85393828 0.85398227 0.85402626 0.85407019 0.85411388 0.85415733\n",
      " 0.85420072 0.85424381 0.85428685 0.85432965 0.85437238 0.85441494\n",
      " 0.85445732 0.85449964 0.85454172 0.85458368 0.85462558 0.85466719\n",
      " 0.85470861 0.8547501  0.85479128 0.85483229 0.85487324 0.85491407\n",
      " 0.8549546  0.85499519 0.85503554 0.85507578 0.85511589 0.85515571\n",
      " 0.85519564 0.85523534 0.85527492 0.8553142  0.85535347 0.85539269\n",
      " 0.85543156 0.85547048 0.85550916 0.8555479  0.85558635 0.85562474\n",
      " 0.85566288 0.85570097 0.85573894 0.85576916]\n",
      "########\n",
      "d:4 alpha_P:8.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:8.0\n",
      "[0.81429452 0.8264215  0.83638203 0.84467733 0.85170788 0.85776287\n",
      " 0.86305076 0.86772317 0.87189353 0.87564808 0.87905353 0.88216257\n",
      " 0.88501722 0.88765156 0.89009374 0.89236665 0.89448994 0.8964799\n",
      " 0.89835078 0.90011412 0.90178031 0.90335858 0.90485656 0.90628105\n",
      " 0.90763843 0.90893382 0.91017205 0.91135746 0.91249377 0.91358459\n",
      " 0.9146328  0.91564125 0.91661274 0.91754937 0.91845334 0.9193266\n",
      " 0.92017096 0.92098796 0.92177916 0.92254603 0.92328972 0.92401153\n",
      " 0.92471248 0.92539364 0.92605609 0.92670029 0.92732763 0.92793852\n",
      " 0.92853397 0.9291144  0.92968041 0.93023288 0.93077242 0.93129915\n",
      " 0.93181384 0.9323169  0.93280882 0.93329    0.93376094 0.93422192\n",
      " 0.93467325 0.93511552 0.93554866 0.93597329 0.93638939 0.93679768\n",
      " 0.93719822 0.93759114 0.93797678 0.93835545 0.93872726 0.93909252\n",
      " 0.93945134 0.93980384 0.94015044 0.94049108 0.94082612 0.94115561\n",
      " 0.94147962 0.94179851 0.94211245 0.94242132 0.94272548 0.94302487\n",
      " 0.94331968 0.94361013 0.94389623 0.94417816 0.94445592 0.94472969\n",
      " 0.94499969 0.94526583 0.94552809 0.94578683 0.94604218 0.94629383\n",
      " 0.94654226 0.9467873  0.94702923 0.94726795 0.94750357 0.94773608\n",
      " 0.9479658  0.94819272 0.94841665 0.9486379  0.94885647 0.94907224\n",
      " 0.94928557 0.94949627 0.94970447 0.94991034 0.95011377 0.95031494\n",
      " 0.95051366 0.95071024 0.95090461 0.95109671 0.95128679 0.95147479\n",
      " 0.95166069 0.95184457 0.95202667 0.95220667 0.95238477 0.95256114\n",
      " 0.95273548 0.9529081  0.95307904 0.9532482  0.95341557 0.95358133\n",
      " 0.95374548 0.95390809 0.95406884 0.95422828 0.95438606 0.95454222\n",
      " 0.95469707 0.95485049 0.95500237 0.95515281 0.95530188 0.95544958\n",
      " 0.95559597 0.95574099 0.95588475 0.95602721 0.95616823 0.95630831\n",
      " 0.95644677 0.95658427 0.95672059 0.95685577 0.95698953 0.95712233\n",
      " 0.95725411 0.95738471 0.95751417 0.9576425  0.95776969 0.95789605\n",
      " 0.95802122 0.95814526 0.95826864 0.95839095 0.95851219 0.95863235\n",
      " 0.95875168 0.95886993 0.95898753 0.95910388 0.95921981 0.95933443\n",
      " 0.95944822 0.95956123 0.9596734  0.95978469 0.95989519 0.96000487\n",
      " 0.96011358 0.96022171 0.960329   0.96043545 0.96054113 0.96064609\n",
      " 0.9607501  0.96085358 0.96095622 0.96105838 0.96115947 0.96126002\n",
      " 0.96135986 0.96145898 0.96155751 0.96165514 0.9617523  0.96184874\n",
      " 0.96194452 0.96203965 0.96213412 0.96222794 0.96232134 0.96241391\n",
      " 0.962506   0.96259743 0.96268815 0.96277839 0.96286798 0.96295714\n",
      " 0.96304578 0.96313351 0.96322101 0.9633078  0.96339399 0.96347988\n",
      " 0.96356499 0.96364975 0.96373385 0.96381748 0.96390057 0.9639833\n",
      " 0.96406531 0.96414685 0.96422791 0.96430862 0.96438885 0.96446842\n",
      " 0.96454763 0.96462637 0.96470469 0.96478242 0.96485996 0.96493673\n",
      " 0.96501321 0.9650892  0.96516484 0.96524    0.96531469 0.96538889\n",
      " 0.96546292 0.9655363  0.96560937 0.96568203 0.96575433 0.96582615\n",
      " 0.96589768 0.96596861 0.9660393  0.96610969 0.96617943 0.96624905\n",
      " 0.96631813 0.96638697 0.96645534 0.96652335 0.966591   0.96665835\n",
      " 0.96672541 0.96679193 0.96685809 0.96692401 0.96698964 0.9670549\n",
      " 0.96711975 0.96718431 0.96724862 0.96731251 0.96737605 0.96743929\n",
      " 0.96750224 0.96756488 0.96762723 0.96768922 0.96775085 0.96781218\n",
      " 0.96787339 0.96793401 0.96799451 0.96805471 0.9681145  0.96817416\n",
      " 0.96823347 0.96829242 0.96835113 0.96840966 0.96846777 0.96852553\n",
      " 0.96858323 0.96864063 0.96869773 0.96875447 0.96881109 0.9688673\n",
      " 0.96892327 0.968979   0.96903455 0.96908975 0.96914476 0.96919942\n",
      " 0.96925402 0.9693082  0.96936202 0.96941578 0.96946931 0.9695226\n",
      " 0.96957558 0.96962839 0.96968096 0.96973318 0.96978533 0.96983707\n",
      " 0.96988875 0.96994013 0.96999127 0.97004217 0.97009289 0.97014344\n",
      " 0.97019374 0.97024369 0.97029358 0.97034323 0.97039264 0.97044176\n",
      " 0.97049075 0.97053957 0.97058815 0.97063655 0.97068477 0.97073281\n",
      " 0.97078037 0.97082788 0.97087532 0.97092241 0.9709695  0.97101623\n",
      " 0.97106302 0.97110921 0.97115546 0.97120154 0.97124749 0.97129291\n",
      " 0.97133863 0.97138369 0.97142893 0.97147369 0.97151852 0.9715631\n",
      " 0.97160733 0.97165167 0.9716956  0.97173941 0.97178322 0.97182673\n",
      " 0.97186995 0.97191322 0.97195607 0.97199917 0.97204173 0.97208411\n",
      " 0.97212654 0.9721688  0.97221076 0.97225249 0.97229433 0.97233588\n",
      " 0.97237724 0.97241837 0.97245955 0.97250032 0.97254121 0.97258174\n",
      " 0.97262216 0.97266251 0.97270262 0.97274268 0.97278249 0.97282219\n",
      " 0.97286189 0.97290128 0.9729405  0.97297961 0.97301853 0.97305745\n",
      " 0.97309601 0.97313464 0.97317308 0.97321147 0.9732495  0.97328764\n",
      " 0.97332549 0.97336322 0.97340083 0.9734382  0.97347558 0.97351283\n",
      " 0.97354978 0.97358668 0.97362363 0.9736529 ]\n",
      "########\n",
      "d:4 alpha_P:16.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:4 alpha:16.0\n",
      "[0.89129776 0.8999359  0.90718991 0.91331911 0.91855961 0.92309529\n",
      " 0.92706525 0.93057448 0.93370336 0.9365145  0.93905699 0.94137043\n",
      " 0.94348621 0.94543058 0.94722497 0.94888693 0.95043218 0.95187283\n",
      " 0.95322037 0.95448405 0.95567161 0.95679063 0.957847   0.95884627\n",
      " 0.95979339 0.96069235 0.96154702 0.96236086 0.96313709 0.96387833\n",
      " 0.96458656 0.96526486 0.96591473 0.96653807 0.96713656 0.96771181\n",
      " 0.96826524 0.96879804 0.96931154 0.96980679 0.97028482 0.97074646\n",
      " 0.97119266 0.97162414 0.97204196 0.97244626 0.97283828 0.97321826\n",
      " 0.97358692 0.9739446  0.97429198 0.97462958 0.97495764 0.97527659\n",
      " 0.97558701 0.97588921 0.9761833  0.97646987 0.97674906 0.97702128\n",
      " 0.97728676 0.97754586 0.97779852 0.97804517 0.97828615 0.97852165\n",
      " 0.97875148 0.97897637 0.97919613 0.97941124 0.97962147 0.97982728\n",
      " 0.98002875 0.98022592 0.98041904 0.98060828 0.9807936  0.98097533\n",
      " 0.98115337 0.98132789 0.98149914 0.98166698 0.98183185 0.98199362\n",
      " 0.98215216 0.98230803 0.98246086 0.98261106 0.98275864 0.98290348\n",
      " 0.98304582 0.98318589 0.9833234  0.98345852 0.98359156 0.98372221\n",
      " 0.98385072 0.9839772  0.98410165 0.98422408 0.98434454 0.98446304\n",
      " 0.98457974 0.98469466 0.98480785 0.98491919 0.98502898 0.98513716\n",
      " 0.98524356 0.98534846 0.98545194 0.98555374 0.98565418 0.98575306\n",
      " 0.98585075 0.98594689 0.98604172 0.98613524 0.98622751 0.98631853\n",
      " 0.98640835 0.98649687 0.98658425 0.98667043 0.98675549 0.98683953\n",
      " 0.98692238 0.9870041  0.98708487 0.98716468 0.98724347 0.98732114\n",
      " 0.98739797 0.98747379 0.98754865 0.98762268 0.98769569 0.98776788\n",
      " 0.98783922 0.98790985 0.98797947 0.98804826 0.98811626 0.98818356\n",
      " 0.9882502  0.98831576 0.98838085 0.98844498 0.98850852 0.98857135\n",
      " 0.98863339 0.98869485 0.98875558 0.98881567 0.98887509 0.98893392\n",
      " 0.98899204 0.98904961 0.98910642 0.98916286 0.98921853 0.98927367\n",
      " 0.98932832 0.98938239 0.98943585 0.98948866 0.98954111 0.98959303\n",
      " 0.98964429 0.98969507 0.9897455  0.98979533 0.98984468 0.98989344\n",
      " 0.9899419  0.9899897  0.99003714 0.99008435 0.9901306  0.99017668\n",
      " 0.99022245 0.99026763 0.9903124  0.99035674 0.99040079 0.9904443\n",
      " 0.99048769 0.99053031 0.99057287 0.99061489 0.99065644 0.99069762\n",
      " 0.99073851 0.9907791  0.99081928 0.99085915 0.99089879 0.99093771\n",
      " 0.99097645 0.99101502 0.9910531  0.99109083 0.99112833 0.99116564\n",
      " 0.99120235 0.99123889 0.99127525 0.99131107 0.99134672 0.99138194\n",
      " 0.99141717 0.99145186 0.99148637 0.99152058 0.9915545  0.99158818\n",
      " 0.99162143 0.99165457 0.99168748 0.99172014 0.99175227 0.99178445\n",
      " 0.99181628 0.99184793 0.99187928 0.99191028 0.99194115 0.99197179\n",
      " 0.99200219 0.99203229 0.99206221 0.99209207 0.9921214  0.99215072\n",
      " 0.99217975 0.9922086  0.99223727 0.99226558 0.99229378 0.99232179\n",
      " 0.99234962 0.9923771  0.99240452 0.9924317  0.99245864 0.99248552\n",
      " 0.99251205 0.99253851 0.99256468 0.99259067 0.99261665 0.99264234\n",
      " 0.99266785 0.99269319 0.99271822 0.99274319 0.99276805 0.99279279\n",
      " 0.9928171  0.99284148 0.9928655  0.99288952 0.99291337 0.99293709\n",
      " 0.99296051 0.99298394 0.99300706 0.99303007 0.9930529  0.99307567\n",
      " 0.99309832 0.99312061 0.9931429  0.99316514 0.99318719 0.99320889\n",
      " 0.99323064 0.99325234 0.99327368 0.99329501 0.99331617 0.99333727\n",
      " 0.99335814 0.99337888 0.9933995  0.99342012 0.99344033 0.99346054\n",
      " 0.99348074 0.99350077 0.99352074 0.99354059 0.99356014 0.99357969\n",
      " 0.99359894 0.99361837 0.99363756 0.99365658 0.99367553 0.99369431\n",
      " 0.99371302 0.99373162 0.99375015 0.99376851 0.99378675 0.99380499\n",
      " 0.99382317 0.99384099 0.99385887 0.99387658 0.99389428 0.9939118\n",
      " 0.99392921 0.99394667 0.9939639  0.99398112 0.99399817 0.99401498\n",
      " 0.99403191 0.99404871 0.99406528 0.99408197 0.99409837 0.99411476\n",
      " 0.99413115 0.99414736 0.99416345 0.99417961 0.99419558 0.99421132\n",
      " 0.99422699 0.99424273 0.99425834 0.99427396 0.9942894  0.99430466\n",
      " 0.99431998 0.99433517 0.99435025 0.99436533 0.9943803  0.99439508\n",
      " 0.99440986 0.99442458 0.99443918 0.99445391 0.99446827 0.9944827\n",
      " 0.994497   0.99451131 0.99452543 0.99453974 0.99455363 0.99456763\n",
      " 0.99458146 0.99459535 0.99460906 0.99462283 0.9946363  0.99464989\n",
      " 0.99466336 0.99467677 0.99469024 0.99470347 0.99471664 0.99472976\n",
      " 0.99474287 0.99475586 0.9947688  0.99478179 0.99479449 0.99480742\n",
      " 0.99482    0.99483269 0.99484521 0.99485773 0.99487025 0.99488258\n",
      " 0.99489492 0.9949072  0.99491936 0.99493146 0.99494362 0.99495572\n",
      " 0.99496776 0.99497956 0.99499142 0.99500328 0.99501503 0.99502677\n",
      " 0.99503839 0.99505007 0.99506158 0.99507302 0.99508446 0.99509591\n",
      " 0.99510711 0.99511844 0.99512964 0.9951387 ]\n",
      "########\n",
      "d:8 alpha_P:0.5 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:0.5\n",
      "[0.24487905 0.24606207 0.24667709 0.24704671 0.24729031 0.24746133\n",
      " 0.24758711 0.24768302 0.24775818 0.24781848 0.24786784 0.24790883\n",
      " 0.24794336 0.24797282 0.24799824 0.24802031 0.24803966 0.2480568\n",
      " 0.24807203 0.2480856  0.24809784 0.24810889 0.24811894 0.24812809\n",
      " 0.24813646 0.24814413 0.24815121 0.2481578  0.24816382 0.24816947\n",
      " 0.24817468 0.24817961 0.2481842  0.24818854 0.24819256 0.24819636\n",
      " 0.24819997 0.24820334 0.24820653 0.24820958 0.24821246 0.24821518\n",
      " 0.24821776 0.24822026 0.24822262 0.24822488 0.248227   0.24822906\n",
      " 0.24823104 0.24823292 0.24823473 0.24823646 0.24823813 0.24823974\n",
      " 0.24824128 0.24824277 0.2482442  0.24824557 0.24824691 0.24824822\n",
      " 0.24824941 0.24825062 0.24825177 0.2482529  0.24825397 0.24825503\n",
      " 0.24825607 0.24825704 0.24825801 0.24825896 0.24825983 0.24826075\n",
      " 0.24826159 0.24826239 0.24826322 0.24826401 0.24826476 0.24826548\n",
      " 0.24826623 0.24826694 0.24826761 0.24826832 0.24826895 0.2482696\n",
      " 0.2482702  0.24827079 0.24827141 0.248272   0.24827255 0.24827309\n",
      " 0.24827366 0.24827419 0.2482747  0.24827518 0.2482757  0.24827617\n",
      " 0.24827665 0.24827713 0.2482776  0.24827804 0.24827847 0.24827892\n",
      " 0.2482793  0.24827974 0.24828014 0.24828056 0.24828093 0.24828127\n",
      " 0.24828167 0.24828205 0.24828242 0.24828275 0.2482831  0.24828345\n",
      " 0.2482838  0.24828412 0.24828443 0.24828476 0.24828506 0.24828538\n",
      " 0.24828568 0.24828598 0.24828628 0.24828659 0.24828687 0.24828711\n",
      " 0.24828741 0.24828769 0.24828793 0.24828821 0.24828848 0.24828874\n",
      " 0.24828899 0.24828923 0.24828944 0.2482897  0.24828993 0.24829017\n",
      " 0.24829039 0.24829063 0.24829085 0.24829103 0.24829128 0.24829151\n",
      " 0.2482917  0.24829194 0.2482921  0.24829231 0.24829251 0.24829273\n",
      " 0.24829292 0.2482931  0.2482933  0.24829349 0.24829367 0.24829386\n",
      " 0.24829401 0.24829419 0.24829438 0.24829456 0.24829473 0.24829489\n",
      " 0.24829504 0.24829522 0.24829538 0.24829555 0.24829569 0.24829584\n",
      " 0.24829602 0.24829617 0.24829635 0.24829648 0.24829663 0.24829678\n",
      " 0.24829692 0.24829705 0.2482972  0.24829735 0.24829748 0.2482976\n",
      " 0.24829775 0.24829791 0.248298   0.24829814 0.24829829 0.24829841\n",
      " 0.24829853 0.24829869 0.24829881 0.24829891 0.24829902 0.24829918\n",
      " 0.24829924 0.24829939 0.24829949 0.24829961 0.24829975 0.24829987\n",
      " 0.24829999 0.24830008 0.24830019 0.2483003  0.24830046 0.24830051\n",
      " 0.24830061 0.2483007  0.24830084 0.24830094 0.24830103 0.2483011\n",
      " 0.24830122 0.24830136 0.24830143 0.24830152 0.24830167 0.24830171\n",
      " 0.2483018  0.24830191 0.248302   0.24830209 0.24830216 0.24830224\n",
      " 0.24830239 0.24830246 0.24830261 0.24830264 0.24830271 0.24830279\n",
      " 0.24830291 0.24830298 0.24830307 0.24830319 0.24830322 0.24830329\n",
      " 0.24830335 0.24830346 0.24830356 0.24830365 0.24830371 0.24830379\n",
      " 0.24830388 0.24830391 0.24830399 0.24830408 0.24830414 0.24830425\n",
      " 0.24830432 0.24830437 0.24830446 0.24830456 0.24830461 0.24830467\n",
      " 0.24830477 0.2483048  0.24830489 0.24830495 0.24830502 0.24830507\n",
      " 0.24830516 0.24830522 0.24830529 0.24830531 0.2483054  0.24830545\n",
      " 0.24830553 0.2483056  0.24830569 0.24830572 0.2483058  0.24830586\n",
      " 0.24830592 0.24830596 0.24830605 0.2483061  0.24830616 0.24830621\n",
      " 0.24830626 0.24830633 0.24830642 0.24830645 0.24830653 0.24830656\n",
      " 0.24830662 0.24830668 0.24830672 0.24830677 0.24830684 0.24830686\n",
      " 0.24830697 0.248307   0.24830705 0.24830711 0.24830714 0.2483072\n",
      " 0.24830726 0.2483073  0.24830739 0.24830742 0.24830747 0.24830754\n",
      " 0.24830759 0.24830763 0.24830766 0.24830773 0.24830776 0.24830782\n",
      " 0.24830785 0.24830791 0.24830797 0.24830802 0.24830808 0.24830809\n",
      " 0.24830817 0.24830821 0.24830823 0.24830827 0.24830835 0.24830838\n",
      " 0.24830842 0.24830849 0.24830852 0.24830857 0.2483086  0.24830864\n",
      " 0.24830869 0.24830875 0.24830879 0.24830884 0.24830887 0.24830891\n",
      " 0.24830891 0.24830899 0.24830903 0.24830909 0.24830912 0.24830917\n",
      " 0.24830921 0.24830922 0.24830927 0.2483093  0.24830934 0.2483094\n",
      " 0.24830943 0.24830946 0.24830949 0.24830954 0.24830958 0.24830963\n",
      " 0.24830963 0.2483097  0.24830973 0.24830978 0.24830979 0.24830984\n",
      " 0.24830985 0.24830991 0.24830996 0.24830997 0.24831006 0.24831007\n",
      " 0.24831009 0.2483101  0.24831015 0.24831021 0.24831022 0.2483103\n",
      " 0.24831031 0.24831033 0.24831037 0.2483104  0.24831043 0.24831048\n",
      " 0.24831049 0.24831054 0.24831057 0.24831058 0.24831064 0.24831067\n",
      " 0.24831067 0.24831073 0.24831076 0.24831077 0.24831085 0.24831085\n",
      " 0.24831089 0.24831094 0.24831094 0.24831095 0.24831098 0.24831103\n",
      " 0.24831107 0.2483111  0.24831115 0.24831116 0.24831122 0.24831121\n",
      " 0.24831124 0.24831128 0.24831131 0.24831133]\n",
      "########\n",
      "d:8 alpha_P:1.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:1.0\n",
      "[0.32993165 0.33279613 0.33465621 0.33594665 0.33688939 0.33760566\n",
      " 0.33816668 0.338617   0.33898562 0.33929241 0.33955118 0.33977222\n",
      " 0.3399629  0.3401289  0.3402746  0.34040332 0.34051779 0.34062013\n",
      " 0.34071219 0.34079531 0.34087068 0.34093937 0.34100214 0.34105968\n",
      " 0.34111261 0.34116149 0.34120673 0.3412486  0.34128755 0.34132391\n",
      " 0.34135783 0.34138948 0.34141925 0.34144714 0.34147343 0.34149814\n",
      " 0.3415215  0.34154353 0.34156436 0.34158415 0.34160289 0.34162065\n",
      " 0.34163755 0.34165365 0.34166896 0.34168363 0.34169751 0.34171087\n",
      " 0.34172362 0.34173587 0.34174752 0.34175876 0.34176946 0.3417798\n",
      " 0.34178969 0.34179929 0.34180844 0.34181726 0.34182578 0.34183398\n",
      " 0.34184188 0.34184954 0.34185687 0.34186399 0.3418709  0.34187755\n",
      " 0.34188396 0.34189019 0.34189624 0.34190208 0.34190771 0.34191322\n",
      " 0.34191859 0.34192371 0.34192878 0.34193364 0.34193838 0.34194297\n",
      " 0.34194744 0.34195182 0.34195608 0.34196019 0.34196422 0.34196809\n",
      " 0.3419719  0.3419756  0.34197927 0.34198278 0.34198621 0.34198958\n",
      " 0.34199283 0.34199604 0.34199917 0.34200221 0.34200519 0.34200811\n",
      " 0.34201095 0.34201372 0.34201643 0.34201908 0.34202167 0.34202424\n",
      " 0.34202668 0.34202918 0.34203157 0.34203386 0.34203622 0.34203839\n",
      " 0.3420406  0.34204277 0.34204489 0.34204698 0.342049   0.34205097\n",
      " 0.34205291 0.34205484 0.34205678 0.34205857 0.34206033 0.34206215\n",
      " 0.34206387 0.34206563 0.34206727 0.34206891 0.34207052 0.34207213\n",
      " 0.34207368 0.34207526 0.34207675 0.34207827 0.34207976 0.34208116\n",
      " 0.34208256 0.34208396 0.3420853  0.3420867  0.34208801 0.3420893\n",
      " 0.34209058 0.34209183 0.34209308 0.34209433 0.34209549 0.34209669\n",
      " 0.34209782 0.34209898 0.34210014 0.34210122 0.34210232 0.34210345\n",
      " 0.34210449 0.34210557 0.34210661 0.34210762 0.34210861 0.34210962\n",
      " 0.3421106  0.3421115  0.34211251 0.34211344 0.34211436 0.34211528\n",
      " 0.34211618 0.3421171  0.34211797 0.34211883 0.34211969 0.34212053\n",
      " 0.34212136 0.34212217 0.34212297 0.34212381 0.34212461 0.34212536\n",
      " 0.34212613 0.34212691 0.34212765 0.3421284  0.34212911 0.34212983\n",
      " 0.34213054 0.34213123 0.34213194 0.3421326  0.34213334 0.34213397\n",
      " 0.34213465 0.34213528 0.34213597 0.34213662 0.34213722 0.34213787\n",
      " 0.3421385  0.34213907 0.34213969 0.34214026 0.34214091 0.34214151\n",
      " 0.34214208 0.34214264 0.34214315 0.3421438  0.34214431 0.34214482\n",
      " 0.34214544 0.34214592 0.34214652 0.34214702 0.34214756 0.34214807\n",
      " 0.34214851 0.34214908 0.34214953 0.34215009 0.34215054 0.34215096\n",
      " 0.34215149 0.34215197 0.34215245 0.34215289 0.34215337 0.34215385\n",
      " 0.34215426 0.34215471 0.34215519 0.34215564 0.34215608 0.34215644\n",
      " 0.34215686 0.34215733 0.34215775 0.34215814 0.3421585  0.34215891\n",
      " 0.34215936 0.34215975 0.34216014 0.34216052 0.34216091 0.34216127\n",
      " 0.34216166 0.34216207 0.34216246 0.34216282 0.34216318 0.34216353\n",
      " 0.34216386 0.34216428 0.34216458 0.3421649  0.34216535 0.34216562\n",
      " 0.34216598 0.34216627 0.34216666 0.34216699 0.34216732 0.34216765\n",
      " 0.34216797 0.3421683  0.3421686  0.3421689  0.34216926 0.34216958\n",
      " 0.34216985 0.34217018 0.34217045 0.34217075 0.34217104 0.34217137\n",
      " 0.34217164 0.34217191 0.34217221 0.3421725  0.3421728  0.34217313\n",
      " 0.34217337 0.34217364 0.34217393 0.3421742  0.34217444 0.34217471\n",
      " 0.34217498 0.34217525 0.34217554 0.34217575 0.34217599 0.34217632\n",
      " 0.34217656 0.3421768  0.34217703 0.3421773  0.34217754 0.34217781\n",
      " 0.34217799 0.34217829 0.34217849 0.34217873 0.34217897 0.34217924\n",
      " 0.34217945 0.34217969 0.34217992 0.34218013 0.34218037 0.34218058\n",
      " 0.34218082 0.34218106 0.34218124 0.34218144 0.34218171 0.34218186\n",
      " 0.3421821  0.34218234 0.34218258 0.34218276 0.34218296 0.34218314\n",
      " 0.34218332 0.34218359 0.3421838  0.34218395 0.34218416 0.34218433\n",
      " 0.34218454 0.34218478 0.34218493 0.34218517 0.34218535 0.3421855\n",
      " 0.34218568 0.34218588 0.34218612 0.34218627 0.34218645 0.34218663\n",
      " 0.34218681 0.34218696 0.34218717 0.34218735 0.34218752 0.3421877\n",
      " 0.34218785 0.34218803 0.34218818 0.34218839 0.34218857 0.34218872\n",
      " 0.34218889 0.34218901 0.34218925 0.34218934 0.34218955 0.3421897\n",
      " 0.34218988 0.34219003 0.34219021 0.34219033 0.3421905  0.34219062\n",
      " 0.34219077 0.34219095 0.34219113 0.34219128 0.34219143 0.34219161\n",
      " 0.34219176 0.34219187 0.34219202 0.34219217 0.34219232 0.34219247\n",
      " 0.34219259 0.34219274 0.34219289 0.34219298 0.34219316 0.34219337\n",
      " 0.34219348 0.3421936  0.34219372 0.34219384 0.34219402 0.34219411\n",
      " 0.34219432 0.34219444 0.34219459 0.34219471 0.34219486 0.34219494\n",
      " 0.34219509 0.34219521 0.34219533 0.34219545 0.3421956  0.34219569\n",
      " 0.34219584 0.34219602 0.34219608 0.34219614]\n",
      "########\n",
      "d:8 alpha_P:2.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:2.0\n",
      "[0.44088933 0.44566295 0.4493894  0.4523499  0.45475301 0.45674226\n",
      " 0.45841703 0.45984736 0.46108404 0.46216437 0.463117   0.46396345\n",
      " 0.46472105 0.46540311 0.46602073 0.46658269 0.46709627 0.46756759\n",
      " 0.46800169 0.46840289 0.46877483 0.46912053 0.46944284 0.46974391\n",
      " 0.47002593 0.47029063 0.47053954 0.47077405 0.47099534 0.47120455\n",
      " 0.47140265 0.4715904  0.47176877 0.47193828 0.47209957 0.47225338\n",
      " 0.47240013 0.47254023 0.47267422 0.47280246 0.47292525 0.47304296\n",
      " 0.47315595 0.47326443 0.47336873 0.47346902 0.47356561 0.47365856\n",
      " 0.47374821 0.47383466 0.47391805 0.47399861 0.47407642 0.47415167\n",
      " 0.47422442 0.47429478 0.47436303 0.47442901 0.47449309 0.47455519\n",
      " 0.47461537 0.47467381 0.47473055 0.47478569 0.47483918 0.4748913\n",
      " 0.47494185 0.47499105 0.47503904 0.47508568 0.47513115 0.47517541\n",
      " 0.47521853 0.47526047 0.47530153 0.47534147 0.47538051 0.47541848\n",
      " 0.47545567 0.47549194 0.47552735 0.47556198 0.47559577 0.47562885\n",
      " 0.47566119 0.47569284 0.47572374 0.4757539  0.47578356 0.47581252\n",
      " 0.47584087 0.4758687  0.47589582 0.47592244 0.47594857 0.47597414\n",
      " 0.47599927 0.47602385 0.4760479  0.47607154 0.47609475 0.47611746\n",
      " 0.47613984 0.47616172 0.47618318 0.47620437 0.47622511 0.4762454\n",
      " 0.47626537 0.47628498 0.47630432 0.47632322 0.47634187 0.4763602\n",
      " 0.4763782  0.47639593 0.47641325 0.47643033 0.47644722 0.47646376\n",
      " 0.47648004 0.47649601 0.47651187 0.47652727 0.47654262 0.47655764\n",
      " 0.47657245 0.47658706 0.47660136 0.47661552 0.47662941 0.47664315\n",
      " 0.47665659 0.47666991 0.47668302 0.47669598 0.47670865 0.47672123\n",
      " 0.4767336  0.47674584 0.47675779 0.47676966 0.47678137 0.47679293\n",
      " 0.47680423 0.47681549 0.47682652 0.47683746 0.47684821 0.47685882\n",
      " 0.47686937 0.47687969 0.47688991 0.47689995 0.47690988 0.47691974\n",
      " 0.47692946 0.47693899 0.47694847 0.47695786 0.47696701 0.47697613\n",
      " 0.4769851  0.47699395 0.4770028  0.47701147 0.47702    0.47702849\n",
      " 0.47703686 0.47704512 0.47705325 0.47706136 0.47706929 0.47707716\n",
      " 0.47708499 0.47709268 0.47710034 0.47710782 0.4771153  0.47712263\n",
      " 0.47712994 0.47713715 0.47714424 0.47715127 0.47715825 0.47716516\n",
      " 0.47717199 0.47717872 0.47718537 0.47719195 0.47719854 0.47720498\n",
      " 0.47721139 0.47721773 0.47722396 0.47723013 0.47723627 0.47724235\n",
      " 0.4772484  0.47725433 0.47726023 0.47726604 0.47727177 0.47727752\n",
      " 0.47728318 0.47728881 0.47729427 0.47729981 0.47730526 0.4773106\n",
      " 0.47731599 0.47732121 0.47732651 0.4773317  0.47733676 0.47734192\n",
      " 0.47734693 0.47735196 0.47735688 0.47736174 0.4773666  0.47737142\n",
      " 0.47737616 0.47738099 0.47738561 0.4773902  0.47739482 0.47739938\n",
      " 0.47740394 0.47740838 0.47741279 0.4774172  0.47742152 0.47742587\n",
      " 0.47743014 0.4774344  0.47743866 0.47744277 0.47744697 0.47745109\n",
      " 0.47745511 0.47745916 0.47746313 0.47746709 0.47747102 0.4774749\n",
      " 0.4774788  0.47748259 0.47748643 0.47749016 0.47749394 0.47749767\n",
      " 0.47750133 0.477505   0.47750863 0.47751218 0.47751579 0.4775193\n",
      " 0.47752285 0.47752631 0.47752979 0.47753319 0.47753662 0.47753996\n",
      " 0.47754332 0.47754663 0.47754991 0.47755319 0.47755641 0.47755966\n",
      " 0.47756284 0.477566   0.47756922 0.47757235 0.47757542 0.47757852\n",
      " 0.47758153 0.47758463 0.47758761 0.47759062 0.47759351 0.47759652\n",
      " 0.4775995  0.47760236 0.47760522 0.47760811 0.47761089 0.47761381\n",
      " 0.47761655 0.47761938 0.47762215 0.47762492 0.47762761 0.47763032\n",
      " 0.477633   0.47763559 0.47763827 0.47764099 0.47764355 0.47764617\n",
      " 0.47764871 0.4776513  0.47765386 0.47765633 0.47765893 0.47766134\n",
      " 0.47766382 0.47766632 0.47766867 0.47767118 0.47767359 0.477676\n",
      " 0.47767836 0.47768077 0.47768304 0.47768542 0.47768769 0.47769007\n",
      " 0.47769234 0.4776946  0.47769687 0.47769904 0.47770128 0.47770357\n",
      " 0.47770572 0.47770798 0.4777101  0.4777123  0.47771442 0.47771657\n",
      " 0.47771868 0.47772083 0.47772291 0.47772491 0.47772706 0.47772911\n",
      " 0.47773114 0.47773317 0.47773513 0.47773719 0.47773921 0.47774124\n",
      " 0.47774315 0.47774512 0.47774705 0.47774905 0.47775087 0.47775283\n",
      " 0.47775474 0.47775668 0.47775853 0.47776034 0.47776222 0.47776404\n",
      " 0.47776589 0.47776774 0.47776958 0.4777714  0.47777319 0.47777498\n",
      " 0.47777674 0.47777849 0.47778022 0.47778204 0.47778371 0.4777855\n",
      " 0.47778714 0.47778893 0.47779059 0.47779226 0.47779399 0.4777956\n",
      " 0.47779727 0.47779894 0.47780052 0.47780222 0.47780383 0.47780547\n",
      " 0.47780707 0.47780862 0.47781023 0.47781175 0.47781333 0.47781494\n",
      " 0.47781649 0.47781807 0.47781956 0.47782114 0.47782263 0.47782409\n",
      " 0.4778257  0.47782716 0.47782865 0.47783011 0.47783163 0.47783309\n",
      " 0.47783455 0.47783598 0.47783747 0.47783858]\n",
      "########\n",
      "d:8 alpha_P:4.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:4.0\n",
      "[0.57750821 0.58391714 0.58952123 0.59441215 0.59870249 0.6024937\n",
      " 0.60586935 0.60889703 0.61163104 0.61411464 0.61638355 0.61846644\n",
      " 0.62038738 0.62216598 0.62381881 0.62536013 0.62680185 0.62815398\n",
      " 0.62942559 0.63062435 0.6317569  0.63282901 0.63384587 0.63481218\n",
      " 0.63573164 0.63660824 0.63744503 0.63824493 0.63901055 0.63974434\n",
      " 0.64044821 0.64112437 0.64177442 0.64240003 0.64300275 0.64358383\n",
      " 0.64414448 0.64468604 0.64520925 0.64571542 0.64620531 0.6466797\n",
      " 0.64713967 0.64758545 0.64801812 0.64843816 0.64884609 0.64924258\n",
      " 0.64962822 0.65000337 0.65036851 0.65072399 0.65107042 0.65140796\n",
      " 0.65173709 0.65205812 0.65237135 0.65267706 0.65297568 0.65326738\n",
      " 0.65355241 0.65383089 0.6541034  0.65436977 0.65463042 0.65488553\n",
      " 0.65513521 0.65537983 0.65561932 0.65585411 0.65608388 0.65630931\n",
      " 0.65653044 0.65674722 0.65695977 0.65716845 0.65737313 0.65757418\n",
      " 0.65777147 0.65796512 0.6581555  0.65834242 0.65852612 0.65870655\n",
      " 0.65888399 0.65905845 0.65922999 0.65939867 0.6595645  0.65972769\n",
      " 0.65988827 0.66004628 0.66020185 0.66035491 0.66050571 0.66065413\n",
      " 0.66080034 0.66094422 0.66108608 0.6612258  0.66136342 0.66149902\n",
      " 0.66163278 0.66176462 0.6618945  0.66202259 0.66214895 0.66227353\n",
      " 0.66239637 0.66251761 0.66263723 0.66275513 0.66287148 0.66298646\n",
      " 0.66309977 0.66321164 0.66332215 0.66343105 0.66353881 0.66364509\n",
      " 0.66375005 0.66385376 0.66395622 0.66405731 0.66415727 0.66425598\n",
      " 0.66435343 0.66444999 0.66454512 0.66463929 0.66473228 0.66482419\n",
      " 0.66491514 0.66500497 0.66509384 0.66518164 0.6652686  0.66535449\n",
      " 0.66543943 0.66552341 0.66560644 0.66568875 0.66577005 0.6658504\n",
      " 0.66593003 0.66600883 0.66608673 0.6661638  0.6662401  0.66631562\n",
      " 0.66639048 0.66646439 0.6665377  0.66661018 0.66668183 0.66675293\n",
      " 0.66682333 0.66689295 0.66696191 0.66703033 0.66709799 0.6671648\n",
      " 0.66723126 0.66729701 0.66736203 0.66742659 0.66749042 0.66755372\n",
      " 0.66761643 0.66767848 0.66773999 0.66780102 0.66786152 0.66792142\n",
      " 0.66798073 0.6680395  0.66809785 0.66815555 0.66821289 0.66826969\n",
      " 0.66832596 0.66838169 0.668437   0.6684919  0.6685462  0.66860008\n",
      " 0.66865355 0.66870654 0.66875905 0.66881114 0.66886282 0.66891414\n",
      " 0.66896492 0.66901529 0.6690653  0.66911489 0.66916412 0.66921294\n",
      " 0.66926134 0.66930932 0.669357   0.66940421 0.66945106 0.66949761\n",
      " 0.6695438  0.66958946 0.66963494 0.66968006 0.66972488 0.66976911\n",
      " 0.66981328 0.66985703 0.66990042 0.66994345 0.66998625 0.67002875\n",
      " 0.67007089 0.67011267 0.67015415 0.67019546 0.67023623 0.67027682\n",
      " 0.67031723 0.67035717 0.67039686 0.67043638 0.6704756  0.6705144\n",
      " 0.67055297 0.67059129 0.67062938 0.67066723 0.67070478 0.67074203\n",
      " 0.67077905 0.67081583 0.67085242 0.67088872 0.67092472 0.67096043\n",
      " 0.67099589 0.67103124 0.67106634 0.67110115 0.67113566 0.67117018\n",
      " 0.67120427 0.67123824 0.67127186 0.67130542 0.67133856 0.67137164\n",
      " 0.67140454 0.67143703 0.67146951 0.67150176 0.67153376 0.67156553\n",
      " 0.67159706 0.67162848 0.67165965 0.67169064 0.6717214  0.6717521\n",
      " 0.67178249 0.67181271 0.67184281 0.67187268 0.67190236 0.6719318\n",
      " 0.67196119 0.67199033 0.67201924 0.67204809 0.67207664 0.67210513\n",
      " 0.67213339 0.67216152 0.67218941 0.67221719 0.67224491 0.67227238\n",
      " 0.67229968 0.67232674 0.67235374 0.67238051 0.67240721 0.67243373\n",
      " 0.67246008 0.67248636 0.67251235 0.67253828 0.67256409 0.6725896\n",
      " 0.67261517 0.6726405  0.67266577 0.67269069 0.67271566 0.6727404\n",
      " 0.67276508 0.67278957 0.67281395 0.67283815 0.67286211 0.67288625\n",
      " 0.67290998 0.67293376 0.67295736 0.67298079 0.67300421 0.6730274\n",
      " 0.6730504  0.67307335 0.67309624 0.67311889 0.67314154 0.67316395\n",
      " 0.67318636 0.67320859 0.67323071 0.6732527  0.67327464 0.67329639\n",
      " 0.67331815 0.67333966 0.67336112 0.67338234 0.67340356 0.67342478\n",
      " 0.67344576 0.67346668 0.67348748 0.67350817 0.67352885 0.67354929\n",
      " 0.67356974 0.67358994 0.67361009 0.67363018 0.67365032 0.67367011\n",
      " 0.67368984 0.67370957 0.67372912 0.67374867 0.6737681  0.67378742\n",
      " 0.67380661 0.67382574 0.67384475 0.67386371 0.67388254 0.67390132\n",
      " 0.67391998 0.67393863 0.67395711 0.67397553 0.67399377 0.67401201\n",
      " 0.67403013 0.6740483  0.67406619 0.67408407 0.67410195 0.67411965\n",
      " 0.67413723 0.67415488 0.67417234 0.67418975 0.67420703 0.67422438\n",
      " 0.67424154 0.67425853 0.67427558 0.67429256 0.67430937 0.67432618\n",
      " 0.67434287 0.6743595  0.67437601 0.67439252 0.67440897 0.67442524\n",
      " 0.67444158 0.67445779 0.67447388 0.67448992 0.67450595 0.6745218\n",
      " 0.67453766 0.67455345 0.67456913 0.67458475 0.6746003  0.6746158\n",
      " 0.67463124 0.67464668 0.67466193 0.67467409]\n",
      "########\n",
      "d:8 alpha_P:8.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:8.0\n",
      "[0.70381385 0.71026677 0.71623093 0.72170627 0.72672725 0.7313382\n",
      " 0.73558402 0.73950619 0.74314177 0.74652308 0.7496779  0.75263065\n",
      " 0.75540215 0.75801039 0.76047158 0.76279908 0.76500517 0.76710039\n",
      " 0.76909393 0.77099431 0.77280897 0.77454406 0.77620596 0.77779961\n",
      " 0.77933002 0.78080136 0.7822175  0.78358203 0.78489828 0.78616923\n",
      " 0.78739733 0.78858525 0.78973526 0.79084951 0.79192972 0.79297793\n",
      " 0.79399562 0.7949844  0.79594582 0.79688096 0.79779124 0.79867786\n",
      " 0.79954165 0.80038393 0.80120534 0.80200708 0.80278981 0.80355442\n",
      " 0.80430144 0.80503172 0.80574602 0.80644482 0.80712873 0.80779845\n",
      " 0.80845416 0.80909663 0.80972642 0.81034368 0.81094915 0.81154305\n",
      " 0.81212568 0.81269765 0.81325918 0.81381053 0.81435221 0.81488442\n",
      " 0.81540745 0.81592172 0.81642711 0.81692421 0.81741333 0.81789434\n",
      " 0.81836796 0.81883383 0.81929272 0.81974447 0.82018936 0.82062757\n",
      " 0.82105935 0.82148468 0.821904   0.82231724 0.8227247  0.82312638\n",
      " 0.82352257 0.82391334 0.82429886 0.8246792  0.82505429 0.82542473\n",
      " 0.82579029 0.82615107 0.82650727 0.826859   0.82720649 0.82754964\n",
      " 0.82788849 0.82822335 0.82855409 0.82888091 0.8292039  0.82952327\n",
      " 0.82983875 0.83015072 0.83045912 0.83076394 0.83106554 0.83136374\n",
      " 0.8316589  0.83195049 0.83223909 0.83252466 0.83280718 0.83308679\n",
      " 0.83336323 0.83363712 0.83390814 0.8341763  0.83444184 0.8347047\n",
      " 0.83496493 0.8352226  0.83547777 0.83573061 0.83598095 0.83622879\n",
      " 0.83647436 0.83671761 0.83695859 0.83719736 0.83743393 0.83766824\n",
      " 0.83790052 0.83813083 0.83835882 0.83858496 0.83880913 0.83903128\n",
      " 0.83925152 0.83946991 0.83968627 0.83990085 0.84011376 0.84032464\n",
      " 0.84053397 0.84074152 0.84094733 0.84115136 0.84135389 0.8415547\n",
      " 0.84175402 0.84195161 0.84214771 0.8423422  0.84253532 0.84272677\n",
      " 0.84291673 0.84310544 0.84329247 0.84347814 0.8436625  0.84384543\n",
      " 0.84402698 0.84420717 0.8443861  0.84456366 0.84473997 0.84491503\n",
      " 0.84508884 0.84526128 0.84543264 0.84560269 0.84577155 0.84593928\n",
      " 0.84610575 0.8462711  0.84643543 0.84659851 0.84676051 0.84692144\n",
      " 0.8470813  0.84724003 0.84739769 0.84755445 0.84771001 0.84786457\n",
      " 0.84801817 0.84817088 0.84832245 0.84847301 0.84862268 0.84877151\n",
      " 0.84891933 0.84906602 0.84921193 0.84935701 0.84950119 0.84964436\n",
      " 0.84978664 0.8499282  0.85006875 0.85020852 0.85034746 0.85048556\n",
      " 0.85062295 0.85075927 0.85089499 0.85102975 0.8511638  0.85129702\n",
      " 0.85142964 0.85156131 0.85169232 0.85182256 0.85195202 0.85208076\n",
      " 0.85220879 0.85233611 0.85246277 0.85258865 0.85271376 0.85283822\n",
      " 0.85296202 0.85308522 0.85320777 0.85332942 0.85345054 0.853571\n",
      " 0.85369092 0.85381013 0.85392863 0.85404658 0.85416394 0.85428059\n",
      " 0.85439664 0.85451227 0.85462713 0.85474133 0.85485506 0.85496819\n",
      " 0.85508072 0.85519272 0.85530412 0.85541499 0.85552526 0.85563505\n",
      " 0.85574424 0.85585278 0.85596097 0.85606843 0.85617554 0.85628217\n",
      " 0.85638827 0.85649359 0.85659868 0.85670316 0.85680717 0.85691065\n",
      " 0.8570137  0.85711604 0.85721815 0.85731965 0.8574208  0.85752141\n",
      " 0.85762155 0.85772115 0.85782033 0.8579191  0.85801744 0.85811526\n",
      " 0.85821265 0.85830963 0.85840631 0.85850221 0.85859793 0.8586933\n",
      " 0.85878801 0.85888237 0.85897648 0.85906994 0.85916311 0.85925579\n",
      " 0.85934812 0.85943997 0.85953158 0.85962272 0.85971355 0.85980386\n",
      " 0.85989386 0.85998344 0.86007273 0.86016136 0.86024988 0.86033803\n",
      " 0.86042571 0.86051303 0.86059999 0.86068672 0.86077297 0.86085886\n",
      " 0.86094439 0.86102968 0.8611145  0.86119902 0.86128318 0.86136705\n",
      " 0.86145055 0.8615337  0.86161661 0.8616991  0.86178112 0.86186314\n",
      " 0.86194462 0.86202574 0.86210668 0.86218721 0.86226755 0.86234742\n",
      " 0.86242712 0.86250639 0.86258548 0.8626641  0.86274254 0.86282063\n",
      " 0.86289847 0.86297607 0.86305326 0.86313021 0.8632068  0.86328328\n",
      " 0.86335915 0.86343509 0.86351061 0.86358577 0.86366075 0.86373544\n",
      " 0.86380982 0.86388391 0.86395782 0.86403137 0.86410475 0.86417776\n",
      " 0.86425054 0.86432308 0.86439526 0.86446732 0.86453903 0.86461061\n",
      " 0.86468184 0.86475289 0.86482358 0.86489403 0.86496431 0.86503422\n",
      " 0.86510408 0.86517358 0.86524284 0.86531174 0.86538059 0.86544913\n",
      " 0.86551756 0.86558557 0.86565334 0.86572105 0.8657884  0.86585563\n",
      " 0.86592251 0.86598927 0.86605573 0.86612195 0.86618811 0.86625379\n",
      " 0.86631954 0.86638486 0.86645007 0.86651504 0.86657983 0.86664432\n",
      " 0.86670864 0.86677265 0.86683655 0.86690027 0.86696368 0.86702704\n",
      " 0.86709005 0.86715311 0.86721557 0.8672781  0.86734039 0.86740243\n",
      " 0.8674643  0.86752594 0.86758745 0.86764878 0.86770982 0.86777067\n",
      " 0.86783153 0.86789197 0.86795229 0.86800051]\n",
      "########\n",
      "d:8 alpha_P:16.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:8 alpha:16.0\n",
      "[0.81159145 0.81676221 0.82165343 0.8262493  0.83055502 0.83458567\n",
      " 0.83836037 0.84189916 0.84522194 0.84834737 0.85129261 0.85407341\n",
      " 0.85670364 0.85919619 0.86156225 0.86381191 0.86595452 0.86799812\n",
      " 0.86995029 0.87181771 0.87360609 0.87532103 0.87696767 0.87855017\n",
      " 0.88007295 0.88153958 0.88295346 0.88431787 0.88563555 0.88690937\n",
      " 0.88814151 0.88933432 0.89049006 0.89161044 0.89269733 0.89375246\n",
      " 0.89477742 0.89577359 0.8967424  0.89768493 0.89860249 0.89949626\n",
      " 0.90036732 0.90121645 0.90204471 0.90285289 0.90364188 0.90441233\n",
      " 0.90516514 0.90590096 0.90662026 0.90732396 0.90801239 0.90868622\n",
      " 0.90934587 0.90999192 0.91062498 0.91124517 0.91185325 0.91244942\n",
      " 0.91303414 0.91360795 0.9141708  0.91472328 0.9152658  0.91579849\n",
      " 0.91632181 0.91683602 0.91734123 0.9178378  0.91832596 0.91880602\n",
      " 0.9192782  0.9197427  0.92019963 0.92064929 0.92109191 0.92152756\n",
      " 0.92195648 0.92237908 0.92279512 0.92320502 0.9236089  0.92400676\n",
      " 0.92439908 0.92478567 0.92516673 0.92554259 0.92591304 0.92627859\n",
      " 0.9266389  0.92699456 0.92734551 0.92769164 0.92803347 0.92837065\n",
      " 0.92870367 0.92903227 0.92935669 0.92967731 0.92999369 0.9303062\n",
      " 0.93061507 0.93092012 0.93122143 0.93151927 0.93181348 0.93210429\n",
      " 0.93239176 0.93267602 0.93295681 0.93323451 0.93350923 0.93378067\n",
      " 0.93404931 0.93431473 0.93457758 0.9348374  0.93509442 0.93534881\n",
      " 0.93560052 0.93584949 0.93609589 0.9363398  0.93658108 0.93682015\n",
      " 0.93705666 0.93729079 0.93752253 0.93775207 0.93797916 0.93820423\n",
      " 0.93842709 0.93864781 0.93886632 0.9390828  0.93929732 0.93950975\n",
      " 0.93972027 0.93992871 0.9401353  0.94034004 0.94054288 0.9407438\n",
      " 0.94094306 0.94114047 0.94133615 0.94153011 0.94172221 0.94191301\n",
      " 0.9421019  0.94228929 0.9424749  0.94265914 0.94284153 0.94302273\n",
      " 0.94320214 0.94338018 0.94355685 0.94373202 0.94390559 0.94407797\n",
      " 0.94424874 0.94441825 0.9445864  0.94475347 0.94491881 0.94508314\n",
      " 0.94524598 0.94540769 0.94556814 0.94572735 0.94588536 0.94604212\n",
      " 0.94619769 0.94635206 0.94650531 0.94665742 0.94680846 0.9469583\n",
      " 0.94710708 0.9472549  0.94740146 0.94754708 0.94769156 0.94783503\n",
      " 0.94797754 0.94811904 0.94825947 0.94839895 0.94853753 0.94867498\n",
      " 0.94881159 0.94894719 0.94908196 0.94921577 0.94934863 0.94948059\n",
      " 0.94961172 0.94974202 0.94987136 0.94999987 0.95012754 0.95025444\n",
      " 0.95038038 0.9505055  0.95062995 0.95075345 0.95087618 0.95099819\n",
      " 0.95111942 0.95123982 0.95135945 0.95147836 0.95159662 0.95171398\n",
      " 0.95183074 0.95194662 0.95206201 0.95217657 0.95229042 0.95240349\n",
      " 0.95251602 0.95262778 0.952739   0.95284957 0.9529593  0.95306849\n",
      " 0.95317698 0.9532848  0.95339221 0.95349866 0.95360464 0.95371002\n",
      " 0.9538148  0.95391899 0.95402265 0.9541257  0.9542281  0.95432985\n",
      " 0.95443124 0.95453185 0.95463192 0.9547314  0.95483047 0.95492887\n",
      " 0.95502681 0.9551242  0.955221   0.95531738 0.95541328 0.95550841\n",
      " 0.95560318 0.95569742 0.95579118 0.9558844  0.95597732 0.95606947\n",
      " 0.95616126 0.95625246 0.95634347 0.95643377 0.9565236  0.956613\n",
      " 0.95670193 0.95679033 0.95687848 0.95696604 0.95705318 0.95713979\n",
      " 0.95722622 0.95731193 0.95739722 0.95748228 0.95756692 0.95765096\n",
      " 0.95773464 0.95781785 0.95790076 0.95798326 0.95806521 0.95814699\n",
      " 0.95822817 0.95830899 0.95838946 0.95846969 0.95854944 0.95862865\n",
      " 0.95870769 0.95878619 0.95886445 0.95894212 0.95901978 0.95909685\n",
      " 0.95917356 0.95924997 0.95932585 0.95940155 0.95947695 0.95955193\n",
      " 0.95962662 0.9597007  0.95977473 0.9598484  0.95992166 0.95999461\n",
      " 0.96006715 0.96013945 0.96021163 0.9602831  0.96035445 0.96042544\n",
      " 0.96049625 0.96056664 0.96063662 0.96070635 0.96077579 0.96084505\n",
      " 0.9609139  0.96098238 0.96105063 0.96111876 0.96118623 0.9612537\n",
      " 0.96132082 0.96138752 0.96145409 0.96152049 0.96158642 0.9616521\n",
      " 0.96171761 0.96178263 0.96184742 0.9619121  0.96197641 0.96204031\n",
      " 0.96210432 0.96216768 0.96223098 0.96229392 0.96235675 0.96241909\n",
      " 0.96248144 0.96254331 0.96260518 0.96266657 0.96272796 0.96278888\n",
      " 0.96284968 0.96291018 0.96297038 0.96303034 0.96309012 0.96314967\n",
      " 0.96320903 0.96326804 0.96332705 0.96338552 0.96344399 0.96350211\n",
      " 0.96356004 0.96361774 0.96367526 0.9637326  0.96378964 0.96384639\n",
      " 0.96390295 0.96395934 0.96401542 0.96407157 0.96412736 0.96418285\n",
      " 0.96423805 0.96429324 0.96434808 0.96440297 0.96445727 0.96451175\n",
      " 0.96456581 0.96461982 0.9646734  0.96472698 0.96478021 0.96483326\n",
      " 0.96488631 0.964939   0.96499151 0.9650439  0.965096   0.96514797\n",
      " 0.96519989 0.96525127 0.96530277 0.96535403 0.96540517 0.96545589\n",
      " 0.96550643 0.9655571  0.96560735 0.96564752]\n",
      "########\n",
      "d:16 alpha_P:0.5 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:0.5\n",
      "[0.17255108 0.17300543 0.17324893 0.1733968  0.17349453 0.17356306\n",
      " 0.17361334 0.17365153 0.17368138 0.17370522 0.17372459 0.17374066\n",
      " 0.17375416 0.17376563 0.17377543 0.17378394 0.17379139 0.17379795\n",
      " 0.17380375 0.17380895 0.17381357 0.17381778 0.17382155 0.17382503\n",
      " 0.17382815 0.17383103 0.17383367 0.17383613 0.17383838 0.17384046\n",
      " 0.17384242 0.17384422 0.17384595 0.17384754 0.17384902 0.17385042\n",
      " 0.17385173 0.17385297 0.17385417 0.17385527 0.17385632 0.17385732\n",
      " 0.17385827 0.17385919 0.17386003 0.17386085 0.17386162 0.17386235\n",
      " 0.17386307 0.17386377 0.17386439 0.17386504 0.17386565 0.1738662\n",
      " 0.17386678 0.1738673  0.17386779 0.1738683  0.17386879 0.17386924\n",
      " 0.17386967 0.17387013 0.17387055 0.17387092 0.17387132 0.17387171\n",
      " 0.17387207 0.17387238 0.17387275 0.17387307 0.17387341 0.17387372\n",
      " 0.17387401 0.1738743  0.1738746  0.17387488 0.17387514 0.17387541\n",
      " 0.17387564 0.17387591 0.17387614 0.17387639 0.17387663 0.17387684\n",
      " 0.17387706 0.17387725 0.17387745 0.17387767 0.17387788 0.17387807\n",
      " 0.17387827 0.17387845 0.17387863 0.17387882 0.17387897 0.17387915\n",
      " 0.17387931 0.17387946 0.17387964 0.17387979 0.17387997 0.1738801\n",
      " 0.17388025 0.17388038 0.1738805  0.17388065 0.17388082 0.17388093\n",
      " 0.17388105 0.17388119 0.17388129 0.17388146 0.17388155 0.17388165\n",
      " 0.1738818  0.17388189 0.17388202 0.17388214 0.17388225 0.17388234\n",
      " 0.17388245 0.17388254 0.17388265 0.17388274 0.17388284 0.17388295\n",
      " 0.17388304 0.17388314 0.17388323 0.17388332 0.17388339 0.17388348\n",
      " 0.17388357 0.17388366 0.17388374 0.17388383 0.17388391 0.17388399\n",
      " 0.17388405 0.17388415 0.17388421 0.1738843  0.17388435 0.17388444\n",
      " 0.1738845  0.17388457 0.17388465 0.17388472 0.17388479 0.17388484\n",
      " 0.17388491 0.17388499 0.17388506 0.17388512 0.17388518 0.17388524\n",
      " 0.17388529 0.17388535 0.17388542 0.17388548 0.17388554 0.1738856\n",
      " 0.17388563 0.17388569 0.17388575 0.17388581 0.17388587 0.17388593\n",
      " 0.173886   0.17388603 0.17388608 0.17388612 0.17388617 0.17388624\n",
      " 0.17388627 0.17388633 0.17388637 0.17388642 0.17388648 0.17388649\n",
      " 0.17388655 0.1738866  0.17388666 0.1738867  0.17388675 0.17388678\n",
      " 0.17388684 0.17388685 0.1738869  0.17388695 0.173887   0.17388703\n",
      " 0.17388706 0.1738871  0.17388716 0.17388719 0.17388721 0.17388728\n",
      " 0.17388731 0.17388733 0.17388737 0.1738874  0.17388745 0.17388748\n",
      " 0.17388751 0.17388757 0.17388758 0.17388763 0.17388766 0.1738877\n",
      " 0.17388771 0.17388776 0.17388776 0.17388783 0.17388785 0.17388789\n",
      " 0.17388791 0.17388794 0.17388798 0.173888   0.17388803 0.17388806\n",
      " 0.17388812 0.1738881  0.17388815 0.17388818 0.17388822 0.17388825\n",
      " 0.17388825 0.1738883  0.17388833 0.17388837 0.1738884  0.17388842\n",
      " 0.17388844 0.17388846 0.17388847 0.17388852 0.17388853 0.17388856\n",
      " 0.17388861 0.17388862 0.17388865 0.17388865 0.17388868 0.1738887\n",
      " 0.17388874 0.17388876 0.1738888  0.17388882 0.17388883 0.17388885\n",
      " 0.17388889 0.17388891 0.17388892 0.17388895 0.17388897 0.173889\n",
      " 0.17388904 0.17388904 0.17388907 0.17388907 0.1738891  0.17388912\n",
      " 0.17388913 0.17388916 0.1738892  0.1738892  0.17388922 0.17388925\n",
      " 0.17388926 0.17388929 0.17388929 0.17388932 0.17388934 0.17388935\n",
      " 0.17388937 0.17388941 0.17388943 0.17388944 0.17388943 0.17388949\n",
      " 0.1738895  0.17388953 0.17388952 0.17388955 0.17388956 0.17388958\n",
      " 0.17388961 0.17388962 0.17388964 0.17388967 0.17388968 0.1738897\n",
      " 0.1738897  0.17388973 0.17388976 0.17388976 0.17388976 0.1738898\n",
      " 0.17388983 0.17388982 0.17388983 0.17388985 0.17388988 0.17388989\n",
      " 0.17388991 0.17388991 0.17388994 0.17388995 0.17388996 0.17388998\n",
      " 0.17388999 0.17388999 0.17389001 0.17389001 0.17389005 0.17389005\n",
      " 0.17389008 0.1738901  0.1738901  0.17389014 0.17389013 0.17389016\n",
      " 0.17389016 0.17389017 0.17389019 0.1738902  0.17389022 0.17389023\n",
      " 0.17389026 0.17389025 0.17389028 0.17389029 0.17389031 0.17389031\n",
      " 0.17389032 0.17389034 0.17389032 0.17389038 0.17389037 0.17389041\n",
      " 0.1738904  0.17389043 0.1738904  0.17389044 0.17389043 0.17389046\n",
      " 0.17389049 0.17389047 0.17389052 0.17389052 0.17389053 0.17389055\n",
      " 0.17389053 0.17389056 0.17389056 0.17389059 0.17389059 0.17389058\n",
      " 0.17389062 0.17389064 0.17389062 0.17389065 0.17389065 0.17389069\n",
      " 0.17389067 0.17389068 0.17389069 0.17389072 0.17389074 0.17389069\n",
      " 0.17389074 0.17389075 0.17389077 0.17389077 0.17389078 0.17389081\n",
      " 0.1738908  0.1738908  0.1738908  0.17389084 0.17389084 0.17389086\n",
      " 0.17389086 0.17389086 0.1738909  0.17389089 0.1738909  0.17389092\n",
      " 0.17389093 0.17389093 0.17389095 0.17389096 0.17389095 0.17389098\n",
      " 0.17389096 0.17389098 0.17389099 0.17389101]\n",
      "########\n",
      "d:16 alpha_P:1.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:1.0\n",
      "[0.24006471 0.2412388  0.2420402  0.24261123 0.24303462 0.24335894\n",
      " 0.24361405 0.24381912 0.24398695 0.24412647 0.24424398 0.24434404\n",
      " 0.24443011 0.24450481 0.24457012 0.24462768 0.24467869 0.24472411\n",
      " 0.24476486 0.24480154 0.24483468 0.24486478 0.24489222 0.2449173\n",
      " 0.24494034 0.24496153 0.24498107 0.24499916 0.24501589 0.24503146\n",
      " 0.24504596 0.24505952 0.24507222 0.24508412 0.24509522 0.24510576\n",
      " 0.24511561 0.24512492 0.24513371 0.24514204 0.24514988 0.24515736\n",
      " 0.24516447 0.24517119 0.24517763 0.24518375 0.24518955 0.24519509\n",
      " 0.24520041 0.24520548 0.24521038 0.24521503 0.24521947 0.24522376\n",
      " 0.24522789 0.24523182 0.24523559 0.24523926 0.24524277 0.24524616\n",
      " 0.24524942 0.24525256 0.24525562 0.24525854 0.24526137 0.2452641\n",
      " 0.24526678 0.24526933 0.2452718  0.24527422 0.24527656 0.24527882\n",
      " 0.24528098 0.24528314 0.24528517 0.24528719 0.24528912 0.24529101\n",
      " 0.24529284 0.24529465 0.24529637 0.24529806 0.24529971 0.24530131\n",
      " 0.24530286 0.24530441 0.2453059  0.24530731 0.24530877 0.24531014\n",
      " 0.24531148 0.24531275 0.24531403 0.24531531 0.24531651 0.24531768\n",
      " 0.24531887 0.24532001 0.24532111 0.24532221 0.24532329 0.24532431\n",
      " 0.24532534 0.24532633 0.24532731 0.24532829 0.24532917 0.24533013\n",
      " 0.24533103 0.24533193 0.24533281 0.24533364 0.24533448 0.24533528\n",
      " 0.2453361  0.24533691 0.24533767 0.2453384  0.24533914 0.24533987\n",
      " 0.2453406  0.24534132 0.24534199 0.24534266 0.24534334 0.24534401\n",
      " 0.24534461 0.24534525 0.24534589 0.24534649 0.24534708 0.24534771\n",
      " 0.24534826 0.24534886 0.24534941 0.24534996 0.24535051 0.24535105\n",
      " 0.24535158 0.24535209 0.24535263 0.24535312 0.24535362 0.24535412\n",
      " 0.24535458 0.24535504 0.24535555 0.24535596 0.24535644 0.2453569\n",
      " 0.24535733 0.24535778 0.24535821 0.2453586  0.24535903 0.24535947\n",
      " 0.24535984 0.24536026 0.24536066 0.24536105 0.24536143 0.2453618\n",
      " 0.24536218 0.24536252 0.24536289 0.24536327 0.24536364 0.24536397\n",
      " 0.24536432 0.24536468 0.24536499 0.24536534 0.24536565 0.24536598\n",
      " 0.24536631 0.24536662 0.24536693 0.24536724 0.24536754 0.24536784\n",
      " 0.24536814 0.24536845 0.24536872 0.24536902 0.24536929 0.2453696\n",
      " 0.24536988 0.24537013 0.2453704  0.24537067 0.24537095 0.24537121\n",
      " 0.24537148 0.24537173 0.24537198 0.24537225 0.24537247 0.24537271\n",
      " 0.24537297 0.24537319 0.24537343 0.24537367 0.2453739  0.24537414\n",
      " 0.24537437 0.24537456 0.24537483 0.24537504 0.24537526 0.24537545\n",
      " 0.24537569 0.2453759  0.24537612 0.24537632 0.2453765  0.24537672\n",
      " 0.24537691 0.24537712 0.24537733 0.24537751 0.24537772 0.2453779\n",
      " 0.24537809 0.2453783  0.24537846 0.24537863 0.24537884 0.24537903\n",
      " 0.24537919 0.24537937 0.24537954 0.24537973 0.24537989 0.24538006\n",
      " 0.24538025 0.2453804  0.2453806  0.24538074 0.24538091 0.24538106\n",
      " 0.24538124 0.24538137 0.24538153 0.24538171 0.24538185 0.24538197\n",
      " 0.24538216 0.24538232 0.24538247 0.24538258 0.24538276 0.24538289\n",
      " 0.24538305 0.24538317 0.24538334 0.24538347 0.24538362 0.24538374\n",
      " 0.2453839  0.24538404 0.24538417 0.24538431 0.24538445 0.24538454\n",
      " 0.24538469 0.24538481 0.24538496 0.2453851  0.2453852  0.24538533\n",
      " 0.24538547 0.24538557 0.24538572 0.24538583 0.24538596 0.24538606\n",
      " 0.24538621 0.24538629 0.24538642 0.24538654 0.24538666 0.24538676\n",
      " 0.2453869  0.24538702 0.24538711 0.24538723 0.24538735 0.24538743\n",
      " 0.24538755 0.24538766 0.24538776 0.24538788 0.24538799 0.24538809\n",
      " 0.24538818 0.24538828 0.2453884  0.24538851 0.2453886  0.2453887\n",
      " 0.24538879 0.24538893 0.245389   0.24538909 0.24538919 0.2453893\n",
      " 0.24538939 0.24538946 0.24538958 0.24538966 0.24538979 0.24538982\n",
      " 0.24538995 0.24539004 0.24539015 0.24539025 0.24539031 0.2453904\n",
      " 0.24539046 0.24539058 0.24539067 0.24539073 0.24539082 0.24539091\n",
      " 0.24539103 0.24539112 0.24539119 0.24539123 0.24539134 0.24539144\n",
      " 0.2453915  0.24539159 0.24539167 0.24539174 0.24539185 0.24539189\n",
      " 0.24539201 0.24539205 0.24539216 0.24539222 0.24539229 0.24539238\n",
      " 0.24539244 0.24539255 0.24539258 0.24539268 0.24539275 0.24539284\n",
      " 0.2453929  0.24539296 0.24539302 0.24539311 0.24539317 0.24539326\n",
      " 0.24539331 0.24539338 0.24539347 0.24539354 0.2453936  0.24539366\n",
      " 0.24539372 0.24539381 0.24539384 0.24539395 0.24539401 0.24539405\n",
      " 0.24539416 0.24539419 0.24539424 0.24539432 0.24539439 0.24539448\n",
      " 0.24539451 0.24539454 0.24539463 0.24539469 0.24539477 0.24539483\n",
      " 0.24539493 0.24539497 0.24539499 0.24539505 0.24539512 0.24539518\n",
      " 0.24539527 0.2453953  0.24539536 0.24539545 0.24539545 0.24539553\n",
      " 0.24539562 0.24539568 0.24539572 0.24539579 0.24539582 0.24539587\n",
      " 0.24539594 0.245396   0.24539606 0.24539609]\n",
      "########\n",
      "d:16 alpha_P:2.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:2.0\n",
      "[0.33094743 0.33290464 0.33450738 0.33582297 0.3369143  0.33783048\n",
      " 0.33860883 0.33927727 0.33985683 0.34036371 0.34081045 0.34120688\n",
      " 0.34156081 0.34187868 0.3421655  0.34242544 0.34266219 0.34287843\n",
      " 0.34307683 0.34325927 0.34342769 0.34358352 0.34372813 0.34386256\n",
      " 0.34398794 0.34410498 0.34421456 0.34431738 0.34441391 0.3445048\n",
      " 0.34459043 0.34467125 0.34474766 0.34481999 0.34488857 0.34495357\n",
      " 0.34501541 0.34507421 0.34513021 0.34518358 0.34523448 0.34528309\n",
      " 0.34532958 0.34537405 0.34541661 0.34545738 0.34549654 0.34553406\n",
      " 0.34557012 0.34560484 0.34563816 0.34567022 0.34570113 0.3457309\n",
      " 0.3457596  0.34578729 0.34581399 0.34583983 0.34586483 0.34588891\n",
      " 0.34591222 0.34593481 0.34595665 0.34597784 0.34599832 0.3460182\n",
      " 0.34603751 0.34605619 0.34607434 0.34609196 0.34610912 0.34612575\n",
      " 0.34614193 0.34615764 0.34617296 0.34618786 0.34620234 0.34621644\n",
      " 0.34623018 0.34624356 0.34625661 0.34626937 0.34628174 0.34629387\n",
      " 0.34630564 0.34631717 0.34632838 0.34633934 0.34635007 0.34636056\n",
      " 0.34637082 0.3463808  0.34639052 0.34640011 0.3464095  0.34641862\n",
      " 0.34642759 0.34643626 0.3464449  0.34645331 0.34646153 0.34646955\n",
      " 0.34647745 0.34648514 0.34649271 0.3465001  0.3465074  0.34651452\n",
      " 0.3465215  0.34652835 0.34653509 0.34654176 0.34654817 0.34655452\n",
      " 0.34656081 0.34656686 0.34657288 0.34657881 0.34658459 0.34659034\n",
      " 0.34659588 0.3466014  0.34660682 0.34661213 0.34661737 0.3466225\n",
      " 0.34662753 0.34663248 0.34663737 0.3466422  0.3466469  0.34665155\n",
      " 0.34665611 0.34666058 0.34666508 0.34666947 0.34667367 0.34667793\n",
      " 0.3466821  0.34668621 0.34669024 0.3466942  0.34669814 0.34670201\n",
      " 0.34670576 0.34670952 0.34671322 0.34671685 0.34672043 0.34672394\n",
      " 0.34672743 0.34673089 0.34673423 0.34673759 0.34674093 0.34674415\n",
      " 0.34674734 0.3467505  0.34675363 0.3467567  0.34675974 0.34676272\n",
      " 0.34676567 0.34676859 0.34677145 0.34677434 0.34677711 0.34677988\n",
      " 0.34678262 0.34678534 0.34678799 0.34679058 0.3467932  0.34679577\n",
      " 0.34679827 0.3468008  0.34680325 0.34680572 0.34680817 0.34681058\n",
      " 0.3468129  0.34681523 0.34681749 0.34681982 0.34682205 0.34682429\n",
      " 0.34682643 0.34682864 0.34683076 0.3468329  0.34683502 0.34683707\n",
      " 0.34683913 0.34684113 0.34684321 0.34684515 0.34684718 0.34684905\n",
      " 0.34685099 0.3468529  0.34685478 0.34685659 0.34685847 0.34686029\n",
      " 0.34686211 0.34686387 0.34686565 0.34686744 0.34686914 0.34687084\n",
      " 0.34687248 0.34687418 0.34687591 0.34687749 0.34687915 0.34688076\n",
      " 0.34688234 0.34688386 0.34688547 0.34688699 0.34688854 0.34689006\n",
      " 0.34689155 0.34689304 0.34689459 0.34689599 0.34689745 0.34689888\n",
      " 0.34690031 0.34690168 0.34690315 0.34690449 0.34690586 0.34690723\n",
      " 0.34690854 0.34690985 0.34691116 0.3469125  0.34691378 0.3469151\n",
      " 0.34691635 0.34691766 0.34691885 0.34692007 0.34692132 0.34692255\n",
      " 0.34692371 0.34692496 0.34692609 0.34692731 0.34692845 0.34692961\n",
      " 0.34693077 0.3469319  0.34693301 0.34693417 0.34693524 0.3469364\n",
      " 0.34693745 0.34693852 0.34693959 0.34694067 0.34694168 0.34694275\n",
      " 0.3469438  0.34694481 0.34694582 0.34694687 0.34694782 0.34694883\n",
      " 0.34694982 0.34695083 0.34695181 0.34695274 0.34695369 0.34695464\n",
      " 0.3469556  0.34695652 0.34695745 0.34695834 0.34695926 0.34696019\n",
      " 0.34696108 0.346962   0.34696287 0.34696373 0.34696463 0.34696549\n",
      " 0.34696633 0.34696719 0.34696808 0.34696892 0.34696978 0.34697056\n",
      " 0.34697139 0.3469722  0.34697303 0.34697387 0.34697461 0.34697545\n",
      " 0.34697622 0.346977   0.34697777 0.34697852 0.34697929 0.34698007\n",
      " 0.34698084 0.34698153 0.34698236 0.34698305 0.34698379 0.34698451\n",
      " 0.34698525 0.34698597 0.34698668 0.3469874  0.34698808 0.3469888\n",
      " 0.34698948 0.34699017 0.34699082 0.34699151 0.34699222 0.34699291\n",
      " 0.34699354 0.34699422 0.34699491 0.3469955  0.34699622 0.34699687\n",
      " 0.34699747 0.34699813 0.34699875 0.34699935 0.34700003 0.34700066\n",
      " 0.34700128 0.34700191 0.34700248 0.34700304 0.3470037  0.34700429\n",
      " 0.34700492 0.34700543 0.34700605 0.34700665 0.34700722 0.34700781\n",
      " 0.34700838 0.34700897 0.34700948 0.34701005 0.34701058 0.34701121\n",
      " 0.34701174 0.34701228 0.34701285 0.34701341 0.34701392 0.34701446\n",
      " 0.34701496 0.34701553 0.34701604 0.34701657 0.34701708 0.34701765\n",
      " 0.34701812 0.34701863 0.34701917 0.34701961 0.34702018 0.34702066\n",
      " 0.34702119 0.34702164 0.34702221 0.34702265 0.34702313 0.34702358\n",
      " 0.34702411 0.34702453 0.34702507 0.34702551 0.34702605 0.34702644\n",
      " 0.34702691 0.34702739 0.34702781 0.34702829 0.34702876 0.34702921\n",
      " 0.34702969 0.3470301  0.34703058 0.347031   0.34703144 0.34703183\n",
      " 0.34703231 0.34703273 0.34703317 0.34703353]\n",
      "########\n",
      "d:16 alpha_P:4.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:4.0\n",
      "[0.44549477 0.44822139 0.45070222 0.45294309 0.4549641  0.45678934\n",
      " 0.4584423  0.45994443 0.46131438 0.46256837 0.4637205  0.46478245\n",
      " 0.46576452 0.46667543 0.46752265 0.46831298 0.46905178 0.46974421\n",
      " 0.47039455 0.47100642 0.47158343 0.47212836 0.47264403 0.47313264\n",
      " 0.47359636 0.47403714 0.47445661 0.47485635 0.4752377  0.47560197\n",
      " 0.47595027 0.47628364 0.47660312 0.4769094  0.47720352 0.47748607\n",
      " 0.47775775 0.47801915 0.47827095 0.47851357 0.47874752 0.47897336\n",
      " 0.47919142 0.47940221 0.47960591 0.47980306 0.47999373 0.48017856\n",
      " 0.48035771 0.48053125 0.48069963 0.480863   0.48102167 0.48117572\n",
      " 0.48132539 0.48147097 0.48161247 0.48175016 0.48188427 0.48201475\n",
      " 0.48214185 0.48226562 0.48238632 0.48250401 0.48261869 0.48273072\n",
      " 0.48283991 0.48294657 0.4830507  0.48315245 0.48325181 0.48334894\n",
      " 0.48344383 0.48353669 0.48362741 0.48371619 0.48380306 0.48388818\n",
      " 0.48397139 0.48405293 0.4841328  0.48421094 0.48428756 0.48436266\n",
      " 0.48443624 0.4845084  0.48457915 0.48464856 0.48471659 0.48478335\n",
      " 0.48484892 0.48491326 0.48497635 0.48503834 0.4850992  0.48515895\n",
      " 0.48521763 0.48527536 0.48533207 0.48538771 0.48544246 0.48549625\n",
      " 0.48554918 0.48560119 0.48565236 0.48570272 0.48575217 0.4858008\n",
      " 0.4858487  0.48589584 0.48594227 0.4859879  0.48603281 0.48607716\n",
      " 0.48612064 0.48616359 0.48620588 0.48624757 0.48628852 0.48632887\n",
      " 0.48636875 0.48640794 0.48644665 0.48648474 0.48652232 0.48655933\n",
      " 0.48659578 0.48663181 0.48666731 0.48670226 0.48673683 0.48677084\n",
      " 0.48680443 0.48683757 0.48687026 0.48690248 0.48693436 0.48696578\n",
      " 0.48699677 0.48702735 0.48705754 0.4870874  0.48711681 0.48714584\n",
      " 0.4871746  0.48720294 0.48723093 0.48725855 0.48728588 0.48731288\n",
      " 0.48733947 0.48736575 0.4873918  0.48741752 0.48744288 0.48746794\n",
      " 0.48749271 0.4875173  0.4875415  0.4875654  0.48758906 0.48761246\n",
      " 0.48763564 0.48765856 0.48768109 0.48770344 0.48772556 0.48774743\n",
      " 0.48776907 0.48779047 0.48781163 0.48783249 0.4878532  0.4878737\n",
      " 0.48789397 0.48791397 0.48793381 0.48795348 0.48797289 0.48799205\n",
      " 0.48801109 0.48802993 0.48804858 0.48806703 0.48808533 0.48810327\n",
      " 0.48812121 0.48813891 0.48815641 0.48817375 0.48819095 0.48820797\n",
      " 0.4882248  0.48824152 0.48825806 0.48827437 0.48829061 0.48830664\n",
      " 0.4883225  0.48833826 0.48835385 0.48836932 0.48838463 0.48839974\n",
      " 0.48841476 0.48842964 0.48844442 0.48845902 0.4884735  0.48848787\n",
      " 0.48850209 0.48851615 0.4885301  0.4885439  0.4885577  0.48857123\n",
      " 0.48858467 0.48859808 0.48861128 0.48862439 0.48863742 0.48865029\n",
      " 0.48866308 0.48867574 0.48868829 0.48870081 0.48871315 0.48872539\n",
      " 0.48873752 0.48874953 0.48876151 0.48877335 0.48878509 0.48879674\n",
      " 0.48880827 0.48881975 0.48883107 0.4888424  0.48885354 0.4888646\n",
      " 0.48887563 0.48888648 0.48889741 0.48890808 0.48891875 0.48892927\n",
      " 0.48893982 0.48895022 0.48896056 0.48897079 0.48898092 0.48899102\n",
      " 0.48900101 0.48901093 0.48902076 0.48903057 0.48904026 0.48904991\n",
      " 0.48905942 0.48906896 0.48907843 0.4890877  0.48909703 0.48910621\n",
      " 0.48911533 0.48912439 0.48913342 0.48914227 0.48915127 0.48916009\n",
      " 0.48916879 0.48917747 0.48918611 0.48919466 0.48920316 0.48921159\n",
      " 0.48921999 0.48922828 0.48923656 0.48924473 0.48925292 0.48926097\n",
      " 0.48926905 0.48927698 0.4892849  0.48929277 0.48930061 0.48930836\n",
      " 0.48931608 0.48932365 0.48933125 0.48933885 0.48934633 0.48935381\n",
      " 0.4893612  0.48936859 0.4893758  0.4893831  0.48939028 0.48939741\n",
      " 0.48940453 0.48941159 0.48941863 0.48942557 0.48943251 0.48943943\n",
      " 0.48944625 0.48945302 0.48945981 0.48946652 0.48947319 0.48947972\n",
      " 0.48948637 0.48949286 0.48949936 0.48950586 0.48951221 0.48951861\n",
      " 0.48952496 0.48953125 0.48953751 0.48954374 0.48954991 0.48955607\n",
      " 0.48956212 0.48956817 0.48957422 0.48958021 0.48958617 0.48959208\n",
      " 0.48959789 0.48960379 0.48960966 0.48961538 0.48962113 0.48962688\n",
      " 0.48963249 0.48963815 0.48964378 0.48964936 0.48965487 0.48966041\n",
      " 0.4896659  0.48967132 0.48967677 0.48968214 0.48968753 0.48969275\n",
      " 0.48969811 0.48970333 0.48970863 0.48971385 0.48971894 0.48972407\n",
      " 0.48972917 0.48973432 0.48973933 0.48974437 0.48974934 0.48975432\n",
      " 0.48975924 0.48976418 0.4897691  0.4897739  0.48977876 0.48978356\n",
      " 0.48978832 0.48979306 0.48979777 0.48980251 0.48980719 0.48981187\n",
      " 0.48981649 0.48982108 0.4898257  0.48983023 0.48983473 0.4898392\n",
      " 0.48984376 0.48984826 0.48985264 0.48985699 0.4898614  0.48986575\n",
      " 0.4898701  0.48987445 0.48987874 0.48988304 0.48988727 0.48989144\n",
      " 0.48989561 0.48989987 0.48990408 0.48990816 0.48991233 0.48991638\n",
      " 0.4899205  0.48992455 0.48992857 0.48993179]\n",
      "########\n",
      "d:16 alpha_P:8.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:8.0\n",
      "[0.5763787  0.57953936 0.58256191 0.5854336  0.58815318 0.59072429\n",
      " 0.59315348 0.59544885 0.59761906 0.59967291 0.60161865 0.60346442\n",
      " 0.60521746 0.60688472 0.60847253 0.60998648 0.61143196 0.61281377\n",
      " 0.61413622 0.61540341 0.61661893 0.61778611 0.61890799 0.61998749\n",
      " 0.62102705 0.62202924 0.62299591 0.62392938 0.62483138 0.62570351\n",
      " 0.62654763 0.62736505 0.62815714 0.6289252  0.62967044 0.63039398\n",
      " 0.63109684 0.63177997 0.63244414 0.63309056 0.63371968 0.63433242\n",
      " 0.63492936 0.63551128 0.63607877 0.63663256 0.63717288 0.6377005\n",
      " 0.63821578 0.63871926 0.63921142 0.63969272 0.6401633  0.64062387\n",
      " 0.64107448 0.64151585 0.64194798 0.64237136 0.6427862  0.64319271\n",
      " 0.64359134 0.64398229 0.64436573 0.64474189 0.64511114 0.64547354\n",
      " 0.64582932 0.64617878 0.64652199 0.64685923 0.64719063 0.64751637\n",
      " 0.64783651 0.64815128 0.64846087 0.6487655  0.64906508 0.64935988\n",
      " 0.64964998 0.64993554 0.6502167  0.6504935  0.65076607 0.65103447\n",
      " 0.65129894 0.65155959 0.65181625 0.65206933 0.65231872 0.65256453\n",
      " 0.65280694 0.65304595 0.65328157 0.65351409 0.65374339 0.65396959\n",
      " 0.65419281 0.65441304 0.65463036 0.65484494 0.65505666 0.65526569\n",
      " 0.65547216 0.65567595 0.65587717 0.65607595 0.65627235 0.65646625\n",
      " 0.65665781 0.65684718 0.65703422 0.65721911 0.6574018  0.6575824\n",
      " 0.65776074 0.65793723 0.65811157 0.65828413 0.6584546  0.65862334\n",
      " 0.65879005 0.65895504 0.65911818 0.65927953 0.65943921 0.6595971\n",
      " 0.65975338 0.659908   0.66006118 0.66021258 0.66036242 0.66051084\n",
      " 0.66065764 0.66080302 0.66094691 0.66108942 0.66123039 0.66136998\n",
      " 0.66150832 0.66164523 0.66178089 0.66191524 0.66204822 0.66218013\n",
      " 0.66231066 0.66243994 0.66256815 0.66269499 0.66282082 0.66294533\n",
      " 0.66306883 0.66319126 0.66331249 0.66343266 0.66355169 0.66366982\n",
      " 0.66378683 0.66390276 0.6640178  0.66413182 0.66424489 0.66435683\n",
      " 0.66446799 0.66457814 0.6646874  0.66479564 0.66490304 0.66500956\n",
      " 0.66511524 0.66522002 0.66532397 0.66542703 0.66552931 0.66563064\n",
      " 0.66573131 0.66583109 0.66593009 0.66602838 0.66612583 0.66622257\n",
      " 0.66631854 0.66641378 0.66650832 0.66660202 0.66669512 0.66678751\n",
      " 0.66687912 0.66697013 0.66706043 0.66715008 0.66723901 0.66732734\n",
      " 0.6674149  0.66750205 0.66758835 0.66767418 0.6677593  0.66784388\n",
      " 0.6679278  0.66801107 0.66809398 0.66817605 0.66825765 0.66833866\n",
      " 0.66841924 0.66849905 0.66857845 0.66865724 0.6687355  0.66881329\n",
      " 0.66889042 0.66896713 0.66904336 0.669119   0.66919416 0.66926885\n",
      " 0.66934299 0.66941667 0.66948986 0.6695627  0.66963494 0.6697067\n",
      " 0.66977799 0.66984886 0.66991925 0.66998923 0.67005885 0.67012793\n",
      " 0.67019659 0.67026478 0.67033261 0.67039996 0.67046684 0.67053336\n",
      " 0.67059952 0.6706652  0.67073059 0.67079556 0.67086005 0.67092413\n",
      " 0.67098796 0.67105126 0.67111433 0.67117691 0.67123914 0.67130101\n",
      " 0.67136264 0.67142379 0.67148453 0.67154497 0.67160505 0.67166483\n",
      " 0.67172432 0.67178339 0.6718421  0.67190045 0.67195851 0.67201638\n",
      " 0.67207372 0.67213076 0.67218751 0.67224401 0.6723001  0.67235595\n",
      " 0.67241144 0.67246675 0.67252165 0.67257619 0.67263043 0.67268455\n",
      " 0.67273819 0.67279166 0.67284477 0.67289764 0.67295021 0.67300248\n",
      " 0.67305464 0.67310631 0.67315775 0.67320889 0.67325985 0.67331052\n",
      " 0.67336094 0.67341107 0.67346102 0.67351049 0.6735599  0.67360896\n",
      " 0.67365795 0.67370653 0.67375493 0.67380309 0.67385095 0.67389852\n",
      " 0.6739459  0.67399305 0.67403996 0.67408669 0.67413318 0.67417938\n",
      " 0.67422533 0.67427111 0.67431676 0.67436206 0.67440712 0.67445201\n",
      " 0.67449665 0.67454118 0.6745854  0.67462945 0.67467326 0.67471683\n",
      " 0.67476016 0.6748035  0.67484635 0.67488915 0.67493176 0.6749742\n",
      " 0.67501634 0.67505836 0.67510015 0.67514181 0.67518318 0.67522436\n",
      " 0.67526537 0.67530614 0.67534679 0.67538732 0.67542756 0.67546773\n",
      " 0.67550755 0.67554736 0.67558694 0.67562628 0.67566544 0.67570454\n",
      " 0.6757434  0.67578202 0.67582053 0.67585886 0.67589706 0.67593503\n",
      " 0.67597282 0.67601055 0.67604804 0.67608541 0.67612261 0.67615956\n",
      " 0.67619646 0.67623323 0.67626971 0.67630607 0.67634237 0.67637837\n",
      " 0.67641425 0.67645007 0.67648566 0.67652124 0.67655641 0.67659158\n",
      " 0.67662668 0.67666155 0.6766963  0.67673081 0.67676532 0.6767996\n",
      " 0.67683369 0.67686778 0.67690164 0.67693532 0.67696887 0.67700237\n",
      " 0.67703569 0.67706895 0.67710197 0.67713493 0.67716765 0.67720026\n",
      " 0.67723286 0.67726535 0.67729753 0.67732972 0.67736167 0.67739356\n",
      " 0.67742532 0.67745697 0.67748851 0.6775198  0.67755109 0.67758226\n",
      " 0.67761326 0.67764413 0.67767495 0.67770541 0.6777361  0.67776656\n",
      " 0.6777969  0.67782706 0.67785698 0.677881  ]\n",
      "########\n",
      "d:16 alpha_P:16.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:16 alpha:16.0\n",
      "[0.70533824 0.70833832 0.7112661  0.71411067 0.71686685 0.71953255\n",
      " 0.72210759 0.72459298 0.72699082 0.72930378 0.73153496 0.73368752\n",
      " 0.73576504 0.73777092 0.73970848 0.74158102 0.74339163 0.74514347\n",
      " 0.7468394  0.74848199 0.75007421 0.75161815 0.75311625 0.75457084\n",
      " 0.75598377 0.75735706 0.75869238 0.75999165 0.76125646 0.76248825\n",
      " 0.7636885  0.76485842 0.76599962 0.76711291 0.76819962 0.76926088\n",
      " 0.77029765 0.77131075 0.77230144 0.77327007 0.77421802 0.77514553\n",
      " 0.77605379 0.77694327 0.77781463 0.77866852 0.77950561 0.78032631\n",
      " 0.78113151 0.78192139 0.78269655 0.78345746 0.78420454 0.78493834\n",
      " 0.78565925 0.78636748 0.78706366 0.78774798 0.78842098 0.78908265\n",
      " 0.78973359 0.79037422 0.79100448 0.79162496 0.79223573 0.79283708\n",
      " 0.79342926 0.79401278 0.79458755 0.79515386 0.79571187 0.79626215\n",
      " 0.79680437 0.79733908 0.79786652 0.79838645 0.79889947 0.79940575\n",
      " 0.79990506 0.80039793 0.80088425 0.80136448 0.80183846 0.80230647\n",
      " 0.80276865 0.8032251  0.80367589 0.8041212  0.80456132 0.80499601\n",
      " 0.80542552 0.80585015 0.80626988 0.80668473 0.80709463 0.80750012\n",
      " 0.80790108 0.80829757 0.80868959 0.8090775  0.80946112 0.80984056\n",
      " 0.81021607 0.81058764 0.81095529 0.81131911 0.81167912 0.8120355\n",
      " 0.81238836 0.81273752 0.81308341 0.81342566 0.81376469 0.81410038\n",
      " 0.81443292 0.81476206 0.81508827 0.81541127 0.81573129 0.81604832\n",
      " 0.81636256 0.81667358 0.81698203 0.81728768 0.81759053 0.8178907\n",
      " 0.81818825 0.81848305 0.81877542 0.81906521 0.81935251 0.81963736\n",
      " 0.81991982 0.82019979 0.82047755 0.82075292 0.82102603 0.82129693\n",
      " 0.82156551 0.82183194 0.82209629 0.82235855 0.82261872 0.82287681\n",
      " 0.82313269 0.82338679 0.8236388  0.82388896 0.82413721 0.8243835\n",
      " 0.824628   0.82487053 0.82511133 0.82535028 0.82558751 0.82582307\n",
      " 0.82605684 0.82628906 0.82651937 0.82674801 0.82697511 0.82720071\n",
      " 0.82742453 0.82764697 0.82786775 0.82808697 0.82830459 0.82852095\n",
      " 0.82873571 0.82894892 0.82916087 0.82937133 0.82958025 0.82978797\n",
      " 0.8299942  0.83019918 0.83040273 0.83060509 0.8308059  0.83100557\n",
      " 0.83120388 0.83140099 0.83159691 0.8317914  0.83198482 0.83217698\n",
      " 0.83236802 0.83255786 0.83274645 0.83293384 0.83312029 0.83330548\n",
      " 0.83348954 0.83367258 0.83385444 0.83403516 0.83421487 0.83439356\n",
      " 0.83457124 0.83474779 0.83492339 0.83509785 0.83527148 0.83544397\n",
      " 0.83561552 0.8357861  0.83595568 0.83612448 0.83629209 0.83645886\n",
      " 0.8366248  0.83678973 0.8369537  0.83711684 0.83727902 0.83744037\n",
      " 0.83760089 0.83776057 0.83791929 0.83807719 0.83823431 0.83839053\n",
      " 0.83854604 0.83870053 0.83885431 0.83900732 0.83915955 0.83931094\n",
      " 0.83946157 0.83961135 0.83976054 0.83990884 0.84005642 0.84020334\n",
      " 0.84034944 0.84049469 0.84063941 0.84078336 0.84092665 0.84106916\n",
      " 0.8412109  0.84135211 0.84149253 0.84163243 0.84177125 0.84190977\n",
      " 0.84204757 0.84218466 0.8423211  0.84245694 0.842592   0.84272659\n",
      " 0.84286052 0.8429938  0.84312642 0.84325856 0.84338993 0.8435207\n",
      " 0.843651   0.84378064 0.84390962 0.84403813 0.84416592 0.84429318\n",
      " 0.8444199  0.84454614 0.84467185 0.8447969  0.84492135 0.84504527\n",
      " 0.84516871 0.84529144 0.84541392 0.8455357  0.84565705 0.84577775\n",
      " 0.84589791 0.84601772 0.84613687 0.84625554 0.84637386 0.84649158\n",
      " 0.84660876 0.8467254  0.84684163 0.84695745 0.84707272 0.8471874\n",
      " 0.84730172 0.84741563 0.84752905 0.847642   0.84775442 0.84786636\n",
      " 0.84797788 0.8480891  0.84819973 0.84830987 0.84841961 0.84852898\n",
      " 0.84863794 0.84874636 0.84885442 0.84896201 0.84906924 0.84917593\n",
      " 0.84928244 0.8493883  0.84949386 0.84959894 0.84970373 0.84980804\n",
      " 0.84991199 0.85001552 0.8501187  0.85022134 0.85032374 0.85042578\n",
      " 0.85052723 0.8506285  0.85072935 0.85082984 0.85092986 0.85102969\n",
      " 0.85112894 0.85122806 0.85132658 0.85142481 0.85152274 0.85162038\n",
      " 0.85171753 0.85181445 0.85191089 0.85200709 0.85210288 0.8521983\n",
      " 0.85229349 0.85238826 0.85248268 0.85257679 0.85267067 0.85276407\n",
      " 0.85285723 0.85294998 0.85304248 0.85313469 0.85322648 0.85331804\n",
      " 0.85340929 0.85350013 0.85359079 0.85368109 0.85377097 0.85386056\n",
      " 0.85395002 0.85403919 0.85412788 0.8542164  0.85430449 0.85439235\n",
      " 0.85448003 0.85456723 0.85465425 0.85474098 0.8548274  0.85491365\n",
      " 0.85499942 0.85508502 0.85517037 0.85525537 0.85534012 0.8554247\n",
      " 0.85550886 0.85559279 0.85567641 0.85575986 0.85584301 0.85592586\n",
      " 0.85600847 0.85609084 0.85617298 0.85625476 0.8563363  0.85641766\n",
      " 0.85649866 0.85657942 0.85666001 0.8567403  0.85682034 0.85690022\n",
      " 0.85697979 0.857059   0.8571381  0.85721689 0.85729563 0.85737389\n",
      " 0.85745203 0.85753    0.85760748 0.85766953]\n",
      "########\n",
      "d:32 alpha_P:0.5 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:0.5\n",
      "[0.12298263 0.12315883 0.12325547 0.12331475 0.12335409 0.12338173\n",
      " 0.12340201 0.1234174  0.1234294  0.12343895 0.12344672 0.12345313\n",
      " 0.1234585  0.12346309 0.12346698 0.12347035 0.12347329 0.12347586\n",
      " 0.12347816 0.12348019 0.123482   0.12348364 0.12348513 0.12348647\n",
      " 0.12348769 0.12348881 0.12348983 0.12349077 0.12349166 0.12349246\n",
      " 0.12349322 0.1234939  0.12349456 0.12349518 0.12349575 0.12349629\n",
      " 0.12349678 0.12349727 0.12349772 0.12349813 0.12349855 0.1234989\n",
      " 0.12349928 0.12349962 0.12349995 0.12350025 0.12350053 0.12350082\n",
      " 0.12350111 0.12350137 0.12350161 0.12350184 0.12350205 0.12350229\n",
      " 0.12350249 0.12350269 0.12350289 0.12350306 0.12350325 0.12350341\n",
      " 0.12350357 0.12350374 0.1235039  0.12350404 0.1235042  0.12350435\n",
      " 0.12350447 0.12350461 0.12350472 0.12350485 0.12350497 0.12350509\n",
      " 0.12350521 0.12350531 0.12350541 0.12350553 0.12350562 0.12350572\n",
      " 0.1235058  0.12350589 0.12350599 0.12350608 0.12350617 0.12350625\n",
      " 0.12350634 0.12350641 0.12350649 0.12350658 0.12350664 0.12350669\n",
      " 0.12350678 0.12350684 0.12350692 0.12350697 0.12350705 0.1235071\n",
      " 0.12350717 0.12350722 0.12350728 0.12350734 0.1235074  0.12350744\n",
      " 0.12350748 0.12350755 0.12350761 0.12350766 0.1235077  0.12350775\n",
      " 0.12350781 0.12350783 0.12350789 0.12350794 0.12350798 0.12350802\n",
      " 0.12350807 0.12350812 0.12350814 0.12350819 0.12350822 0.12350827\n",
      " 0.1235083  0.12350834 0.12350837 0.12350842 0.12350845 0.12350848\n",
      " 0.12350853 0.12350856 0.12350859 0.1235086  0.12350865 0.12350868\n",
      " 0.12350871 0.12350874 0.12350878 0.1235088  0.12350883 0.12350886\n",
      " 0.12350889 0.12350892 0.12350895 0.12350897 0.123509   0.12350902\n",
      " 0.12350905 0.12350907 0.1235091  0.12350912 0.12350915 0.12350918\n",
      " 0.12350919 0.12350921 0.12350923 0.12350928 0.12350929 0.1235093\n",
      " 0.12350933 0.12350934 0.12350935 0.1235094  0.1235094  0.12350945\n",
      " 0.12350947 0.12350948 0.1235095  0.12350952 0.12350954 0.12350957\n",
      " 0.12350959 0.1235096  0.12350961 0.12350962 0.12350964 0.12350965\n",
      " 0.12350969 0.12350969 0.12350971 0.12350973 0.12350974 0.12350976\n",
      " 0.12350977 0.12350982 0.1235098  0.12350983 0.12350985 0.12350985\n",
      " 0.12350989 0.12350988 0.12350992 0.12350991 0.12350993 0.12350995\n",
      " 0.12350997 0.12350997 0.12350999 0.12351    0.12351002 0.12351002\n",
      " 0.12351005 0.12351005 0.12351006 0.12351007 0.12351011 0.12351009\n",
      " 0.12351012 0.12351014 0.12351014 0.12351016 0.12351018 0.12351018\n",
      " 0.12351017 0.1235102  0.1235102  0.12351023 0.12351024 0.12351025\n",
      " 0.12351027 0.12351028 0.12351029 0.12351029 0.12351032 0.12351033\n",
      " 0.12351032 0.12351035 0.12351035 0.12351035 0.12351038 0.12351038\n",
      " 0.12351039 0.12351041 0.12351041 0.12351041 0.12351044 0.12351043\n",
      " 0.12351045 0.12351047 0.12351046 0.12351047 0.12351047 0.12351049\n",
      " 0.1235105  0.12351052 0.12351051 0.12351053 0.12351055 0.12351053\n",
      " 0.12351055 0.12351055 0.12351057 0.12351058 0.12351058 0.12351059\n",
      " 0.12351059 0.12351061 0.12351061 0.12351063 0.12351064 0.12351064\n",
      " 0.12351065 0.12351064 0.12351066 0.12351068 0.12351068 0.12351067\n",
      " 0.1235107  0.12351071 0.12351069 0.12351071 0.1235107  0.12351072\n",
      " 0.12351074 0.12351075 0.12351077 0.12351076 0.12351076 0.12351076\n",
      " 0.12351076 0.12351079 0.12351079 0.12351079 0.12351082 0.12351081\n",
      " 0.12351082 0.12351082 0.12351081 0.12351083 0.12351086 0.12351085\n",
      " 0.12351087 0.12351087 0.12351087 0.12351088 0.12351089 0.12351087\n",
      " 0.1235109  0.1235109  0.12351088 0.1235109  0.12351091 0.12351091\n",
      " 0.12351092 0.12351093 0.12351093 0.12351094 0.12351094 0.12351094\n",
      " 0.12351096 0.12351096 0.12351097 0.12351099 0.12351097 0.12351098\n",
      " 0.12351099 0.12351099 0.12351099 0.12351101 0.12351101 0.123511\n",
      " 0.12351102 0.12351102 0.12351102 0.12351103 0.12351105 0.12351105\n",
      " 0.12351104 0.12351104 0.12351106 0.12351107 0.12351105 0.12351108\n",
      " 0.12351107 0.12351108 0.12351108 0.12351109 0.12351108 0.1235111\n",
      " 0.12351111 0.12351111 0.12351111 0.12351111 0.12351112 0.12351113\n",
      " 0.12351113 0.12351114 0.12351114 0.12351113 0.12351115 0.12351116\n",
      " 0.12351115 0.12351116 0.12351117 0.12351117 0.12351118 0.12351117\n",
      " 0.12351118 0.12351119 0.12351119 0.1235112  0.12351121 0.1235112\n",
      " 0.12351119 0.1235112  0.12351122 0.1235112  0.12351122 0.12351122\n",
      " 0.12351121 0.12351122 0.12351124 0.12351123 0.12351125 0.12351124\n",
      " 0.12351124 0.12351125 0.12351125 0.12351125 0.12351128 0.12351127\n",
      " 0.12351127 0.12351127 0.12351125 0.12351128 0.12351128 0.12351128\n",
      " 0.12351129 0.12351129 0.12351129 0.12351129 0.1235113  0.12351131\n",
      " 0.1235113  0.12351132 0.12351131 0.12351132 0.12351132 0.12351133\n",
      " 0.12351132 0.12351131 0.12351133 0.12351133]\n",
      "########\n",
      "d:32 alpha_P:1.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:1.0\n",
      "[0.17272712 0.1731458  0.17343855 0.17365012 0.17380831 0.17393008\n",
      " 0.17402616 0.17410353 0.17416689 0.17421956 0.17426389 0.17430164\n",
      " 0.17433408 0.17436223 0.17438678 0.17440839 0.17442754 0.17444457\n",
      " 0.1744598  0.17447349 0.17448589 0.17449711 0.17450732 0.17451666\n",
      " 0.1745252  0.17453305 0.17454031 0.174547   0.17455317 0.17455895\n",
      " 0.1745643  0.17456932 0.17457397 0.17457834 0.17458245 0.1745863\n",
      " 0.17458993 0.17459334 0.17459655 0.17459959 0.17460251 0.17460521\n",
      " 0.17460783 0.17461029 0.17461264 0.17461488 0.17461701 0.174619\n",
      " 0.17462097 0.17462282 0.17462459 0.17462628 0.1746279  0.17462946\n",
      " 0.17463097 0.1746324  0.17463377 0.1746351  0.17463638 0.17463762\n",
      " 0.17463878 0.17463994 0.17464103 0.17464212 0.17464313 0.17464414\n",
      " 0.17464508 0.17464602 0.17464691 0.17464779 0.17464863 0.17464945\n",
      " 0.17465024 0.17465101 0.17465177 0.17465249 0.17465317 0.17465389\n",
      " 0.17465453 0.17465518 0.17465582 0.17465642 0.17465703 0.1746576\n",
      " 0.17465816 0.17465872 0.17465925 0.17465979 0.1746603  0.17466079\n",
      " 0.17466128 0.17466176 0.1746622  0.17466266 0.17466311 0.17466354\n",
      " 0.17466396 0.17466439 0.17466478 0.17466517 0.17466557 0.17466591\n",
      " 0.1746663  0.17466664 0.17466703 0.17466736 0.17466769 0.17466803\n",
      " 0.17466836 0.17466868 0.17466898 0.1746693  0.17466961 0.17466989\n",
      " 0.17467017 0.17467049 0.17467076 0.17467102 0.17467129 0.17467155\n",
      " 0.17467181 0.17467208 0.17467231 0.17467257 0.1746728  0.17467302\n",
      " 0.17467327 0.17467353 0.17467372 0.17467396 0.17467417 0.17467439\n",
      " 0.17467462 0.17467482 0.17467502 0.17467523 0.17467543 0.17467561\n",
      " 0.17467579 0.17467597 0.17467618 0.17467636 0.17467654 0.17467673\n",
      " 0.1746769  0.17467704 0.17467721 0.1746774  0.17467755 0.17467773\n",
      " 0.17467786 0.17467803 0.17467818 0.17467836 0.1746785  0.17467865\n",
      " 0.17467882 0.17467895 0.1746791  0.17467923 0.17467938 0.1746795\n",
      " 0.17467965 0.17467976 0.17467993 0.17468002 0.17468017 0.17468029\n",
      " 0.17468043 0.17468055 0.17468065 0.17468075 0.17468089 0.17468101\n",
      " 0.17468114 0.17468128 0.17468138 0.17468147 0.17468159 0.17468172\n",
      " 0.17468183 0.17468193 0.17468202 0.17468216 0.17468226 0.17468235\n",
      " 0.17468245 0.17468254 0.17468265 0.17468275 0.17468284 0.17468295\n",
      " 0.17468302 0.17468309 0.1746832  0.17468329 0.17468341 0.17468347\n",
      " 0.17468357 0.17468366 0.17468373 0.17468382 0.1746839  0.17468399\n",
      " 0.17468408 0.17468415 0.17468424 0.17468435 0.17468441 0.17468449\n",
      " 0.17468455 0.17468464 0.1746847  0.17468479 0.17468488 0.17468494\n",
      " 0.17468502 0.17468508 0.17468514 0.17468521 0.1746853  0.17468534\n",
      " 0.17468543 0.17468549 0.17468557 0.17468564 0.17468572 0.17468576\n",
      " 0.17468582 0.1746859  0.17468597 0.17468603 0.17468609 0.17468615\n",
      " 0.17468622 0.17468628 0.17468634 0.1746864  0.17468646 0.17468654\n",
      " 0.17468657 0.17468664 0.1746867  0.17468674 0.1746868  0.17468686\n",
      " 0.17468694 0.17468698 0.17468703 0.1746871  0.17468715 0.17468721\n",
      " 0.17468724 0.1746873  0.17468737 0.1746874  0.17468745 0.17468749\n",
      " 0.17468756 0.17468762 0.17468765 0.17468771 0.17468776 0.1746878\n",
      " 0.17468783 0.17468789 0.17468795 0.174688   0.17468806 0.1746881\n",
      " 0.17468812 0.17468816 0.17468821 0.17468826 0.17468831 0.17468835\n",
      " 0.1746884  0.17468846 0.17468849 0.17468853 0.17468858 0.17468861\n",
      " 0.17468865 0.1746887  0.17468874 0.17468879 0.17468883 0.17468886\n",
      " 0.17468891 0.17468895 0.17468898 0.17468902 0.17468905 0.17468907\n",
      " 0.17468913 0.17468919 0.17468922 0.17468925 0.17468928 0.17468932\n",
      " 0.17468937 0.1746894  0.17468943 0.17468949 0.1746895  0.17468955\n",
      " 0.17468956 0.17468959 0.17468964 0.17468967 0.17468971 0.17468974\n",
      " 0.17468978 0.17468981 0.17468984 0.17468986 0.17468993 0.17468995\n",
      " 0.17468996 0.17469001 0.17469004 0.17469007 0.1746901  0.17469014\n",
      " 0.17469017 0.17469022 0.17469025 0.17469025 0.17469029 0.17469032\n",
      " 0.17469034 0.17469038 0.1746904  0.17469043 0.17469046 0.1746905\n",
      " 0.17469051 0.17469054 0.17469059 0.17469063 0.17469065 0.17469068\n",
      " 0.17469072 0.17469071 0.17469075 0.17469077 0.1746908  0.17469083\n",
      " 0.17469087 0.1746909  0.17469092 0.17469093 0.17469098 0.17469096\n",
      " 0.17469102 0.17469104 0.17469107 0.1746911  0.17469111 0.17469116\n",
      " 0.17469117 0.17469122 0.17469122 0.17469124 0.17469129 0.1746913\n",
      " 0.1746913  0.17469135 0.17469135 0.17469139 0.17469139 0.17469142\n",
      " 0.17469145 0.1746915  0.17469151 0.17469153 0.17469157 0.17469159\n",
      " 0.1746916  0.17469162 0.17469163 0.17469166 0.17469168 0.17469171\n",
      " 0.17469172 0.17469177 0.17469178 0.17469178 0.17469183 0.17469184\n",
      " 0.17469186 0.17469187 0.17469192 0.17469193 0.17469195 0.17469196\n",
      " 0.174692   0.17469203 0.17469203 0.17469203]\n",
      "########\n",
      "d:32 alpha_P:2.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:2.0\n",
      "[0.24125615 0.24198511 0.24259368 0.24310052 0.24352504 0.24388368\n",
      " 0.24418955 0.24445274 0.24468109 0.24488071 0.24505648 0.2452122\n",
      " 0.24535097 0.24547526 0.24558713 0.2456883  0.24578011 0.24586375\n",
      " 0.24594024 0.24601041 0.24607494 0.2461345  0.24618956 0.24624063\n",
      " 0.24628808 0.24633227 0.2463735  0.24641207 0.2464482  0.2464821\n",
      " 0.24651393 0.24654393 0.24657218 0.24659887 0.24662408 0.24664794\n",
      " 0.24667059 0.24669202 0.24671239 0.24673177 0.24675019 0.24676782\n",
      " 0.24678455 0.24680056 0.24681582 0.24683045 0.24684438 0.24685778\n",
      " 0.24687065 0.2468829  0.24689469 0.24690609 0.24691699 0.24692748\n",
      " 0.24693759 0.2469473  0.24695665 0.24696569 0.24697442 0.24698278\n",
      " 0.2469909  0.24699873 0.24700633 0.24701367 0.24702074 0.24702761\n",
      " 0.24703425 0.24704066 0.2470469  0.24705295 0.24705884 0.24706452\n",
      " 0.24707007 0.24707541 0.24708061 0.24708571 0.24709062 0.24709539\n",
      " 0.24710006 0.2471046  0.24710903 0.2471133  0.24711752 0.24712157\n",
      " 0.24712554 0.24712943 0.2471332  0.24713688 0.24714048 0.24714397\n",
      " 0.24714737 0.24715073 0.247154   0.2471572  0.2471603  0.24716334\n",
      " 0.24716634 0.24716921 0.24717206 0.24717489 0.24717759 0.24718024\n",
      " 0.24718285 0.24718539 0.24718791 0.24719036 0.24719277 0.24719511\n",
      " 0.24719737 0.24719964 0.24720187 0.24720405 0.24720615 0.24720824\n",
      " 0.24721031 0.24721232 0.24721427 0.24721618 0.24721809 0.24721996\n",
      " 0.24722175 0.2472236  0.24722534 0.24722707 0.24722877 0.24723043\n",
      " 0.24723208 0.24723372 0.24723528 0.24723683 0.24723838 0.24723989\n",
      " 0.24724138 0.24724284 0.24724427 0.2472457  0.24724704 0.24724843\n",
      " 0.24724977 0.24725108 0.24725239 0.24725367 0.24725494 0.24725619\n",
      " 0.24725741 0.2472586  0.2472598  0.24726096 0.24726214 0.24726328\n",
      " 0.24726439 0.2472655  0.24726658 0.24726768 0.24726872 0.24726975\n",
      " 0.24727076 0.24727178 0.2472728  0.2472738  0.24727476 0.24727568\n",
      " 0.24727665 0.2472776  0.24727848 0.24727942 0.24728028 0.24728116\n",
      " 0.24728206 0.24728289 0.24728376 0.24728462 0.24728543 0.24728623\n",
      " 0.24728706 0.24728787 0.24728864 0.24728939 0.24729016 0.24729098\n",
      " 0.24729173 0.24729243 0.24729317 0.24729393 0.24729463 0.24729533\n",
      " 0.24729604 0.24729672 0.24729741 0.24729806 0.24729876 0.2472994\n",
      " 0.24730004 0.2473007  0.24730133 0.24730195 0.24730259 0.24730323\n",
      " 0.24730381 0.24730441 0.24730502 0.24730562 0.24730618 0.24730672\n",
      " 0.24730735 0.24730787 0.24730843 0.24730897 0.24730954 0.24731009\n",
      " 0.24731062 0.24731115 0.24731168 0.24731219 0.24731268 0.24731319\n",
      " 0.24731369 0.24731423 0.24731472 0.24731518 0.24731565 0.24731615\n",
      " 0.24731663 0.24731711 0.24731755 0.247318   0.24731848 0.24731894\n",
      " 0.24731937 0.24731977 0.24732025 0.2473207  0.24732108 0.24732153\n",
      " 0.24732196 0.24732238 0.24732281 0.2473232  0.24732359 0.247324\n",
      " 0.24732442 0.24732482 0.24732521 0.24732558 0.24732597 0.24732634\n",
      " 0.24732673 0.24732712 0.24732748 0.24732786 0.24732825 0.24732858\n",
      " 0.24732897 0.24732929 0.24732962 0.24733001 0.24733037 0.2473307\n",
      " 0.24733101 0.24733138 0.24733171 0.24733207 0.24733235 0.24733268\n",
      " 0.24733301 0.24733333 0.24733366 0.24733397 0.2473343  0.24733461\n",
      " 0.24733491 0.24733521 0.24733551 0.24733584 0.24733616 0.2473364\n",
      " 0.24733669 0.24733703 0.2473373  0.24733756 0.24733791 0.24733818\n",
      " 0.24733843 0.24733873 0.247339   0.24733928 0.24733953 0.24733981\n",
      " 0.24734008 0.24734034 0.2473406  0.24734087 0.24734113 0.24734141\n",
      " 0.24734166 0.24734193 0.24734212 0.24734241 0.24734263 0.2473429\n",
      " 0.24734315 0.24734338 0.24734366 0.24734387 0.24734411 0.24734434\n",
      " 0.24734461 0.24734485 0.24734506 0.2473453  0.24734549 0.24734576\n",
      " 0.24734597 0.24734621 0.24734643 0.24734664 0.24734685 0.24734712\n",
      " 0.24734731 0.24734752 0.24734776 0.24734792 0.24734813 0.24734835\n",
      " 0.24734858 0.24734879 0.24734898 0.24734916 0.24734938 0.24734958\n",
      " 0.24734978 0.24734996 0.24735019 0.24735041 0.24735057 0.24735078\n",
      " 0.24735095 0.24735115 0.24735132 0.24735153 0.24735171 0.24735188\n",
      " 0.24735208 0.24735227 0.24735247 0.24735263 0.24735284 0.24735299\n",
      " 0.24735318 0.24735336 0.24735355 0.2473537  0.24735385 0.24735408\n",
      " 0.24735419 0.24735436 0.24735457 0.24735473 0.24735491 0.24735509\n",
      " 0.24735525 0.24735543 0.24735555 0.24735574 0.24735588 0.24735604\n",
      " 0.24735621 0.24735636 0.24735652 0.24735667 0.24735686 0.24735698\n",
      " 0.24735714 0.24735729 0.24735743 0.24735758 0.24735774 0.2473579\n",
      " 0.24735804 0.24735822 0.24735834 0.2473585  0.24735865 0.24735877\n",
      " 0.2473589  0.24735908 0.24735922 0.24735935 0.24735951 0.24735963\n",
      " 0.24735977 0.24735992 0.24736002 0.2473602  0.24736033 0.24736045\n",
      " 0.24736059 0.24736074 0.24736087 0.24736097]\n",
      "########\n",
      "d:32 alpha_P:4.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:4.0\n",
      "[0.33316386 0.33424899 0.33524996 0.33616644 0.33700275 0.3377654\n",
      " 0.33846152 0.33909807 0.33968121 0.34021688 0.3407101  0.34116545\n",
      " 0.34158689 0.34197786 0.34234142 0.34268025 0.34299666 0.34329286\n",
      " 0.3435705  0.34383133 0.34407678 0.34430811 0.34452641 0.34473279\n",
      " 0.34492818 0.34511343 0.3452892  0.34545621 0.34561521 0.34576648\n",
      " 0.34591085 0.34604859 0.34618011 0.34630597 0.34642637 0.34654173\n",
      " 0.3466523  0.34675837 0.34686023 0.34695807 0.34705219 0.34714279\n",
      " 0.34722993 0.34731391 0.34739488 0.347473   0.3475484  0.3476212\n",
      " 0.34769151 0.34775952 0.34782532 0.34788889 0.34795049 0.34801021\n",
      " 0.348068   0.348124   0.34817839 0.34823114 0.34828234 0.34833202\n",
      " 0.34838033 0.34842718 0.34847283 0.34851712 0.3485603  0.34860227\n",
      " 0.34864312 0.34868291 0.34872162 0.34875941 0.34879622 0.34883207\n",
      " 0.34886706 0.34890118 0.34893444 0.34896693 0.34899864 0.34902957\n",
      " 0.34905982 0.34908935 0.3491182  0.34914643 0.34917399 0.34920096\n",
      " 0.34922737 0.34925312 0.34927842 0.3493031  0.34932724 0.3493509\n",
      " 0.34937412 0.34939682 0.34941906 0.34944081 0.34946218 0.34948307\n",
      " 0.34950364 0.34952372 0.34954345 0.34956276 0.34958178 0.34960037\n",
      " 0.34961864 0.34963655 0.34965417 0.34967142 0.34968841 0.34970504\n",
      " 0.34972143 0.34973747 0.34975323 0.34976876 0.34978399 0.34979901\n",
      " 0.3498137  0.34982815 0.3498424  0.34985638 0.34987015 0.34988365\n",
      " 0.34989697 0.34991005 0.34992298 0.34993565 0.34994811 0.34996039\n",
      " 0.34997249 0.34998438 0.34999612 0.35000765 0.35001898 0.35003021\n",
      " 0.35004121 0.35005203 0.35006273 0.35007325 0.35008365 0.35009387\n",
      " 0.35010397 0.3501139  0.3501237  0.35013333 0.35014284 0.35015222\n",
      " 0.35016146 0.35017058 0.35017958 0.35018852 0.35019726 0.3502059\n",
      " 0.35021442 0.35022277 0.35023108 0.35023931 0.35024738 0.35025531\n",
      " 0.35026321 0.35027099 0.35027862 0.35028616 0.35029364 0.35030103\n",
      " 0.3503083  0.35031551 0.3503226  0.35032964 0.35033655 0.35034338\n",
      " 0.35035014 0.35035679 0.3503634  0.35036996 0.35037643 0.35038275\n",
      " 0.35038903 0.35039523 0.3504014  0.35040748 0.35041347 0.3504194\n",
      " 0.35042527 0.35043105 0.35043678 0.35044244 0.35044807 0.35045362\n",
      " 0.3504591  0.35046449 0.35046986 0.35047519 0.35048041 0.35048565\n",
      " 0.35049072 0.35049579 0.35050085 0.3505058  0.35051072 0.35051557\n",
      " 0.35052037 0.35052511 0.35052988 0.3505345  0.35053915 0.35054371\n",
      " 0.35054824 0.35055271 0.35055715 0.35056153 0.35056588 0.35057011\n",
      " 0.35057437 0.35057864 0.35058281 0.35058695 0.35059103 0.35059506\n",
      " 0.35059911 0.35060307 0.35060704 0.35061085 0.35061473 0.35061857\n",
      " 0.35062239 0.35062611 0.35062984 0.35063353 0.35063717 0.35064077\n",
      " 0.35064435 0.3506479  0.35065138 0.35065493 0.35065836 0.35066175\n",
      " 0.35066518 0.35066849 0.35067186 0.35067517 0.35067841 0.35068166\n",
      " 0.35068485 0.35068804 0.3506912  0.35069436 0.3506974  0.3507005\n",
      " 0.3507036  0.35070658 0.35070959 0.35071257 0.35071552 0.35071841\n",
      " 0.35072136 0.35072422 0.35072705 0.35072988 0.35073268 0.35073546\n",
      " 0.3507382  0.35074097 0.35074365 0.35074636 0.35074905 0.3507517\n",
      " 0.35075429 0.35075688 0.35075951 0.35076204 0.3507646  0.35076714\n",
      " 0.35076967 0.35077211 0.35077459 0.350777   0.35077944 0.35078186\n",
      " 0.35078424 0.35078663 0.35078895 0.35079131 0.35079363 0.35079592\n",
      " 0.35079822 0.35080045 0.35080269 0.35080492 0.35080713 0.35080934\n",
      " 0.35081154 0.35081369 0.3508158  0.35081801 0.35082009 0.35082221\n",
      " 0.3508243  0.35082635 0.35082844 0.35083047 0.35083249 0.35083452\n",
      " 0.35083649 0.35083851 0.35084045 0.35084242 0.35084435 0.35084629\n",
      " 0.35084823 0.35085011 0.35085198 0.35085386 0.35085571 0.35085759\n",
      " 0.35085943 0.35086125 0.35086307 0.35086486 0.35086665 0.35086843\n",
      " 0.35087022 0.35087192 0.35087371 0.35087541 0.35087717 0.35087889\n",
      " 0.35088056 0.35088223 0.3508839  0.35088557 0.35088721 0.35088885\n",
      " 0.35089052 0.35089216 0.35089368 0.35089538 0.35089695 0.3508985\n",
      " 0.35090005 0.35090166 0.35090315 0.35090467 0.35090619 0.35090774\n",
      " 0.35090932 0.35091078 0.3509123  0.3509137  0.35091519 0.35091665\n",
      " 0.35091814 0.35091954 0.35092101 0.35092247 0.3509239  0.3509253\n",
      " 0.35092667 0.35092804 0.35092947 0.35093087 0.35093221 0.35093352\n",
      " 0.35093495 0.35093623 0.3509376  0.35093895 0.35094029 0.35094157\n",
      " 0.35094288 0.35094419 0.35094547 0.35094675 0.35094804 0.35094932\n",
      " 0.35095051 0.35095176 0.35095307 0.35095426 0.35095555 0.35095677\n",
      " 0.35095799 0.35095915 0.35096037 0.3509616  0.35096273 0.35096395\n",
      " 0.35096508 0.35096627 0.35096747 0.3509686  0.35096979 0.35097092\n",
      " 0.35097203 0.35097316 0.35097429 0.35097539 0.35097656 0.35097763\n",
      " 0.35097876 0.35097986 0.35098094 0.3509818 ]\n",
      "########\n",
      "d:32 alpha_P:8.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:8.0\n",
      "[0.44586793 0.44725919 0.44860286 0.44989401 0.45113078 0.45231318\n",
      " 0.45344207 0.45451912 0.45554635 0.45652601 0.45746064 0.45835248\n",
      " 0.45920411 0.46001783 0.46079573 0.4615401  0.46225283 0.46293578\n",
      " 0.4635908  0.46421948 0.46482322 0.46540362 0.4659619  0.46649939\n",
      " 0.46701711 0.46751612 0.46799755 0.46846229 0.46891111 0.46934488\n",
      " 0.46976426 0.47017014 0.47056314 0.47094375 0.47131267 0.47167027\n",
      " 0.47201735 0.47235426 0.47268128 0.4729991  0.47330794 0.47360826\n",
      " 0.47390038 0.47418457 0.47446135 0.47473088 0.47499353 0.47524941\n",
      " 0.47549891 0.47574225 0.47597969 0.47621134 0.47643748 0.4766584\n",
      " 0.47687411 0.4770849  0.47729093 0.47749236 0.4776893  0.477882\n",
      " 0.47807059 0.47825503 0.47843558 0.47861242 0.47878554 0.47895524\n",
      " 0.47912151 0.47928435 0.47944409 0.47960055 0.47975403 0.47990465\n",
      " 0.48005238 0.48019731 0.48033956 0.48047918 0.48061624 0.48075083\n",
      " 0.48088306 0.48101288 0.4811404  0.48126575 0.48138893 0.48151001\n",
      " 0.48162898 0.48174605 0.48186114 0.4819743  0.48208562 0.48219514\n",
      " 0.48230287 0.48240894 0.48251325 0.48261592 0.4827171  0.48281667\n",
      " 0.48291472 0.48301125 0.48310632 0.48319998 0.48329228 0.48338312\n",
      " 0.48347273 0.48356101 0.48364803 0.48373374 0.48381826 0.48390171\n",
      " 0.4839839  0.48406485 0.48414481 0.48422366 0.48430139 0.48437804\n",
      " 0.48445374 0.48452842 0.48460218 0.48467484 0.48474663 0.48481745\n",
      " 0.48488739 0.48495641 0.48502457 0.48509187 0.48515823 0.48522392\n",
      " 0.48528874 0.48535267 0.48541591 0.48547837 0.48554003 0.48560104\n",
      " 0.4856613  0.48572075 0.48577952 0.4858377  0.48589516 0.48595193\n",
      " 0.48600805 0.48606357 0.48611838 0.48617262 0.48622623 0.48627925\n",
      " 0.4863317  0.4863835  0.48643479 0.48648548 0.4865357  0.4865852\n",
      " 0.48663431 0.48668292 0.48673099 0.4867785  0.4868255  0.48687202\n",
      " 0.48691809 0.48696369 0.48700872 0.48705336 0.48709756 0.48714131\n",
      " 0.48718455 0.48722738 0.48726982 0.48731184 0.48735341 0.48739454\n",
      " 0.48743537 0.48747566 0.48751569 0.48755521 0.48759446 0.48763332\n",
      " 0.48767179 0.48770988 0.48774755 0.48778495 0.487822   0.48785868\n",
      " 0.48789501 0.48793101 0.48796672 0.48800203 0.48803705 0.48807186\n",
      " 0.48810622 0.48814026 0.48817402 0.48820755 0.48824072 0.48827365\n",
      " 0.48830622 0.48833847 0.48837054 0.48840231 0.48843375 0.48846498\n",
      " 0.48849598 0.48852658 0.48855704 0.48858714 0.48861712 0.48864678\n",
      " 0.4886761  0.48870534 0.48873425 0.48876294 0.48879138 0.48881954\n",
      " 0.48884752 0.4888753  0.48890284 0.48893014 0.48895726 0.48898411\n",
      " 0.48901072 0.48903716 0.48906353 0.4890894  0.48911521 0.48914087\n",
      " 0.48916635 0.4891915  0.4892166  0.48924145 0.48926601 0.48929045\n",
      " 0.48931476 0.48933879 0.48936272 0.48938644 0.48940995 0.48943335\n",
      " 0.48945659 0.4894796  0.4895024  0.48952514 0.48954767 0.48957002\n",
      " 0.48959222 0.48961419 0.48963606 0.48965779 0.48967937 0.48970073\n",
      " 0.48972201 0.48974314 0.48976403 0.48978472 0.48980549 0.48982596\n",
      " 0.48984635 0.48986661 0.48988658 0.48990658 0.4899264  0.48994604\n",
      " 0.48996559 0.48998493 0.49000424 0.4900234  0.49004233 0.49006122\n",
      " 0.49008003 0.49009866 0.49011707 0.49013549 0.49015376 0.49017185\n",
      " 0.49018985 0.49020782 0.49022558 0.49024323 0.49026072 0.49027824\n",
      " 0.4902955  0.49031273 0.4903298  0.49034682 0.49036369 0.49038044\n",
      " 0.49039704 0.49041367 0.49043015 0.49044642 0.49046266 0.49047887\n",
      " 0.49049497 0.49051085 0.49052668 0.49054241 0.49055812 0.49057361\n",
      " 0.49058908 0.4906044  0.49061972 0.49063492 0.49065    0.49066493\n",
      " 0.49067989 0.49069467 0.49070945 0.49072409 0.49073857 0.49075308\n",
      " 0.49076745 0.49078172 0.49079594 0.49081007 0.49082407 0.49083805\n",
      " 0.49085188 0.49086571 0.49087942 0.4908931  0.49090666 0.4909201\n",
      " 0.49093351 0.49094683 0.49096012 0.49097326 0.49098641 0.49099937\n",
      " 0.49101233 0.49102527 0.49103802 0.49105078 0.49106348 0.49107605\n",
      " 0.49108857 0.491101   0.49111339 0.4911257  0.49113798 0.49115017\n",
      " 0.49116227 0.49117434 0.49118635 0.49119821 0.49121007 0.49122185\n",
      " 0.49123356 0.49124527 0.49125689 0.49126837 0.49127987 0.49129125\n",
      " 0.49130264 0.49131393 0.4913252  0.49133641 0.49134746 0.49135858\n",
      " 0.49136955 0.49138045 0.49139133 0.49140218 0.491413   0.49142367\n",
      " 0.49143431 0.49144498 0.4914555  0.49146602 0.49147654 0.49148682\n",
      " 0.49149719 0.49150747 0.49151775 0.49152794 0.49153805 0.49154818\n",
      " 0.49155813 0.49156815 0.49157807 0.491588   0.49159774 0.49160767\n",
      " 0.49161735 0.49162701 0.49163672 0.49164629 0.4916558  0.49166533\n",
      " 0.49167481 0.4916842  0.49169356 0.49170288 0.49171212 0.49172133\n",
      " 0.49173057 0.49173972 0.49174878 0.49175787 0.49176687 0.49177584\n",
      " 0.49178475 0.49179366 0.49180251 0.49180952]\n",
      "########\n",
      "d:32 alpha_P:16.0 lambda:1.0 epochs:2000  lr:0.05  max_norm:20.0  \n",
      "d:32 alpha:16.0\n",
      "[0.57631731 0.57785237 0.57936317 0.58084589 0.58229822 0.58371896\n",
      " 0.58510721 0.5864625  0.58778483 0.58907413 0.59033096 0.59155566\n",
      " 0.59274888 0.59391129 0.59504372 0.596147   0.59722191 0.59826928\n",
      " 0.59929013 0.60028523 0.6012553  0.6022014  0.60312426 0.60402477\n",
      " 0.60490352 0.60576141 0.60659921 0.60741746 0.60821688 0.60899818\n",
      " 0.60976207 0.61050886 0.61123949 0.61195421 0.61265373 0.61333847\n",
      " 0.61400902 0.61466569 0.61530906 0.61593944 0.61655742 0.61716324\n",
      " 0.61775738 0.61834013 0.61891186 0.61947286 0.62002355 0.62056416\n",
      " 0.62109512 0.62161642 0.62212873 0.62263203 0.62312669 0.62361288\n",
      " 0.62409085 0.62456089 0.62502313 0.62547785 0.62592524 0.62636548\n",
      " 0.62679881 0.62722528 0.62764513 0.62805861 0.62846577 0.62886685\n",
      " 0.62926191 0.62965125 0.63003474 0.63041282 0.63078552 0.63115281\n",
      " 0.63151509 0.63187218 0.6322245  0.63257188 0.6329146  0.63325268\n",
      " 0.63358641 0.63391566 0.63424051 0.63456124 0.63487786 0.63519043\n",
      " 0.63549924 0.63580394 0.63610494 0.63640231 0.63669598 0.63698608\n",
      " 0.63727278 0.63755602 0.63783586 0.63811243 0.63838583 0.63865602\n",
      " 0.63892323 0.63918734 0.6394484  0.63970661 0.63996202 0.6402145\n",
      " 0.64046431 0.64071137 0.64095581 0.64119756 0.6414367  0.64167339\n",
      " 0.64190775 0.64213943 0.64236885 0.64259589 0.6428206  0.64304304\n",
      " 0.64326316 0.64348125 0.64369714 0.64391083 0.64412242 0.64433199\n",
      " 0.64453959 0.64474511 0.64494878 0.64515042 0.64535022 0.64554811\n",
      " 0.6457442  0.64593846 0.64613104 0.64632165 0.64651072 0.64669812\n",
      " 0.64688355 0.64706761 0.64724994 0.64743072 0.64760983 0.64778739\n",
      " 0.64796358 0.64813805 0.64831114 0.6484828  0.64865297 0.64882159\n",
      " 0.64898896 0.64915484 0.64931953 0.64948261 0.64964455 0.64980507\n",
      " 0.64996439 0.65012228 0.65027905 0.65043455 0.65058881 0.65074193\n",
      " 0.65089363 0.65104437 0.65119386 0.65134215 0.65148944 0.65163553\n",
      " 0.65178043 0.65192425 0.65206712 0.65220886 0.65234953 0.65248919\n",
      " 0.65262789 0.65276551 0.65290201 0.65303761 0.65317225 0.65330595\n",
      " 0.65343863 0.65357041 0.65370131 0.65383118 0.65396023 0.65408832\n",
      " 0.65421546 0.65434182 0.65446734 0.65459198 0.65471578 0.65483868\n",
      " 0.65496069 0.65508199 0.65520251 0.65532202 0.65544093 0.65555906\n",
      " 0.65567631 0.65579289 0.65590858 0.65602356 0.65613788 0.65625149\n",
      " 0.65636426 0.65647638 0.65658772 0.65669841 0.65680838 0.65691769\n",
      " 0.65702623 0.65713418 0.65724152 0.65734798 0.65745395 0.65755928\n",
      " 0.65766388 0.65776795 0.65787131 0.65797412 0.65807623 0.65817785\n",
      " 0.65827876 0.65837902 0.65847886 0.65857786 0.65867651 0.6587745\n",
      " 0.65887201 0.65896893 0.65906513 0.65916085 0.6592561  0.65935081\n",
      " 0.65944493 0.65953851 0.65963143 0.65972406 0.65981609 0.65990752\n",
      " 0.65999866 0.66008902 0.66017908 0.66026855 0.66035759 0.66044611\n",
      " 0.66053426 0.66062176 0.66070884 0.66079545 0.66088158 0.66096735\n",
      " 0.66105258 0.66113734 0.66122168 0.66130561 0.66138905 0.66147214\n",
      " 0.66155469 0.66163683 0.66171849 0.66179997 0.66188079 0.66196126\n",
      " 0.66204125 0.662121   0.66220027 0.66227907 0.66235751 0.66243565\n",
      " 0.66251332 0.66259056 0.66266751 0.66274399 0.6628201  0.66289592\n",
      " 0.66297138 0.66304636 0.66312099 0.66319531 0.66326922 0.66334283\n",
      " 0.66341603 0.66348886 0.66356152 0.66363364 0.66370547 0.66377705\n",
      " 0.66384804 0.66391891 0.66398948 0.66405958 0.66412938 0.66419894\n",
      " 0.66426808 0.66433692 0.66440547 0.66447371 0.66454166 0.66460925\n",
      " 0.66467661 0.6647436  0.66481018 0.66487658 0.66494274 0.66500849\n",
      " 0.66507387 0.6651392  0.66520411 0.66526872 0.66533303 0.66539711\n",
      " 0.66546088 0.66552436 0.66558754 0.66565055 0.66571313 0.6657756\n",
      " 0.66583765 0.66589957 0.66596115 0.66602248 0.66608346 0.66614431\n",
      " 0.66620487 0.66626513 0.66632521 0.66638494 0.66644448 0.66650373\n",
      " 0.66656274 0.66662151 0.66668004 0.66673833 0.66679639 0.66685414\n",
      " 0.66691172 0.66696912 0.66702622 0.66708302 0.66713977 0.66719609\n",
      " 0.6672523  0.66730821 0.667364   0.66741955 0.66747469 0.66752982\n",
      " 0.66758466 0.6676392  0.66769361 0.6677478  0.66780186 0.6678555\n",
      " 0.66790909 0.66796237 0.66801548 0.66806853 0.66812122 0.66817367\n",
      " 0.668226   0.66827816 0.66833007 0.66838169 0.66843313 0.66848451\n",
      " 0.66853559 0.66858643 0.66863728 0.66868776 0.66873813 0.66878831\n",
      " 0.6688382  0.66888797 0.66893762 0.66898698 0.66903615 0.6690852\n",
      " 0.66913414 0.66918278 0.6692313  0.66927963 0.66932768 0.66937566\n",
      " 0.66942346 0.66947109 0.66951847 0.66956574 0.66961282 0.66965979\n",
      " 0.66970658 0.66975307 0.66979951 0.66984588 0.66989189 0.66993785\n",
      " 0.66998357 0.67002916 0.67007458 0.67011988 0.670165   0.67020994\n",
      " 0.67025471 0.67029947 0.67034382 0.67037928]\n"
     ]
    }
   ],
   "source": [
    "#epochs_to_save = [10,20,50,100,200,400,800,1600]\n",
    "\n",
    "t_arr = np.arange(5,2001,5)\n",
    "\n",
    "curve = np.zeros((len(d_arr), len(alpha_P_arr), len(t_arr)))\n",
    "l_numerical = np.zeros((len(d_arr), len(alpha_P_arr), len(t_arr)))\n",
    "\n",
    "for i_d, d in enumerate(d_arr):\n",
    "    for i_a, alpha_P in enumerate(alpha_P_arr):  \n",
    "        P = int(alpha_P*N)\n",
    "        print(\"########\")\n",
    "        print(\"d:{} alpha_P:{} lambda:{} epochs:{}  lr:{}  max_norm:{}  \".format(d, alpha_P, l, epochs, lr, max_grad))\n",
    "        model_name_base = \"{}_{}_GD_TEACHER_fixed{}_N_{}_P_{}_d{}_l_{}_epochs{}_lr{}_l2{}\".format(spin_type, loss_type, fixed_norm, N, P, d, l, epochs, lr, l2)\n",
    "        model_name = model_name_base + \".pth\"\n",
    "        save_idx = -1\n",
    "        h5_path = os.path.join(data_PATH, model_name_base + \".h5\")\n",
    "        # 4. Load weights + metrics\n",
    "        dataset, model, optimizer = initialize(N, P, d, lr, spin_type,device, gamma, init_Hebb=init_Hebb, downf=downf)\n",
    "        model, optimizer, metrics, epoch_saved = load_training(\n",
    "            h5_path=h5_path,\n",
    "            save_idx=save_idx,\n",
    "            model=model,\n",
    "            METRIC_NAMES=METRIC_NAMES,\n",
    "            device=device,\n",
    "        )\n",
    "        print(\"d:{} alpha:{}\".format(d, alpha_P))\n",
    "        curve[i_d, i_a] = metrics[\"R\"] \n",
    "        l_numerical[i_d, i_a] = metrics[\"norm_J\"]\n",
    "        print(metrics[\"R\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 8), sharey=True)\n",
    "\n",
    "for i_d, ax in enumerate(axs):\n",
    "    d = d_arr[i_d]\n",
    "    ax.set_title(f\"d={d}\")\n",
    "    for i_a, alpha in enumerate(alpha_P_arr):\n",
    "        ax.plot(l_numerical[i_d, i_a, :-1], curve[i_d, i_a, :-1], \"-\", label=r\"$\\alpha$={}\".format(alpha))\n",
    "        \n",
    "    ax.set_ylim((0, 1.0))\n",
    "    ax.legend()\n",
    "\n",
    "# common labels\n",
    "fig.supxlabel(r\"$\\lambda$\", fontsize=16)\n",
    "fig.supylabel(r\"$R(\\lambda)$\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"results_free_norm\"\n",
    "\n",
    "to_save = np.zeros((2,len(d_arr), len(alpha_P_arr), len(t_arr)))\n",
    "\n",
    "to_save[0] = l_numerical\n",
    "to_save[1] = curve\n",
    "\n",
    "np.save(\"/home/benedetti/PL_library_study/analysis/\"+name, to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbms-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
